{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition\n",
    "\n",
    "A survey of statistical and deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Speech Recognition Pipeline\n",
    "\n",
    "<img src='assets/speech/asr-pipeline.jpg'/>\n",
    "\n",
    "\n",
    "Source: https://www.techrepublic.com/article/how-we-learned-to-talk-to-computers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speech Feature Extraction\n",
    "\n",
    "<img src='assets/speech/Spectrogram-19thC.png'/>\n",
    "\n",
    "A spectrogram for \"nineteen century\" - power vs. frequency\n",
    "\n",
    "Common method: Mel-frequency cepstral coefficients (MFCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Speech Recognition\n",
    "\n",
    "$$W^* = \\underset{W}{\\operatorname{argmax}}P(W|X)$$\n",
    "\n",
    "- word sequence: $W$\n",
    "- most likely word sequence: $W^*$\n",
    "- acoustic input feature vector (e.g. MFCC): $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Speech Recognition\n",
    "\n",
    "After Bayes' Theorem:\n",
    "\n",
    "$$W^* = \\underset{W}{\\operatorname{argmax}}p(X|W)P(W)$$\n",
    "\n",
    "- acoustic model: $p(X|W)$\n",
    "- language model (e.g. N-gram): $P(W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Acoustic Model: $p(X|W)$\n",
    "\n",
    "<img src='assets/speech/acoustic-statistical.png' width='50%'/>\n",
    "\n",
    "Credits: https://www.inf.ed.ac.uk/teaching/courses/asr/2016-17/asr03-hmmgmm-handout.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Markov Model: $p(S_i|S_{i-1})$, Gaussian Mixture Model: $p(X|S_i)$\n",
    "\n",
    "<img src='assets/speech/acoustic-hmm-gmm.png' width='50%'/>\n",
    "\n",
    "Credits: https://www.inf.ed.ac.uk/teaching/courses/asr/2016-17/asr03-hmmgmm-handout.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "Mixture distribution: combine multiple probabability distributions to make an improved model\n",
    "\n",
    "$$P(x) = \\sum_iP(c=i)P(x \\mid c=i)$$\n",
    "\n",
    "$i^{th}$ Gaussian component: $P(x \\mid c=i)$\n",
    "\n",
    "Applications\n",
    "- Clustering\n",
    "- Classification\n",
    "\n",
    "Nice intro:\n",
    "https://yulearning.blogspot.sg/2014/11/einsteins-most-famous-equation-is-emc2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: GMM gender detector\n",
    "---\n",
    "\n",
    "Credits: https://github.com/abhijeet3922/PyGender-Voice\n",
    "\n",
    "<img src='assets/speech/workshop1_pygender.png' style='float:right'/>\n",
    "\n",
    "1. Download data from [here](\n",
    "https://www.dropbox.com/s/hcku4t7alrhacqv/pygender.zip?dl=0)\n",
    "\n",
    "2. Extract the .zip file to a folder of your choice. Note down the path as you will need to enter it in the workshop code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "!pip3 install python_speech_features\n",
    "\n",
    "import os\n",
    "from os.path import basename, join\n",
    "import numpy as np\n",
    "\n",
    "import python_speech_features as mfcc\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "TRAIN_PATH = 'C:\\\\mldds\\\\pygender\\\\train_data\\\\youtube\\\\' # modify to your actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def get_MFCC(audio_file, scale=True):\n",
    "    '''Computes the Mel-frequency ceptrum coefficients for an audio file,\n",
    "    with optional scaling\n",
    "    See: https://github.com/jameslyons/python_speech_features\n",
    "    '''\n",
    "    sample_rate, audio = read(audio_file)\n",
    "    features = mfcc.mfcc(audio, sample_rate, winlen=0.025, winstep=0.01, numcep=13, appendEnergy=False)\n",
    "    if scale:\n",
    "        features = preprocessing.scale(features) # scale to (0, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Playback a sample file\n",
    "from IPython import display\n",
    "\n",
    "sample_file = join(TRAIN_PATH, 'male', 'male1.wav')\n",
    "sample_rate, audio = read(sample_file)\n",
    "display.Audio(data=audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the MFCC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mfcc_vector = get_MFCC(sample_file, scale=False)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,4))\n",
    "cax = ax.matshow(np.transpose(mfcc_vector), interpolation='nearest', aspect='auto', cmap='coolwarm', origin='lower')\n",
    "fig.colorbar(cax)\n",
    "plt.title(\"Spectrogram of {}\".format(sample_file))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def train_GMM(data_path, n_components=8, covariance_type='diag'):\n",
    "    '''Trains a Gaussian mixture model for a given label and data path'''\n",
    "    files = [join(data_path, f) for f in os.listdir(data_path) if f.endswith('.wav')]\n",
    "    features = np.asarray(());\n",
    "\n",
    "    for f in files:\n",
    "        mfcc_vector = get_MFCC(f)\n",
    "\n",
    "        if features.size:\n",
    "            features = np.vstack((features, mfcc_vector))\n",
    "        else:\n",
    "            features = mfcc_vector\n",
    "\n",
    "    # http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html\n",
    "    gmm = GaussianMixture(n_components=n_components, covariance_type=covariance_type,\n",
    "                          max_iter=200, n_init=3)\n",
    "    gmm.fit(features)\n",
    "    \n",
    "    # print some metrics applicable to GMMs\n",
    "    print('BIC: ', gmm.bic(features), ', AIC: ', gmm.aic(features))\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "%time models['male'] = train_GMM(join(TRAIN_PATH, 'male'), n_components=8, covariance_type='diag')\n",
    "\n",
    "# ==================================================================\n",
    "# Exercise:\n",
    "# Add code below to train the female model, using the above as an example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Optional Exercises:\n",
    "# a. Try different values of n_component (e.g. 2, 16)\n",
    "# b. Try different values of covariance_type (e.g. full)\n",
    "#\n",
    "# See http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html\n",
    "# on how to interpret the BIC and AIC metrics for selecting models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def test_GMM(models, test_data_path):\n",
    "    '''Tests multiple Gaussian mixture models with test data'''\n",
    "    files = [os.path.join(test_data_path,f) for f in os.listdir(test_data_path)\n",
    "             if f.endswith(\".wav\")]\n",
    "    \n",
    "    predictions = []\n",
    "    for f in files:\n",
    "        features = get_MFCC(f)\n",
    "        keys = []\n",
    "        log_likelihood = np.zeros(len(models))\n",
    "\n",
    "        for i, (key, gmm) in enumerate(models.items()):\n",
    "            scores = np.array(gmm.score(features))\n",
    "            keys.append(key)\n",
    "            log_likelihood[i] = scores.sum()\n",
    "\n",
    "        # find the model with the maximum score\n",
    "        winner = np.argmax(log_likelihood)\n",
    "        # print('prediction:', keys[winner], \"\\tscores:\", log_likelihood[winner])\n",
    "        predictions.append(keys[winner])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Exercise:\n",
    "# 1. Complete the code below to test the GMM models using test_GMM().\n",
    "#    Be sure to run against both male and female models.\n",
    "# 2. Plot the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "TEST_PATH = 'C:\\\\mldds\\\\pygender\\\\test_data\\\\AudioSet' # modify to your actual path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shortcomings of Statistical Approaches\n",
    "\n",
    "Lots of hand-tuning\n",
    "\n",
    "Inefficient for approximating non-linear data: combination covariance matrices get very large / complicated\n",
    "\n",
    "Solution: deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Belief Nets\n",
    "\n",
    "[Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf)\n",
    "\n",
    "- Replaces the Gaussian Mixture Model in an ASR system \n",
    "- Generative pre-training\n",
    " - Speeds up training with less overfitting\n",
    " - Train a hidden layer (using Restricted Boltmann Machines or Gaussian RBM)\n",
    " - Use the weights as inputs to train next layer\n",
    " - Stack up into a forward-only \"Deep Belief Net\" (DBN)\n",
    " - Add softmax to create the DBM-DNN\n",
    "\n",
    "Python: https://pypi.org/project/nolearn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![DBN-DNN](assets/speech/dbn-dnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![DBN-DNN performance](assets/speech/dbn-dnn-compare.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Error Rate\n",
    "\n",
    "$$WER = \\frac{S + D + I}{N}$$\n",
    "\n",
    "- substitution word count: $S$\n",
    "- deletion word count: $D$\n",
    "- insertion word count: $I$\n",
    "- correct word count: $C$\n",
    "- number of reference words: $N = S + D + C$\n",
    "\n",
    "## Word Accuracy\n",
    "$$WAcc = 1 - WER = \\frac{(N - S - D) - I}{N} = \\frac{C - I}{N} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "[Paper](http://proceedings.mlr.press/v32/graves14.pdf)\n",
    "\n",
    "End-to-End Speech Recognition\n",
    "- Transcription is hard. Skip it and train model that converts speech directly to text\n",
    "- Bidirectional LSTM to learn long sequences\n",
    "- Correctionist Temporal Classification (CTC) to align audio with text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "\n",
    "- Learns long sequences (like speech)\n",
    "- Forget gate learns what to forget\n",
    "\n",
    "![lstm](assets/speech/lstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Birectional RNN\n",
    "\n",
    "- Forward ($\\overrightarrow{h}$) and backward ($\\overleftarrow{h}$) hidden sequences\n",
    "- Learns context in both directions (like words in speech)\n",
    "- Bidirectional LSTM: LSTM hidden units \n",
    "\n",
    "![BRNN](assets/speech/brnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correctionist Temporal Classification\n",
    "\n",
    "[Paper](https://www.cs.toronto.edu/~graves/icml_2006.pdf)\n",
    "\n",
    "- To train speech to text, we need to know how audio \"lines up\" with transcripts\n",
    "- Input sequences: $X$ (such as audio)\n",
    "- Output sequences: $Y$ (such as transcripts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correctionist Temporal Classification\n",
    "\n",
    "Problem: $X$ and $Y$ can vary in length, in different ways\n",
    "![CTC](assets/speech/naive_alignment.svg)\n",
    "\n",
    "Solution: introduce the blank token ($\\epsilon$)\n",
    "![CTC](assets/speech/ctc_alignment_steps.svg)\n",
    "\n",
    "Process: train an RNN to estimate probabilities of each character per time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correctionist Temporal Classification\n",
    "\n",
    "![CTC](assets/speech/full_collapse_from_audio.svg)\n",
    "\n",
    "[Visual Guide and Explanation](https://distill.pub/2017/ctc/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: CTC + BLSTM\n",
    "---\n",
    "Credits: https://github.com/jonrein/tensorflow_CTC_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/lspvic/jupyter_tensorboard\n",
    "!pip3 install jupyter-tensorboard\n",
    "\n",
    "'''\n",
    "Example of a single-layer bidirectional long short-term memory network trained with\n",
    "connectionist temporal classification to predict character sequences from nFeatures x nFrames\n",
    "arrays of Mel-Frequency Cepstral Coefficients.  This is test code to run on the\n",
    "8-item data set in the \"sample_data\" directory, for those without access to TIMIT.\n",
    "Author: Jon Rein\n",
    "'''\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import ctc_ops as ctc\n",
    "\n",
    "# Learning Parameters\n",
    "learningRate = 0.001\n",
    "momentum = 0.9\n",
    "nEpochs = 300\n",
    "batchSize = 4\n",
    "\n",
    "# Network Parameters\n",
    "nFeatures = 26 # 12 MFCC coefficients + energy, and derivatives\n",
    "nHidden = 128\n",
    "nClasses = 28 # 27 characters, plus the \"blank\" for CTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "sys.path += ['demos/tensorflow_CTC_example']\n",
    "from utils import load_batched_data\n",
    "\n",
    "# directory of MFCC nFeatures x nFrames 2-D array .npy files\n",
    "INPUT_PATH = 'demos/tensorflow_CTC_example/sample_data/mfcc' \n",
    "\n",
    "# directory of nCharacters 1-D array .npy files\n",
    "TARGET_PATH = 'demos/tensorflow_CTC_example/sample_data/char_y/'\n",
    "\n",
    "batchedData, maxTimeSteps, totalN = load_batched_data(INPUT_PATH, TARGET_PATH, batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Define graph\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    ####NOTE: try variable-steps inputs and dynamic bidirectional rnn, when it's implemented in tensorflow\n",
    "        \n",
    "    ####Graph input\n",
    "    inputX = tf.placeholder(tf.float32, shape=(maxTimeSteps, batchSize, nFeatures))\n",
    "    #Prep input data to fit requirements of rnn.bidirectional_rnn\n",
    "    #  Reshape to 2-D tensor (nTimeSteps*batchSize, nfeatures)\n",
    "    inputXrs = tf.reshape(inputX, [-1, nFeatures])\n",
    "    #  Split to get a list of 'n_steps' tensors of shape (batch_size, n_hidden)\n",
    "    inputList = tf.split(inputXrs, maxTimeSteps, 0)\n",
    "    targetIxs = tf.placeholder(tf.int64)\n",
    "    targetVals = tf.placeholder(tf.int32)\n",
    "    targetShape = tf.placeholder(tf.int64)\n",
    "    targetY = tf.SparseTensor(targetIxs, targetVals, targetShape)\n",
    "    seqLengths = tf.placeholder(tf.int32, shape=(batchSize))\n",
    "\n",
    "    ####Weights & biases\n",
    "    weightsOutH1 = tf.Variable(tf.truncated_normal([2, nHidden],\n",
    "                                                   stddev=np.sqrt(2.0 / (2*nHidden))))\n",
    "    biasesOutH1 = tf.Variable(tf.zeros([nHidden]))\n",
    "    weightsOutH2 = tf.Variable(tf.truncated_normal([2, nHidden],\n",
    "                                                   stddev=np.sqrt(2.0 / (2*nHidden))))\n",
    "    biasesOutH2 = tf.Variable(tf.zeros([nHidden]))\n",
    "    weightsClasses = tf.Variable(tf.truncated_normal([nHidden, nClasses],\n",
    "                                                     stddev=np.sqrt(2.0 / nHidden)))\n",
    "    biasesClasses = tf.Variable(tf.zeros([nClasses]))\n",
    "\n",
    "    ####Network\n",
    "    forwardH1 = tf.contrib.rnn.LSTMCell(nHidden, use_peepholes=True, state_is_tuple=True)\n",
    "    backwardH1 = tf.contrib.rnn.LSTMCell(nHidden, use_peepholes=True, state_is_tuple=True)\n",
    "    fbH1, _, _ = tf.contrib.rnn.static_bidirectional_rnn(forwardH1, backwardH1, inputList, dtype=tf.float32,\n",
    "                                                         scope='BDLSTM_H1')\n",
    "    fbH1rs = [tf.reshape(t, [batchSize, 2, nHidden]) for t in fbH1]\n",
    "    outH1 = [tf.reduce_sum(tf.multiply(t, weightsOutH1), reduction_indices=1) + biasesOutH1 for t in fbH1rs]\n",
    "\n",
    "    logits = [tf.matmul(t, weightsClasses) + biasesClasses for t in outH1]\n",
    "\n",
    "    ####Optimizing\n",
    "    logits3d = tf.stack(logits)\n",
    "    loss = tf.reduce_mean(ctc.ctc_loss(targetY, logits3d, seqLengths))\n",
    "    optimizer = tf.train.MomentumOptimizer(learningRate, momentum).minimize(loss)\n",
    "\n",
    "    ####Evaluating\n",
    "    logitsMaxTest = tf.slice(tf.argmax(logits3d, 2), [0, 0], [seqLengths[0], 1])\n",
    "    predictions = tf.to_int32(ctc.ctc_beam_search_decoder(logits3d, seqLengths)[0][0])\n",
    "    errorRate = tf.reduce_sum(tf.edit_distance(predictions, targetY, normalize=False)) / \\\n",
    "                tf.to_float(tf.size(targetY.values))\n",
    "\n",
    "    # TensorBoard\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    tf.summary.scalar('error_rate', errorRate)\n",
    "    merged_summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing\n",
      "Epoch 1 ...\n",
      "[ 0  1  2  3  4  5  6  7  8  9 11 12 14 16 17 18 19 20 21 22 23 24 25 27]\n",
      "Minibatch 0 / 1 loss: 952.65106\n",
      "Minibatch 0 / 1 error rate: 1.5361446\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 796.5039\n",
      "Minibatch 1 / 0 error rate: 1.2878228\n",
      "Epoch 1 error rate: 1.4119837284088135\n",
      "Epoch 2 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 729.9696\n",
      "Minibatch 0 / 1 error rate: 0.92771083\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 375.2119\n",
      "Minibatch 1 / 0 error rate: 0.7527675\n",
      "Epoch 2 error rate: 0.8402391672134399\n",
      "Epoch 3 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 268.21786\n",
      "Minibatch 0 / 1 error rate: 0.97590363\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 312.0193\n",
      "Minibatch 1 / 0 error rate: 1.0\n",
      "Epoch 3 error rate: 0.9879518151283264\n",
      "Epoch 4 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 504.63135\n",
      "Minibatch 0 / 1 error rate: 1.0\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 404.34372\n",
      "Minibatch 1 / 0 error rate: 1.0\n",
      "Epoch 4 error rate: 1.0\n",
      "Epoch 5 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 411.62268\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 205.49542\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 5 error rate: 0.9865958392620087\n",
      "Epoch 6 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 340.12134\n",
      "Minibatch 0 / 0 error rate: 1.4354243\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 412.23123\n",
      "Minibatch 1 / 1 error rate: 1.3012048\n",
      "Epoch 6 error rate: 1.3683145642280579\n",
      "Epoch 7 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 241.9964\n",
      "Minibatch 0 / 1 error rate: 0.97590363\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 240.39128\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 7 error rate: 0.9805717468261719\n",
      "Epoch 8 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 284.8253\n",
      "Minibatch 0 / 0 error rate: 0.98892987\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 361.87415\n",
      "Minibatch 1 / 1 error rate: 0.9879518\n",
      "Epoch 8 error rate: 0.9884408414363861\n",
      "Epoch 9 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 307.57544\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 207.56946\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 9 error rate: 0.9865958392620087\n",
      "Epoch 10 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 234.12248\n",
      "Minibatch 0 / 0 error rate: 0.98523986\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 306.56934\n",
      "Minibatch 1 / 1 error rate: 0.9879518\n",
      "Epoch 10 error rate: 0.9865958392620087\n",
      "Epoch 11 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 221.4812\n",
      "Minibatch 0 / 0 error rate: 0.98523986\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 251.41869\n",
      "Minibatch 1 / 1 error rate: 0.9879518\n",
      "Epoch 11 error rate: 0.9865958392620087\n",
      "Epoch 12 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 196.8143\n",
      "Minibatch 0 / 0 error rate: 0.9704797\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 257.19742\n",
      "Minibatch 1 / 1 error rate: 0.97590363\n",
      "Epoch 12 error rate: 0.9731916785240173\n",
      "Epoch 13 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 261.59674\n",
      "Minibatch 0 / 1 error rate: 0.97590363\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 205.70299\n",
      "Minibatch 1 / 0 error rate: 0.9815498\n",
      "Epoch 13 error rate: 0.978726714849472\n",
      "Epoch 14 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 258.85062\n",
      "Minibatch 0 / 1 error rate: 0.98493975\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 201.03825\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 14 error rate: 0.9850898087024689\n",
      "Epoch 15 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 198.2065\n",
      "Minibatch 0 / 0 error rate: 0.98523986\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 246.52534\n",
      "Minibatch 1 / 1 error rate: 0.9879518\n",
      "Epoch 15 error rate: 0.9865958392620087\n",
      "Epoch 16 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 195.0434\n",
      "Minibatch 0 / 0 error rate: 0.98523986\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 243.40698\n",
      "Minibatch 1 / 1 error rate: 0.9879518\n",
      "Epoch 16 error rate: 0.9865958392620087\n",
      "Epoch 17 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 243.7712\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 197.4917\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 17 error rate: 0.9865958392620087\n",
      "Epoch 18 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 245.43768\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 197.97939\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 18 error rate: 0.9865958392620087\n",
      "Epoch 19 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 243.99222\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 194.47533\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 19 error rate: 0.9865958392620087\n",
      "Epoch 20 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 192.46465\n",
      "Minibatch 0 / 0 error rate: 0.98523986\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 240.28043\n",
      "Minibatch 1 / 1 error rate: 0.9879518\n",
      "Epoch 20 error rate: 0.9865958392620087\n",
      "Epoch 21 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 240.2157\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 189.60074\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 21 error rate: 0.9865958392620087\n",
      "Epoch 22 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 240.43272\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 188.39868\n",
      "Minibatch 1 / 0 error rate: 0.98523986\n",
      "Epoch 22 error rate: 0.9865958392620087\n",
      "Epoch 23 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 187.5348\n",
      "Minibatch 0 / 0 error rate: 0.9704797\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 238.64705\n",
      "Minibatch 1 / 1 error rate: 0.96385545\n",
      "Epoch 23 error rate: 0.9671675860881805\n",
      "Epoch 24 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 237.8447\n",
      "Minibatch 0 / 1 error rate: 0.91566265\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 185.55368\n",
      "Minibatch 1 / 0 error rate: 0.86346865\n",
      "Epoch 24 error rate: 0.889565646648407\n",
      "Epoch 25 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 237.3955\n",
      "Minibatch 0 / 1 error rate: 0.8915663\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 185.7852\n",
      "Minibatch 1 / 0 error rate: 0.86346865\n",
      "Epoch 25 error rate: 0.8775174617767334\n",
      "Epoch 26 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 237.71036\n",
      "Minibatch 0 / 1 error rate: 0.88554215\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 185.26282\n",
      "Minibatch 1 / 0 error rate: 0.8708487\n",
      "Epoch 26 error rate: 0.8781954348087311\n",
      "Epoch 27 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 237.20018\n",
      "Minibatch 0 / 1 error rate: 0.91566265\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 184.0097\n",
      "Minibatch 1 / 0 error rate: 0.900369\n",
      "Epoch 27 error rate: 0.9080158174037933\n",
      "Epoch 28 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 183.67467\n",
      "Minibatch 0 / 0 error rate: 0.94833946\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 237.61046\n",
      "Minibatch 1 / 1 error rate: 0.9698795\n",
      "Epoch 28 error rate: 0.9591094851493835\n",
      "Epoch 29 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 237.82602\n",
      "Minibatch 0 / 1 error rate: 0.9879518\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 183.42117\n",
      "Minibatch 1 / 0 error rate: 0.9704797\n",
      "Epoch 29 error rate: 0.9792157709598541\n",
      "Epoch 30 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 183.2193\n",
      "Minibatch 0 / 0 error rate: 0.9778598\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 236.99565\n",
      "Minibatch 1 / 1 error rate: 0.9879518\n",
      "Epoch 30 error rate: 0.9829058051109314\n",
      "Epoch 31 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 236.66263\n",
      "Minibatch 0 / 1 error rate: 0.97590363\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 183.16255\n",
      "Minibatch 1 / 0 error rate: 0.94833946\n",
      "Epoch 31 error rate: 0.9621215462684631\n",
      "Epoch 32 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 236.44885\n",
      "Minibatch 0 / 1 error rate: 0.96686745\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 183.40578\n",
      "Minibatch 1 / 0 error rate: 0.9409594\n",
      "Epoch 32 error rate: 0.9539134204387665\n",
      "Epoch 33 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 236.20917\n",
      "Minibatch 0 / 1 error rate: 0.9578313\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 182.98668\n",
      "Minibatch 1 / 0 error rate: 0.96309966\n",
      "Epoch 33 error rate: 0.9604654908180237\n",
      "Epoch 34 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 182.72491\n",
      "Minibatch 0 / 0 error rate: 0.9594096\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 236.10345\n",
      "Minibatch 1 / 1 error rate: 0.95481926\n",
      "Epoch 34 error rate: 0.9571144282817841\n",
      "Epoch 35 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 182.561\n",
      "Minibatch 0 / 0 error rate: 0.94464946\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 236.2983\n",
      "Minibatch 1 / 1 error rate: 0.9608434\n",
      "Epoch 35 error rate: 0.9527464210987091\n",
      "Epoch 36 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 236.081\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 182.46564\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 36 error rate: 0.9368192255496979\n",
      "Epoch 37 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 182.50351\n",
      "Minibatch 0 / 0 error rate: 0.92250925\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 235.36838\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 37 error rate: 0.9371582269668579\n",
      "Epoch 38 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 235.26898\n",
      "Minibatch 0 / 1 error rate: 0.939759\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 182.76009\n",
      "Minibatch 1 / 0 error rate: 0.900369\n",
      "Epoch 38 error rate: 0.9200640022754669\n",
      "Epoch 39 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 182.68521\n",
      "Minibatch 0 / 0 error rate: 0.9151291\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 234.98933\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 39 error rate: 0.9334681630134583\n",
      "Epoch 40 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 234.93417\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 182.33394\n",
      "Minibatch 1 / 0 error rate: 0.9298893\n",
      "Epoch 40 error rate: 0.942354291677475\n",
      "Epoch 41 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 182.27728\n",
      "Minibatch 0 / 0 error rate: 0.9298893\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27]\n",
      "Minibatch 1 / 1 loss: 234.89272\n",
      "Minibatch 1 / 1 error rate: 0.9608434\n",
      "Epoch 41 error rate: 0.9453663527965546\n",
      "Epoch 42 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 234.77744\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 182.19875\n",
      "Minibatch 1 / 0 error rate: 0.9298893\n",
      "Epoch 42 error rate: 0.942354291677475\n",
      "Epoch 43 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 234.37598\n",
      "Minibatch 0 / 1 error rate: 0.9518072\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 182.28412\n",
      "Minibatch 1 / 0 error rate: 0.92619926\n",
      "Epoch 43 error rate: 0.9390032291412354\n",
      "Epoch 44 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 182.27621\n",
      "Minibatch 0 / 0 error rate: 0.9335793\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 234.0835\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 44 error rate: 0.9426932632923126\n",
      "Epoch 45 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 234.00851\n",
      "Minibatch 0 / 1 error rate: 0.94578314\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 182.07428\n",
      "Minibatch 1 / 0 error rate: 0.92619926\n",
      "Epoch 45 error rate: 0.9359911978244781\n",
      "Epoch 46 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 233.83617\n",
      "Minibatch 0 / 1 error rate: 0.9487952\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 181.93863\n",
      "Minibatch 1 / 0 error rate: 0.92619926\n",
      "Epoch 46 error rate: 0.9374972283840179\n",
      "Epoch 47 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 233.69733\n",
      "Minibatch 0 / 1 error rate: 0.9427711\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 181.8104\n",
      "Minibatch 1 / 0 error rate: 0.92250925\n",
      "Epoch 47 error rate: 0.9326401650905609\n",
      "Epoch 48 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 233.53429\n",
      "Minibatch 0 / 1 error rate: 0.939759\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 181.7051\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 48 error rate: 0.9292891025543213\n",
      "Epoch 49 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 233.31392\n",
      "Minibatch 0 / 1 error rate: 0.94578314\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 181.61751\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 49 error rate: 0.9323011636734009\n",
      "Epoch 50 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 233.0777\n",
      "Minibatch 0 / 1 error rate: 0.93373495\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 181.52193\n",
      "Minibatch 1 / 0 error rate: 0.92250925\n",
      "Epoch 50 error rate: 0.9281221032142639\n",
      "Epoch 51 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 232.85907\n",
      "Minibatch 0 / 1 error rate: 0.9487952\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 181.38072\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 51 error rate: 0.9338071942329407\n",
      "Epoch 52 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 181.25424\n",
      "Minibatch 0 / 0 error rate: 0.9114391\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 232.68002\n",
      "Minibatch 1 / 1 error rate: 0.9427711\n",
      "Epoch 52 error rate: 0.9271050989627838\n",
      "Epoch 53 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 180.98712\n",
      "Minibatch 0 / 0 error rate: 0.92619926\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 232.55916\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 53 error rate: 0.9390032291412354\n",
      "Epoch 54 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 232.37782\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 180.8277\n",
      "Minibatch 1 / 0 error rate: 0.92619926\n",
      "Epoch 54 error rate: 0.9405092597007751\n",
      "Epoch 55 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 231.95839\n",
      "Minibatch 0 / 1 error rate: 0.9427711\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 180.81796\n",
      "Minibatch 1 / 0 error rate: 0.92619926\n",
      "Epoch 55 error rate: 0.9344851672649384\n",
      "Epoch 56 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 231.62953\n",
      "Minibatch 0 / 1 error rate: 0.9427711\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 180.62541\n",
      "Minibatch 1 / 0 error rate: 0.9298893\n",
      "Epoch 56 error rate: 0.9363301992416382\n",
      "Epoch 57 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 231.3472\n",
      "Minibatch 0 / 1 error rate: 0.9518072\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 180.34853\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 57 error rate: 0.9426932632923126\n",
      "Epoch 58 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 231.11684\n",
      "Minibatch 0 / 1 error rate: 0.9518072\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 180.12741\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 58 error rate: 0.9426932632923126\n",
      "Epoch 59 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 180.00266\n",
      "Minibatch 0 / 0 error rate: 0.9298893\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 230.7511\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 59 error rate: 0.9408482611179352\n",
      "Epoch 60 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 230.51428\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 179.74722\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 60 error rate: 0.9441992938518524\n",
      "Epoch 61 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 229.97443\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 179.70795\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 61 error rate: 0.9441992938518524\n",
      "Epoch 62 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 179.5462\n",
      "Minibatch 0 / 0 error rate: 0.9372694\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 229.4839\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 62 error rate: 0.9445382952690125\n",
      "Epoch 63 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 229.34332\n",
      "Minibatch 0 / 1 error rate: 0.9518072\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 178.99017\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 63 error rate: 0.9426932632923126\n",
      "Epoch 64 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 228.94742\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 178.74702\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 64 error rate: 0.9441992938518524\n",
      "Epoch 65 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 228.50392\n",
      "Minibatch 0 / 1 error rate: 0.95481926\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 178.51242\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 65 error rate: 0.9441992938518524\n",
      "Epoch 66 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 228.04286\n",
      "Minibatch 0 / 1 error rate: 0.9518072\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 178.25679\n",
      "Minibatch 1 / 0 error rate: 0.9335793\n",
      "Epoch 66 error rate: 0.9426932632923126\n",
      "Epoch 67 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 178.05597\n",
      "Minibatch 0 / 0 error rate: 0.9335793\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 227.53322\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 67 error rate: 0.9426932632923126\n",
      "Epoch 68 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 177.59323\n",
      "Minibatch 0 / 0 error rate: 0.9335793\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 227.21982\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 68 error rate: 0.9426932632923126\n",
      "Epoch 69 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 177.19373\n",
      "Minibatch 0 / 0 error rate: 0.9188192\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 226.77034\n",
      "Minibatch 1 / 1 error rate: 0.95481926\n",
      "Epoch 69 error rate: 0.9368192255496979\n",
      "Epoch 70 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 226.48055\n",
      "Minibatch 0 / 1 error rate: 0.9608434\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 176.83266\n",
      "Minibatch 1 / 0 error rate: 0.9409594\n",
      "Epoch 70 error rate: 0.9509013891220093\n",
      "Epoch 71 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 176.6409\n",
      "Minibatch 0 / 0 error rate: 0.9298893\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 225.78891\n",
      "Minibatch 1 / 1 error rate: 0.9518072\n",
      "Epoch 71 error rate: 0.9408482611179352\n",
      "Epoch 72 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 176.01288\n",
      "Minibatch 0 / 0 error rate: 0.92619926\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 225.50575\n",
      "Minibatch 1 / 1 error rate: 0.95481926\n",
      "Epoch 72 error rate: 0.9405092597007751\n",
      "Epoch 73 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 225.27916\n",
      "Minibatch 0 / 1 error rate: 0.9487952\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 175.33737\n",
      "Minibatch 1 / 0 error rate: 0.92619926\n",
      "Epoch 73 error rate: 0.9374972283840179\n",
      "Epoch 74 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 175.12161\n",
      "Minibatch 0 / 0 error rate: 0.9409594\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 224.51276\n",
      "Minibatch 1 / 1 error rate: 0.92469877\n",
      "Epoch 74 error rate: 0.9328290820121765\n",
      "Epoch 75 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 174.5969\n",
      "Minibatch 0 / 0 error rate: 0.9335793\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 224.06929\n",
      "Minibatch 1 / 1 error rate: 0.92469877\n",
      "Epoch 75 error rate: 0.9291390478610992\n",
      "Epoch 76 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 174.09302\n",
      "Minibatch 0 / 0 error rate: 0.9335793\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 223.58748\n",
      "Minibatch 1 / 1 error rate: 0.9186747\n",
      "Epoch 76 error rate: 0.926127016544342\n",
      "Epoch 77 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 223.28741\n",
      "Minibatch 0 / 1 error rate: 0.9186747\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 173.5057\n",
      "Minibatch 1 / 0 error rate: 0.92619926\n",
      "Epoch 77 error rate: 0.9224369823932648\n",
      "Epoch 78 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 222.65825\n",
      "Minibatch 0 / 1 error rate: 0.92469877\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 173.06012\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 78 error rate: 0.9217589795589447\n",
      "Epoch 79 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 172.70778\n",
      "Minibatch 0 / 0 error rate: 0.92619926\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 222.01419\n",
      "Minibatch 1 / 1 error rate: 0.9126506\n",
      "Epoch 79 error rate: 0.9194249212741852\n",
      "Epoch 80 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 221.78484\n",
      "Minibatch 0 / 1 error rate: 0.9186747\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 171.83524\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 80 error rate: 0.9187469482421875\n",
      "Epoch 81 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 171.57613\n",
      "Minibatch 0 / 0 error rate: 0.9188192\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 220.85568\n",
      "Minibatch 1 / 1 error rate: 0.9186747\n",
      "Epoch 81 error rate: 0.9187469482421875\n",
      "Epoch 82 ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27]\n",
      "Minibatch 0 / 1 loss: 220.4983\n",
      "Minibatch 0 / 1 error rate: 0.9126506\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 170.90096\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 82 error rate: 0.9157348871231079\n",
      "Epoch 83 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 170.64674\n",
      "Minibatch 0 / 0 error rate: 0.9188192\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 219.54288\n",
      "Minibatch 1 / 1 error rate: 0.9126506\n",
      "Epoch 83 error rate: 0.9157348871231079\n",
      "Epoch 84 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 219.23558\n",
      "Minibatch 0 / 1 error rate: 0.9126506\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 169.73485\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 84 error rate: 0.9157348871231079\n",
      "Epoch 85 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 169.40288\n",
      "Minibatch 0 / 0 error rate: 0.9114391\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 218.35645\n",
      "Minibatch 1 / 1 error rate: 0.9066265\n",
      "Epoch 85 error rate: 0.9090328216552734\n",
      "Epoch 86 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 218.01537\n",
      "Minibatch 0 / 1 error rate: 0.9066265\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 168.50656\n",
      "Minibatch 1 / 0 error rate: 0.9114391\n",
      "Epoch 86 error rate: 0.9090328216552734\n",
      "Epoch 87 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 217.13837\n",
      "Minibatch 0 / 1 error rate: 0.9186747\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 168.03813\n",
      "Minibatch 1 / 0 error rate: 0.9114391\n",
      "Epoch 87 error rate: 0.9150569140911102\n",
      "Epoch 88 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 216.31711\n",
      "Minibatch 0 / 1 error rate: 0.9066265\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 167.44913\n",
      "Minibatch 1 / 0 error rate: 0.9114391\n",
      "Epoch 88 error rate: 0.9090328216552734\n",
      "Epoch 89 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 167.01862\n",
      "Minibatch 0 / 0 error rate: 0.9114391\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 215.38145\n",
      "Minibatch 1 / 1 error rate: 0.92168677\n",
      "Epoch 89 error rate: 0.91656294465065\n",
      "Epoch 90 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 215.03853\n",
      "Minibatch 0 / 1 error rate: 0.9066265\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 165.87677\n",
      "Minibatch 1 / 0 error rate: 0.90405905\n",
      "Epoch 90 error rate: 0.9053427875041962\n",
      "Epoch 91 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 214.09735\n",
      "Minibatch 0 / 1 error rate: 0.9186747\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 165.27849\n",
      "Minibatch 1 / 0 error rate: 0.9151291\n",
      "Epoch 91 error rate: 0.9169019162654877\n",
      "Epoch 92 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 213.15616\n",
      "Minibatch 0 / 1 error rate: 0.9096386\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 164.61504\n",
      "Minibatch 1 / 0 error rate: 0.900369\n",
      "Epoch 92 error rate: 0.9050037860870361\n",
      "Epoch 93 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 212.24866\n",
      "Minibatch 0 / 1 error rate: 0.9126506\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 163.81758\n",
      "Minibatch 1 / 0 error rate: 0.896679\n",
      "Epoch 93 error rate: 0.9046647846698761\n",
      "Epoch 94 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 163.3113\n",
      "Minibatch 0 / 0 error rate: 0.900369\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 211.15402\n",
      "Minibatch 1 / 1 error rate: 0.9126506\n",
      "Epoch 94 error rate: 0.9065097868442535\n",
      "Epoch 95 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 162.57372\n",
      "Minibatch 0 / 0 error rate: 0.9114391\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 210.4103\n",
      "Minibatch 1 / 1 error rate: 0.9126506\n",
      "Epoch 95 error rate: 0.9120448529720306\n",
      "Epoch 96 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 161.65373\n",
      "Minibatch 0 / 0 error rate: 0.90405905\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 209.3746\n",
      "Minibatch 1 / 1 error rate: 0.9126506\n",
      "Epoch 96 error rate: 0.9083548188209534\n",
      "Epoch 97 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 208.72641\n",
      "Minibatch 0 / 1 error rate: 0.9096386\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 160.7003\n",
      "Minibatch 1 / 0 error rate: 0.90405905\n",
      "Epoch 97 error rate: 0.906848818063736\n",
      "Epoch 98 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 160.27559\n",
      "Minibatch 0 / 0 error rate: 0.90405905\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 206.97372\n",
      "Minibatch 1 / 1 error rate: 0.9096386\n",
      "Epoch 98 error rate: 0.906848818063736\n",
      "Epoch 99 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 206.42126\n",
      "Minibatch 0 / 1 error rate: 0.9126506\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 158.65538\n",
      "Minibatch 1 / 0 error rate: 0.896679\n",
      "Epoch 99 error rate: 0.9046647846698761\n",
      "Epoch 100 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 205.25302\n",
      "Minibatch 0 / 1 error rate: 0.9126506\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 157.569\n",
      "Minibatch 1 / 0 error rate: 0.9114391\n",
      "Epoch 100 error rate: 0.9120448529720306\n",
      "Epoch 101 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 157.0101\n",
      "Minibatch 0 / 0 error rate: 0.9114391\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 203.60474\n",
      "Minibatch 1 / 1 error rate: 0.9186747\n",
      "Epoch 101 error rate: 0.9150569140911102\n",
      "Epoch 102 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 155.87718\n",
      "Minibatch 0 / 0 error rate: 0.9151291\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 202.38998\n",
      "Minibatch 1 / 1 error rate: 0.92469877\n",
      "Epoch 102 error rate: 0.9199139475822449\n",
      "Epoch 103 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 201.59978\n",
      "Minibatch 0 / 1 error rate: 0.92168677\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 154.52466\n",
      "Minibatch 1 / 0 error rate: 0.9151291\n",
      "Epoch 103 error rate: 0.9184079468250275\n",
      "Epoch 104 ...\n",
      "[27]\n",
      "Minibatch 0 / 0 loss: 154.044\n",
      "Minibatch 0 / 0 error rate: 0.9151291\n",
      "[27]\n",
      "Minibatch 1 / 1 loss: 199.3846\n",
      "Minibatch 1 / 1 error rate: 0.9186747\n",
      "Epoch 104 error rate: 0.9169019162654877\n",
      "Epoch 105 ...\n",
      "[27]\n",
      "Minibatch 0 / 1 loss: 198.6478\n",
      "Minibatch 0 / 1 error rate: 0.9186747\n",
      "[27]\n",
      "Minibatch 1 / 0 loss: 152.2271\n",
      "Minibatch 1 / 0 error rate: 0.9188192\n",
      "Epoch 105 error rate: 0.9187469482421875\n",
      "Epoch 106 ...\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    print('Initializing')\n",
    "\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # TensorBoard\n",
    "    summary_writer = tf.summary.FileWriter('./logs/train', session.graph)\n",
    "    \n",
    "    for epoch in range(nEpochs):\n",
    "        print('Epoch', epoch+1, '...')\n",
    "        batchErrors = np.zeros(len(batchedData))\n",
    "        batchRandIxs = np.random.permutation(len(batchedData)) #randomize batch order\n",
    "        for batch, batchOrigI in enumerate(batchRandIxs):\n",
    "            batchInputs, batchTargetSparse, batchSeqLengths = batchedData[batchOrigI]\n",
    "            batchTargetIxs, batchTargetVals, batchTargetShape = batchTargetSparse\n",
    "            feedDict = {inputX: batchInputs, targetIxs: batchTargetIxs, targetVals: batchTargetVals,\n",
    "                        targetShape: batchTargetShape, seqLengths: batchSeqLengths}\n",
    "\n",
    "            _, l, er, lmt, summary = session.run([optimizer, loss, errorRate, logitsMaxTest, merged_summary_op], feed_dict=feedDict)\n",
    " \n",
    "            summary_writer.add_summary(summary, epoch * batchSize + batch)\n",
    "            \n",
    "            print(np.unique(lmt)) #print unique argmax values of first sample in batch; should be blank for a while, then spit out target values\n",
    "            if (batch % 1) == 0:\n",
    "                print('Minibatch', batch, '/', batchOrigI, 'loss:', l)\n",
    "                print('Minibatch', batch, '/', batchOrigI, 'error rate:', er)\n",
    "            batchErrors[batch] = er*len(batchSeqLengths)\n",
    "        epochErrorRate = batchErrors.sum() / totalN\n",
    "        print('Epoch', epoch+1, 'error rate:', epochErrorRate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Speech\n",
    "\n",
    "Deep Speech 1: https://arxiv.org/abs/1412.5567\n",
    "\n",
    "Deep Speech 2: https://arxiv.org/abs/1512.02595"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Deep Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
   "autolaunch": true,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
