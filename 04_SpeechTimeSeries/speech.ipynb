{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition\n",
    "\n",
    "A survey of statistical and deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop Setup\n",
    "\n",
    "This lesson requires an Ubuntu Linux environment for the DeepSpeech workshop, including downloading the pre-trained model.\n",
    "\n",
    "Please follow instructions [here](speech.ipynb##Deep-Speech-Workshop-Setup), so that you can run your download in the background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Automatic Speech Recognition Pipeline\n",
    "\n",
    "<img src='assets/speech/asr-pipeline.jpg'/>\n",
    "\n",
    "\n",
    "(image: https://www.techrepublic.com/article/how-we-learned-to-talk-to-computers/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "Phoneme: basic unit of sound (e.g. /e/ in 'pet')\n",
    "\n",
    "Grapheme: letter(s) that represent phonemes (e.g. sh, ng)\n",
    "\n",
    "Acoustic model: maps audio signals to phonetic units (phonemes or graphemes)\n",
    "\n",
    "Language model: maps phonetic units to word sequences\n",
    "\n",
    "Utterance: spoken input (word, phrase, sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "Speaker-dependent speech recognition: recognizes only users it is trained for. Usually large vocabulary but few speakers.\n",
    "\n",
    "Speaker-independent speech recognition: recognizes multiple users without training. Usually small vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Error Rate\n",
    "\n",
    "$WER = \\frac{S + D + I}{N}$\n",
    "\n",
    "- substitution word count: $S$\n",
    "- deletion word count: $D$\n",
    "- insertion word count: $I$\n",
    "- correct word count: $C$\n",
    "- number of reference words: $N = S + D + C$\n",
    "\n",
    "Sometimes called the \"Length Normalized Edit Distance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word Accuracy\n",
    "$WAcc = 1 - WER = \\frac{(N - S - D) - I}{N} = \\frac{C - I}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speech Feature Extraction\n",
    "\n",
    "<img src='assets/speech/Spectrogram-19thC.png'/>\n",
    "(Image: Wikipedia)\n",
    "\n",
    "A spectrogram for \"nineteen century\" - power vs. frequency\n",
    "\n",
    "Common method: Mel-frequency cepstral coefficients (MFCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Speech Recognition\n",
    "\n",
    "$$W^* = \\underset{W}{\\operatorname{argmax}}P(W \\mid X)$$\n",
    "\n",
    "- word sequence: $W$\n",
    "- most likely word sequence: $W^*$\n",
    "- acoustic input feature vector (e.g. MFCC): $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Speech Recognition\n",
    "\n",
    "After Bayes' Theorem:\n",
    "\n",
    "$$W^* = \\underset{W}{\\operatorname{argmax}}p(X \\mid W)P(W)$$\n",
    "\n",
    "- acoustic model: $p(X \\mid W)$\n",
    "- language model (e.g. N-gram): $P(W)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Statistical Acoustic Model: $p(X|W)$\n",
    "\n",
    "<img src='assets/speech/acoustic-statistical.png' width='40%'/>\n",
    "\n",
    "(image: https://www.inf.ed.ac.uk/teaching/courses/asr/2016-17/asr03-hmmgmm-handout.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hidden Markov Model: $p(S_i \\mid S_{i-1})$, Gaussian Mixture Model: $p(X \\mid S_i)$\n",
    "\n",
    "<img src='assets/speech/acoustic-hmm-gmm.png' width='40%'/>\n",
    "\n",
    "(image: https://www.inf.ed.ac.uk/teaching/courses/asr/2016-17/asr03-hmmgmm-handout.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Mixture Model\n",
    "\n",
    "Mixture distribution: combine multiple probabability distributions to make an improved model\n",
    "\n",
    "$$P(x) = \\sum_iP(c=i)P(x \\mid c=i)$$\n",
    "\n",
    "$i^{th}$ Gaussian component: $P(x \\mid c=i)$\n",
    "\n",
    "Applications\n",
    "- Clustering\n",
    "- Classification\n",
    "\n",
    "Nice intro:\n",
    "https://yulearning.blogspot.sg/2014/11/einsteins-most-famous-equation-is-emc2.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: GMM gender detector\n",
    "---\n",
    "\n",
    "Credits: https://github.com/abhijeet3922/PyGender-Voice\n",
    "\n",
    "<img src='assets/speech/workshop1_pygender.png' style='float:right'/>\n",
    "\n",
    "1. Download data from [here](\n",
    "https://www.dropbox.com/s/hcku4t7alrhacqv/pygender.zip?dl=0)\n",
    "\n",
    "2. Extract the .zip file to a folder of your choice. Note down the path as you will need to enter it in the workshop code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import basename, join\n",
    "import numpy as np\n",
    "\n",
    "from python_speech_features import mfcc\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn import preprocessing\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "TRAIN_PATH = 'C:\\\\mldds\\\\pygender\\\\train_data\\\\youtube\\\\' # modify to your actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def get_MFCC(audio_file, scale=True):\n",
    "    \"\"\"Computes the Mel-frequency ceptrum coefficients for an audio file\n",
    "    Args:\n",
    "        audio_file: input audio file\n",
    "        scale: whether to scale the features between 0-1\n",
    "    Returns:\n",
    "        the MFCC features\n",
    "    \"\"\"    \n",
    "    sample_rate, audio = read(audio_file)\n",
    "    features = mfcc(audio, sample_rate, winlen=0.025, winstep=0.01,\n",
    "                    numcep=13, appendEnergy=False)\n",
    "    if scale:\n",
    "        features = preprocessing.scale(features) # scale to (0, 1)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Playback a sample file\n",
    "from IPython import display\n",
    "\n",
    "sample_file = join(TRAIN_PATH, 'male', 'male1.wav')\n",
    "sample_rate, audio = read(sample_file)\n",
    "display.Audio(data=audio, rate=sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the MFCC\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mfcc_vector = get_MFCC(sample_file, scale=False)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,4))\n",
    "cax = ax.matshow(np.transpose(mfcc_vector), interpolation='nearest',\n",
    "                 aspect='auto', cmap='plasma', origin='lower')\n",
    "fig.colorbar(cax)\n",
    "plt.title(\"Spectrogram of {}\".format(sample_file))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def train_GMM(data_path, n_components=8, covariance_type='diag'):\n",
    "    \"\"\"Trains a Gaussian mixture model\n",
    "    Args:\n",
    "        data_path: data folder path\n",
    "        n_components: number of Gaussian components\n",
    "        covariance_type: type of covariance matrix\n",
    "    Returns:\n",
    "        the GMM model\n",
    "    \"\"\"    \n",
    "    files = [join(data_path, f) for f in os.listdir(data_path) if f.endswith('.wav')]\n",
    "    features = np.asarray(());\n",
    "\n",
    "    for f in files:\n",
    "        mfcc_vector = get_MFCC(f)\n",
    "\n",
    "        if features.size:\n",
    "            features = np.vstack((features, mfcc_vector))\n",
    "        else:\n",
    "            features = mfcc_vector\n",
    "\n",
    "    gmm = GaussianMixture(n_components=n_components,\n",
    "                          covariance_type=covariance_type,\n",
    "                          max_iter=200, n_init=3)\n",
    "    gmm.fit(features)\n",
    "    \n",
    "    # print some metrics applicable to GMMs\n",
    "    print('BIC: ', gmm.bic(features), ', AIC: ', gmm.aic(features))\n",
    "    return gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "models = dict()\n",
    "%time models['male'] = train_GMM(join(TRAIN_PATH, 'male'), n_components=8, \\\n",
    "                                 covariance_type='diag')\n",
    "\n",
    "# ==================================================================\n",
    "# Exercise:\n",
    "# Add code below to train the female model, using the above as an example\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ==================================================================\n",
    "# Optional Exercises:\n",
    "# a. Try different values of n_component (e.g. 2, 16)\n",
    "# b. Try different values of covariance_type (e.g. full)\n",
    "#\n",
    "# See http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html\n",
    "# on how to interpret the BIC and AIC metrics for selecting models\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def test_GMM(models, test_data_path):\n",
    "    \"\"\"Trains a Gaussian mixture model\n",
    "    Args:\n",
    "        models: {name: gmm} dictionary of models\n",
    "        test_data_path: test data folder path\n",
    "    Returns:\n",
    "        the predictions\n",
    "    \"\"\"    \n",
    "    files = [os.path.join(test_data_path,f) for f in os.listdir(test_data_path)\n",
    "             if f.endswith(\".wav\")]\n",
    "    \n",
    "    predictions = []\n",
    "    for f in files:\n",
    "        features = get_MFCC(f)\n",
    "        keys = []\n",
    "        log_likelihood = np.zeros(len(models))\n",
    "\n",
    "        for i, (key, gmm) in enumerate(models.items()):\n",
    "            scores = np.array(gmm.score(features))\n",
    "            keys.append(key)\n",
    "            log_likelihood[i] = scores.sum()\n",
    "\n",
    "        # find the model with the maximum score\n",
    "        winner = np.argmax(log_likelihood)\n",
    "        # print('prediction:', keys[winner], \"\\tscores:\", log_likelihood[winner])\n",
    "        predictions.append(keys[winner])\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Exercise:\n",
    "# 1. Complete the code below to test the GMM models using test_GMM().\n",
    "#    Be sure to run against both male and female models.\n",
    "# 2. Plot the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "TEST_PATH = 'C:\\\\mldds\\\\pygender\\\\test_data\\\\AudioSet' # modify to your actual path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Shortcomings of Statistical Approaches\n",
    "\n",
    "Lots of hand-tuning\n",
    "\n",
    "Inefficient for approximating non-linear data: combination covariance matrices get very large / complicated\n",
    "\n",
    "Solution: deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Belief Nets\n",
    "\n",
    "[Paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/38131.pdf)\n",
    "\n",
    "- Replaces the Gaussian Mixture Model in an ASR system \n",
    "- Generative pre-training\n",
    " - Speeds up training with less overfitting\n",
    " - Train a hidden layer (using Restricted Boltmann Machines or Gaussian RBM)\n",
    " - Use the weights as inputs to train next layer\n",
    " - Stack up into a forward-only \"Deep Belief Net\" (DBN)\n",
    " - Add softmax to create the DBM-DNN\n",
    "\n",
    "Python: https://pypi.org/project/nolearn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![DBN-DNN](assets/speech/dbn-dnn.png)\n",
    "\n",
    "(image: Hinton et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![DBN-DNN performance](assets/speech/dbn-dnn-compare.png)\n",
    "\n",
    "(image: Hinton et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks\n",
    "\n",
    "[Paper](http://proceedings.mlr.press/v32/graves14.pdf)\n",
    "\n",
    "End-to-End Speech Recognition\n",
    "- Transcription is hard. Skip it and train model that converts speech directly to text\n",
    "- Bidirectional LSTM to learn long sequences\n",
    "- Correctionist Temporal Classification (CTC) to align audio with text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## LSTM\n",
    "\n",
    "- Learns long sequences (like speech)\n",
    "- Forget gate learns what to forget\n",
    "\n",
    "![lstm](assets/speech/lstm.png)\n",
    "\n",
    "(image: Graves et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bidirectional RNN\n",
    "![BRNN](assets/speech/brnn.png)\n",
    "\n",
    "(image: Graves et al.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Birectional RNN\n",
    "\n",
    "- Forward\n",
    "$$\\overrightarrow{h}^l_t = g(h^{l-1}_t, \\overrightarrow{h}^l_{t-1})$$\n",
    "- Backward\n",
    "$$\\overleftarrow{h}^l_t = g(h^{l-1}_t, \\overleftarrow{h}^l_{t+1})$$\n",
    "- $g(.)$ can be recurrent, GRU, LSTM\n",
    "- Learns context in both directions (like words in speech)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Correctionist Temporal Classification\n",
    "\n",
    "https://www.cs.toronto.edu/~graves/icml_2006.pdf\n",
    "\n",
    "- Goal: a temporal classifier that can classify unseen audio input sequences with low error rate\n",
    "- To train speech to text, we need to know how audio \"lines up\" with transcripts\n",
    "- Input sequences: $X$ (such as audio)\n",
    "- Output sequences: $Y$ (such as transcripts)\n",
    "- Error rate: normalized edit distance between predicted and target labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Problem: $X$ and $Y$ can vary in length, in different ways\n",
    "![CTC](assets/speech/naive_alignment.svg)\n",
    "\n",
    "Solution: introduce the blank token ($\\epsilon$)\n",
    "![CTC](assets/speech/ctc_alignment_steps.svg)\n",
    "\n",
    "Process: train an RNN to estimate probabilities of each character per time step\n",
    "\n",
    "(images: Hannun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CTC implementation\n",
    "\n",
    "- Recurrent neural network, e.g. bidirectional LSTM\n",
    "- CTC loss function, maximize log likelihood\n",
    "- Decoding: prefix search instead of best path search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![CTC](assets/speech/full_collapse_from_audio.svg)\n",
    "\n",
    "(image: [Visual Guide and Explanation](https://distill.pub/2017/ctc/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: Correctionist Temporal Classification\n",
    "---\n",
    "Credits: https://github.com/igormq/ctc_tensorflow_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"This is a simple example that illustrates how CTC is computed from TensorFlow\n",
    "For brevity, it only supports 1 training input and 1 validation input\n",
    "For an example of batched input, use:\n",
    "https://github.com/philipperemy/tensorflow-ctc-speech-recognition\n",
    "\"\"\"\n",
    "\n",
    "# !pip3 install jupyter-tensorboard\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from os.path import join\n",
    "from python_speech_features import mfcc\n",
    "from scipy.io.wavfile import read\n",
    "from sklearn import preprocessing\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "# Constants\n",
    "SPACE_TOKEN = '<space>'\n",
    "SPACE_INDEX = 0\n",
    "FIRST_INDEX = ord('a') - 1  # 0 is reserved to space\n",
    "\n",
    "# Configuration\n",
    "num_features = 13\n",
    "num_units = 50 # Number of units in the LSTM cell\n",
    "\n",
    "# Number of classes: 'a' to 'z' +  space + blank label = 28 characters\n",
    "num_classes = ord('z') - ord('a') + 1 + 1 + 1\n",
    "\n",
    "# Hyper-parameters\n",
    "num_epochs = 200\n",
    "num_hidden = 50\n",
    "# Get training and validation inputs\n",
    "num_layers = 1\n",
    "batch_size = 1\n",
    "initial_learning_rate = 1e-2\n",
    "momentum = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get training and validation inputs\n",
    "# Validation input is just the training audio shifted randomly\n",
    "\n",
    "def process_audio(audio_file, do_random_shift=None):\n",
    "    \"\"\"Extracts the MFCC features from an audio file.\n",
    "    Args:\n",
    "        audio_file: file containing the input audio\n",
    "        do_random_shift: whether to randomly shift the audio\n",
    "                      (e.g. for generating test inputs)\n",
    "    Returns:\n",
    "        A tuple with (features, sequence length)\n",
    "    \"\"\"\n",
    "    sample_rate, audio = read(audio_file)\n",
    "\n",
    "    if do_random_shift:\n",
    "        random_shift = np.random.randint(low=1, high=1000)\n",
    "        print('random_shift =', random_shift)\n",
    "        truncated_audio = audio[random_shift:]\n",
    "        \n",
    "    features = mfcc(audio, sample_rate)\n",
    "    features = preprocessing.scale(features) # scale to (0, 1)\n",
    "\n",
    "    features = np.asarray(features[np.newaxis, :])\n",
    "    features_seq_len = [features.shape[1]]\n",
    "    return features, features_seq_len\n",
    "\n",
    "# audio_file = join('files', 'CTC', 'LDC93S1.wav')\n",
    "audio_file = join('files', 'CTC', '61-70968-0002.wav')\n",
    "\n",
    "train_input, train_seq_len = process_audio(audio_file)\n",
    "val_input, val_seq_len = process_audio(audio_file, do_random_shift=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sparse_tuple_from(sequences, dtype=np.int32):\n",
    "    \"\"\"Create a sparse representation of sequences.\n",
    "    Args:\n",
    "        sequences: a list of lists of type dtype where each element is a sequence\n",
    "    Returns:\n",
    "        A tuple with (indices, values, shape)\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    values = []\n",
    "\n",
    "    for n, seq in enumerate(sequences):\n",
    "        indices.extend(zip([n]*len(seq), range(len(seq))))\n",
    "        values.extend(seq)\n",
    "\n",
    "    indices = np.asarray(indices, dtype=np.int64)\n",
    "    values = np.asarray(values, dtype=dtype)\n",
    "    shape = np.asarray([len(sequences), np.asarray(indices).max(0)[1]+1], dtype=np.int64)\n",
    "\n",
    "    return indices, values, shape\n",
    "\n",
    "def process_targets(text_file):\n",
    "    \"\"\"Extracts the CTC tokens from an input text file.\n",
    "    Args:\n",
    "        text_file: file containing the input text\n",
    "    Returns:\n",
    "        A nested tuple with (original text, (indices, CTC tokens, shape))\n",
    "    \"\"\"\n",
    "    with open(text_file, 'r') as f:\n",
    "        line = f.readlines()[-1] # take the last line\n",
    "\n",
    "        # Get only the words between [a-z], replace period for none\n",
    "        original = ' '.join(line.strip().lower().split(' ')).replace('.', '')\n",
    "        targets = original.replace(' ', '  ')\n",
    "        targets = targets.split(' ')\n",
    "\n",
    "    # Add blank label\n",
    "    targets = np.hstack([SPACE_TOKEN if x == '' else list(x) for x in targets])\n",
    "\n",
    "    # Transform char into index\n",
    "    targets = np.asarray([SPACE_INDEX if x == SPACE_TOKEN else ord(x) - FIRST_INDEX\n",
    "                          for x in targets])\n",
    "\n",
    "    # Creat sparse representation to feed into the graph\n",
    "    return original, sparse_tuple_from([targets])\n",
    "\n",
    "# Generate training and validation targets\n",
    "# target = join('files', 'CTC', 'LDC93S1.txt')\n",
    "target = join('files', 'CTC', '61-70968-0002.txt')\n",
    "train_original, train_targets = process_targets(target)\n",
    "val_original, val_targets = train_original, train_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Builds a CTC graph for speech recognition, using\n",
    "one or more LSTM cells with a CTC loss function.\n",
    "\"\"\"\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    # e.g: log filter bank or MFCC features\n",
    "    # Has size [batch_size, max_stepsize, num_features], but the\n",
    "    # batch_size and max_stepsize can vary along each step\n",
    "    inputs = tf.placeholder(tf.float32, [None, None, num_features])\n",
    "\n",
    "    # Here we use sparse_placeholder that will generate a\n",
    "    # SparseTensor required by ctc_loss op.\n",
    "    targets = tf.sparse_placeholder(tf.int32)\n",
    "\n",
    "    # 1d array of size [batch_size]\n",
    "    seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "    # Defining the cell\n",
    "    # Can be:\n",
    "    #   tf.nn.rnn_cell.RNNCell\n",
    "    #   tf.nn.rnn_cell.GRUCell \n",
    "    cells = []\n",
    "    for _ in range(num_layers):\n",
    "        cell = tf.contrib.rnn.LSTMCell(num_units)\n",
    "        cells.append(cell)\n",
    "    stack = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    # The second output is the last state and we will not use that\n",
    "    outputs, _ = tf.nn.dynamic_rnn(stack, inputs, seq_len, dtype=tf.float32)\n",
    "\n",
    "    shape = tf.shape(inputs)\n",
    "    batch_s, max_timesteps = shape[0], shape[1]\n",
    "\n",
    "    # Reshaping to apply the same weights over the timesteps\n",
    "    outputs = tf.reshape(outputs, [-1, num_hidden])\n",
    "\n",
    "    # Truncated normal with mean 0 and stdev=0.1\n",
    "    W = tf.Variable(tf.truncated_normal([num_hidden,\n",
    "                                         num_classes],\n",
    "                                        stddev=0.1))\n",
    "    # Zero initialization\n",
    "    b = tf.Variable(tf.constant(0., shape=[num_classes]))\n",
    "\n",
    "    # Doing the affine projection\n",
    "    logits = tf.matmul(outputs, W) + b\n",
    "\n",
    "    # Reshaping back to the original shape\n",
    "    logits = tf.reshape(logits, [batch_s, -1, num_classes])\n",
    "\n",
    "    # Time major\n",
    "    logits = tf.transpose(logits, (1, 0, 2))\n",
    "\n",
    "    loss = tf.nn.ctc_loss(targets, logits, seq_len)\n",
    "    cost = tf.reduce_mean(loss)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(initial_learning_rate,\n",
    "                                           0.9).minimize(cost)\n",
    "\n",
    "    # Option 2: tf.nn.ctc_beam_search_decoder\n",
    "    # (it's slower but you'll get better results)\n",
    "    decoded, log_prob = tf.nn.ctc_greedy_decoder(logits, seq_len)\n",
    "\n",
    "    # Inaccuracy: label error rate\n",
    "    ler = tf.reduce_mean(tf.edit_distance(tf.cast(decoded[0], tf.int32),\n",
    "                                          targets))\n",
    "    \n",
    "    # TensorBoard summary operation\n",
    "    tf.summary.scalar('ctc_cost', cost)\n",
    "    tf.summary.scalar('label_error_rate', ler)\n",
    "    summary_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    log_index = int(time.time())\n",
    "    train_summary_writer = tf.summary.FileWriter('./logs/ctc/{}/train'.format(log_index), session.graph)\n",
    "    val_summary_writer = tf.summary.FileWriter('./logs/ctc/{}/val'.format(log_index), session.graph)\n",
    "\n",
    "    # Repeat for num_epochs\n",
    "    for curr_epoch in range(num_epochs):\n",
    "        start = time.time()\n",
    "\n",
    "        feed = {inputs: train_input,\n",
    "                targets: train_targets,\n",
    "                seq_len: train_seq_len}\n",
    "\n",
    "        train_cost, train_ler, _, summary = session.run([cost, ler, optimizer, summary_op], feed_dict=feed)\n",
    "        train_summary_writer.add_summary(summary, curr_epoch)\n",
    "\n",
    "        val_feed = {inputs: val_input,\n",
    "                    targets: val_targets,\n",
    "                    seq_len: val_seq_len}\n",
    "\n",
    "        val_cost, val_ler, val_summary = session.run([cost, ler, summary_op], feed_dict=val_feed)\n",
    "        val_summary_writer.add_summary(val_summary, curr_epoch)\n",
    "        \n",
    "        log = \"Epoch {}/{}, train_cost = {:.3f}, train_ler = {:.3f}, val_cost = {:.3f}, val_ler = {:.3f}, time = {:.3f}\"\n",
    "        print(log.format(curr_epoch+1, num_epochs, train_cost, train_ler,\n",
    "                         val_cost, val_ler, time.time() - start))\n",
    "    # Decoding\n",
    "    d = session.run(decoded[0], feed_dict=val_feed)\n",
    "    str_decoded = ''.join([chr(x) for x in np.asarray(d[1]) + FIRST_INDEX])\n",
    "\n",
    "    # Replacing blank label to none\n",
    "    str_decoded = str_decoded.replace(chr(ord('z') + 1), '')\n",
    "\n",
    "    # Replacing space label to space\n",
    "    str_decoded = str_decoded.replace(chr(ord('a') - 1), ' ')\n",
    "\n",
    "    print('Original:\\n%s' % val_original)\n",
    "    print('Decoded:\\n%s' % str_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Speech\n",
    "\n",
    "- Deep Speech: https://arxiv.org/pdf/1412.5567.pdf\n",
    "- Deep Speech 2: https://arxiv.org/pdf/1512.02595.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DS1 Architecture\n",
    "\n",
    "RNN: 3 non-recurrent and 1 bi-directional recurrent layer, 1 non-recurrent\n",
    "- Input: spectrogram\n",
    "- Output: sequence of _character_ probabilities\n",
    "- Decoding: 5-gram language model\n",
    "- Loss function: CTC\n",
    "- Nesterov's Accelerated Gradient Descent ([evolution of gradient descent](https://medium.com/@ramrajchandradevan/the-evolution-of-gradient-descend-optimization-algorithm-4106a6702d39))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DS1 Architecture\n",
    "\n",
    "<img src='assets/speech/deepspeech-arch.png'/>\n",
    "\n",
    "(image: DeepSpeech paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoding with Language Model\n",
    "\n",
    "Reduce WER by including language model:\n",
    "\n",
    "$\\underset{y}{\\operatorname{argmax}}Q(y) = log(p_ctc(y \\mid x)) + \\alpha * log(p_lm(y)) + \\beta * \n",
    "word\\_count(y)$\n",
    "- $\\alpha$: relative weight of language model vs. CTC\n",
    "- $\\beta$: considers more words\n",
    "\n",
    "Beam search: take $k$ best $y$ on every iteration, similar to breath-first-search but with beam width $k$\n",
    "\n",
    "![decoding](assets/speech/deepspeech-decoding.png)\n",
    "\n",
    "(image: DeepSpeech paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset Construction\n",
    "\n",
    "- Synthesize noisy speech by overlaying tracks\n",
    "- Lombard Effect: play loud background noise while recording so that speaker will raise their pitch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance\n",
    "\n",
    "![performance](assets/speech/deepspeech-performance.png)\n",
    "\n",
    "\n",
    "Dataset: https://catalog.ldc.upenn.edu/LDC2002S23\n",
    "\n",
    "(image: DeepSpeech paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DeepSpeech 2\n",
    "\n",
    "- Same model recognizes multiple languages (English or Mandarin Chinese)\n",
    "   - no separate pronounciation model needed\n",
    "- Parellel GPU training optimizations cut down training & iteration times\n",
    "   - GPU implementation of CTC\n",
    "   - custom memory allocators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DS2 Architecture\n",
    "\n",
    "<img src='assets/speech/deepspeech2-arch.png' style='float:right;' width='50%'/>\n",
    "- More layers (recurrent, convolutional)\n",
    "- Batch Normalization\n",
    "- SortaGrad learning strategy\n",
    "\n",
    "(image: DeepSpeech 2 paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- Original paper: https://arxiv.org/abs/1502.03167\n",
    "\n",
    "$$B(x) = \\gamma \\frac{x - Mean[x]}{(Variance[x] + \\epsilon)^\\frac{1}{2}} + \\beta $$\n",
    "\n",
    "- $Mean[x]$, $Variance[x]$: Minibatch mean and variance\n",
    "- $\\gamma$, $\\beta$: learned parameters\n",
    "- $\\epsilon$: small constant to avoid division by zero\n",
    "- Typically applied before activation $f(x)$:\n",
    "$$h^l = f(B(W^lh^{l-1}))$$\n",
    "- For bidirectional RNNs, Batch Norm is applied sequence-wise:\n",
    "$$h^l_t = f(B(W^lh^{l-1}) + U^lh^l_{t-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization Effect\n",
    "\n",
    "- Generally: speeds up convergence by removing \"uninteresting\" changes in mean and variance (\"internal covariate shift\")\n",
    "- Deep Speech 2: improved generalization error by as much as 12%\n",
    "\n",
    "<img src='assets/speech/deepspeech2-batchnorm.png'/>\n",
    "\n",
    "(image: DeepSpeech 2 paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SortaGrad (Curriculum Strategy)\n",
    "\n",
    "- Longer utterances usually have higher CTC cost than shorter utterances\n",
    "  - Early in training: susceptible to numerical instabilities like underflow, exploding gradient    \n",
    "- Solution\n",
    "  - During the first epoch, sort utterances so that shorter utterances are encountered first\n",
    "  - Subsequent epochs: revert to random order\n",
    "\n",
    "<img src='assets/speech/deepspeech2-sortagrad.png'/>\n",
    "\n",
    "(image: DeepSpeech 2 paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decoding\n",
    "\n",
    "Fuse acoustic model with language model:\n",
    "\n",
    "$$Q(y) = log(p_ctc(y \\mid x)) + \\alpha * log(p_lm(y)) + \\beta * \n",
    "word\\_count(y)$$\n",
    "- $\\alpha$: relative weight of language model vs. CTC\n",
    "- $\\beta$: considers more words\n",
    "\n",
    "Language model\n",
    "- n-gram: scales to large quantities of unlabeled text\n",
    "- English: 850 million n-grams\n",
    "- Mandarin: 2 billion n-grams (character level).\n",
    "   - Output layer is 6000 characters\n",
    "   - includes English alphabets to handle mixed Chinese-English transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset Construction\n",
    "\n",
    "Pipeline\n",
    "- Birectional RNN with CTC to align transcription to audio\n",
    "- Segmentation: splice audio between long silences, average 7s length\n",
    "- Linear classifier to predict bad examples from failed alignments, using crowd sourced ground truth transcriptions\n",
    "\n",
    "Data augmentation by adding noise to 40% of random utterances\n",
    "- too much noise makes optimization difficult\n",
    "- too little noise makes system less robust to noisy speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Performance\n",
    "\n",
    "![performance](assets/speech/deepspeech2-perf.png)\n",
    "\n",
    "(image: DeepSpeech 2 paper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementations\n",
    "\n",
    "Deep Speech 1\n",
    "- https://github.com/mozilla/DeepSpeech\n",
    "\n",
    "Deep Speech 2\n",
    "- https://github.com/NervanaSystems/deepspeech\n",
    "- https://github.com/yao-matrix/deepSpeech2\n",
    "\n",
    "## Datasets\n",
    "\n",
    "- [CHiME](http://spandh.dcs.shef.ac.uk/chime_challenge/index.html)\n",
    "- [Mozilla Common Voice](https://voice.mozilla.org/en/data)\n",
    "- [VoxForge](http://www.voxforge.org)\n",
    "- [AudioSet](https://research.google.com/audioset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Deep Speech Evaluation\n",
    "---\n",
    "|Setup|Requirement|\n",
    "|--|--|\n",
    "|OS|Ubuntu|\n",
    "|GPU|No|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Speech Workshop Setup\n",
    "\n",
    "Repo: https://github.com/mozilla/DeepSpeech\n",
    "\n",
    "DeepSpeech requires Ubuntu, run the following commands from an Ubuntu machine.\n",
    "\n",
    "```\n",
    "conda create --name deepspeech\n",
    "source activate deepspeech\n",
    "pip3 install jupyter matplotlib python_speech_features --user\n",
    "\n",
    "# If running without GPU\n",
    "pip3 install deepspeech\n",
    "\n",
    "# If running on GPU\n",
    "pip3 install deepspeech-gpu\n",
    "\n",
    "# Download pre-trained models (approx. 1.4GB)\n",
    "curl -o deepspeech-0.1.1-models.tar.gz -L https://github.com/mozilla/DeepSpeech/releases/download/v0.1.1/deepspeech-0.1.1-models.tar.gz\n",
    "\n",
    "# Extract\n",
    "tar -xvzf deepspeech-0.1.1-models.tar.gz\n",
    "\n",
    "# Install SoX to convert audio to single channel 16kHz PCM\n",
    "# needed by DeepSpeech\n",
    "sudo apt-get install -y sox\n",
    "\n",
    "# Launch Jupyter and browse to this notebook\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Ref: https://github.com/mozilla/DeepSpeech/blob/master/native_client/python/client.py\n",
    "from deepspeech.model import Model\n",
    "import numpy as np\n",
    "\n",
    "# Model\n",
    "MODEL_GRAPH = 'models/output_graph.pb'\n",
    "LANGUAGE_MODEL = 'models/lm.binary'\n",
    "LM_ALPHABET = 'models/alphabet.txt'\n",
    "LM_TRIE = 'trie'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Processing\n",
    "\n",
    "We'll define a helper function `prepare_input` that prepares the input to match the training data characteristics:\n",
    "- Sample rate of 16kHz\n",
    "- Duration of 6 seconds\n",
    "\n",
    "We'll also define another function `visualize_input` that plots the spectrogram of the audio sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def prepare_input(audio_path):\n",
    "    \"\"\"Ensures audio matches the sample rate supported by Deep Speech,\n",
    "       and truncates the audio to the required length.\n",
    "    Args:\n",
    "        audio_path: path ot the audio file\n",
    "    Returns:\n",
    "        A tuple with (sample_rate, converted_audio)    \n",
    "    \"\"\"\n",
    "    import subprocess\n",
    "    import scipy.io.wavfile as wav\n",
    "    sample_rate = 16000\n",
    "    duration_secs = 6\n",
    "\n",
    "    fs, audio = wav.read(audio_path)\n",
    "    if fs != sample_rate:\n",
    "        if fs < sample_rate:\n",
    "            print('Warning: original sample rate (%d) is lower than 16kHz. Up-sampling might produce erratic speech recognition.' % (fs), file=sys.stderr)\n",
    "    \n",
    "        print('Resampling audio from {}kHz to 16kHz'.format(fs/1000))\n",
    "        sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate {} - '.format(audio_path, sample_rate)\n",
    "        try:\n",
    "            p = subprocess.Popen(sox_cmd.split(),\n",
    "                                 stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "            output, err = p.communicate()\n",
    "\n",
    "            if p.returncode:\n",
    "                raise RuntimeError('SoX returned non-zero status: {}'.format(err))\n",
    "\n",
    "        except OSError as e:\n",
    "            raise OSError('SoX not found, use 16kHz files or install it: ', e)\n",
    "\n",
    "        audio = np.fromstring(output, dtype=np.int16)\n",
    "\n",
    "    # Truncate to smaller length, as DeepSpeech is trained on\n",
    "    # short utterances (a few seconds long)\n",
    "    print('Truncating audio from {}s to {}s'.format(len(audio)/sample_rate, duration_secs))\n",
    "\n",
    "    max_frames = sample_rate * duration_secs\n",
    "    audio = audio[:max_frames]\n",
    "    return sample_rate, audio\n",
    "\n",
    "def visualize_input(sample_rate, audio, title):\n",
    "    \"\"\"Plots an audio spectrogram and shows a player for it\n",
    "    Args:\n",
    "        sample_rate: sample rate\n",
    "        audio: audio sample buffer\n",
    "        title: plot title\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    from python_speech_features import mfcc\n",
    "    from scipy.io.wavfile import read\n",
    "    \n",
    "    # plot spectrogram\n",
    "    features = mfcc(audio, sample_rate, winlen=0.025, winstep=0.01,\n",
    "                    numcep=26, appendEnergy=False)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(20,4))\n",
    "    cax = ax.matshow(np.transpose(features), interpolation='nearest',\n",
    "                     aspect='auto', cmap='plasma', origin='lower')\n",
    "    fig.colorbar(cax)\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Test input\n",
    "Just for fun, we'll try a corpus of Singapore English.\n",
    "\n",
    "You can find examples from the [NIE Corpus of Spoken Singapore English](http://videoweb.nie.edu.sg/phonetic/niecsse/index.htm)\n",
    "\n",
    "For comparison, we'll generate a US English sample using an [online text-to-speech generator](https://www.text2speech.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Try something from the NIE Corpus of Spoken Singapore English\n",
    "# http://videoweb.nie.edu.sg/phonetic/niecsse/index.htm\n",
    "transcription = 'the north wind and the sun were disputing which was the stronger when a traveler came along'\n",
    "\n",
    "# Singaporean English\n",
    "!curl -o singaporean.wav -L 'http://videoweb.nie.edu.sg/phonetic/niecsse/nws/nws-SE-f1.WAV'\n",
    "\n",
    "# ==================================================================\n",
    "# American English\n",
    "# 1. Go to https://www.text2speech.org in your browser\n",
    "# 2. Generate a speech sample using the transcription text above\n",
    "# 3. Replace the link to the .wav\n",
    "!curl -o american.wav -L 'PASTE_YOUR_LINK_HERE'\n",
    "\n",
    "\n",
    "\n",
    "sample_rate1, audio1 = prepare_input('singaporean.wav')\n",
    "sample_rate2, audio2 = prepare_input('american.wav')\n",
    "\n",
    "visualize_input(sample_rate1, audio1, \"Singaporean English\")\n",
    "visualize_input(sample_rate2, audio2, \"American English\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Perform the inference\n",
    "- Load the acoustic model\n",
    "- Enable language model decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "# These constants control the beam search decoder\n",
    "# Beam width used in the CTC decoder when building candidate transcriptions\n",
    "BEAM_WIDTH = 500\n",
    "\n",
    "# The alpha hyperparameter of the CTC decoder. Language Model weight\n",
    "LM_WEIGHT = 1.75\n",
    "\n",
    "# The beta hyperparameter of the CTC decoder. Word insertion weight (penalty)\n",
    "WORD_COUNT_WEIGHT = 1.00\n",
    "\n",
    "# Valid word insertion weight. This is used to lessen the word insertion penalty\n",
    "# when the inserted word is part of the vocabulary\n",
    "VALID_WORD_COUNT_WEIGHT = 1.00\n",
    "\n",
    "# These constants are tied to the shape of the graph used (changing them changes\n",
    "# the geometry of the first layer), so make sure you use the same constants that\n",
    "# were used during training\n",
    "\n",
    "# Number of MFCC features to use\n",
    "N_FEATURES = 26\n",
    "\n",
    "# Size of the context window used for producing timesteps in the input vector\n",
    "N_CONTEXT = 9\n",
    "\n",
    "# Load the acoustic model\n",
    "ds = Model(MODEL_GRAPH, N_FEATURES, N_CONTEXT, LM_ALPHABET, BEAM_WIDTH)\n",
    "print('Loaded model')\n",
    "\n",
    "# Load the language model\n",
    "ds.enableDecoderWithLM(LM_ALPHABET, LANGUAGE_MODEL, LM_TRIE, LM_WEIGHT,\n",
    "                       WORD_COUNT_WEIGHT, VALID_WORD_COUNT_WEIGHT)\n",
    "\n",
    "print('Loaded language model')\n",
    "\n",
    "# ==================================================================\n",
    "# Exercise:\n",
    "# Add code below to get the evaluation result for both audio samples\n",
    "# \n",
    "# Use ds.stt(), see an example at:\n",
    "# https://github.com/mozilla/DeepSpeech/blob/master/native_client/python/client.py\n",
    "\n",
    "result1 = '' # replace this with your code\n",
    "\n",
    "\n",
    "\n",
    "result2 = '' # replace this with your code\n",
    "\n",
    "\n",
    "print('Original:', transcription)\n",
    "print('Singaporean:', result1)\n",
    "print('American:', result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate metrics\n",
    "\n",
    "We'll compute the edit distance of the two text transcriptions (the actual vs. predicted) to determine how far apart they are.\n",
    "\n",
    "Refer to http://www.nltk.org/howto/metrics.html for the available metrics, and complete the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(truth, prediction):\n",
    "    \"\"\"Returns a metric indicating how good the prediction is\n",
    "       This should be the word error rate.\n",
    "    Args:\n",
    "        prediction: the prediction\n",
    "        truth: the correct value\n",
    "    \"\"\"\n",
    "    # ==================================================================\n",
    "    # Complete the code below, using nltk.metrics.edit_distance\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    pass\n",
    "    \n",
    "print('Singaporean:', evaluate_metrics(transcription, result1))\n",
    "print('American:', evaluate_metrics(transcription, result2))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Deep Speech Training\n",
    "---\n",
    "|Setup|Requirement|\n",
    "|--|--|\n",
    "|OS|Ubuntu|\n",
    "|GPU|Recommended|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What to expect\n",
    "\n",
    "Training Deep Speech can take a while (at least a couple of days), so the goal of this workshop is to get you familiarized with the training process using a very tiny dataset.\n",
    "\n",
    "We'll also cover fine-tuning an existing model with new data.\n",
    "\n",
    "### Sections\n",
    "1. [Setting up training environment](speech.ipynb#Training-Environment)\n",
    "2. [Sanity checking by overfitting small dataset](speech.ipynb#Overfit-Small-Dataset)\n",
    "3. [Fine-tuning an existing model](speech.ipynb#Fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Training Environment\n",
    "\n",
    "- Documentation: https://github.com/mozilla/DeepSpeech#training\n",
    "- Ubuntu + GPU is required for actual training.\n",
    "- Even if you don't have a GPU machine readily available, you should try this using CPU. \n",
    "\n",
    "### Create environment\n",
    "```\n",
    "conda create --name deepspeech-train\n",
    "source activate deepspeech-train\n",
    "conda install pip\n",
    "```\n",
    "### Clone\n",
    "```\n",
    "git clone https://github.com/mozilla/DeepSpeech.git\n",
    "```\n",
    "\n",
    "### Install requirements\n",
    "```\n",
    "cd DeepSpeech\n",
    "pip install -r requirements.txt\n",
    "python util/taskcluster.py --target .\n",
    "\n",
    "# Install libdeepspeech.so, otherwise training scripts will emit warning about it\n",
    "pip install deepspeech\n",
    "\n",
    "## hack to get the provided training scripts to work\n",
    "cd native_client\n",
    "ln -s libctc_decoder_with_kenlm.so .\n",
    "cd ..\n",
    "```\n",
    "\n",
    "### If running on GPU: install TensorFlow GPU\n",
    "```\n",
    "pip uninstall tensorflow --user\n",
    "pip install 'tensorflow-gpu==1.6.0' --user\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Overfit Small Dataset\n",
    "\n",
    "First, we'll validate our setup by overfitting a small dataset. We'll use the toy LDC93S1 dataset, which contains just 1 utterance.\n",
    "\n",
    "Open `bin/run-ldc93s1.sh` in an editor and inspect it. It contains two parts:\n",
    "1. Downloading and importing the dataset\n",
    "2. Training using `Deepspeech.py`\n",
    "\n",
    "We'll use this script as a reference for setting up DeepSpeech training for other datasets.\n",
    "\n",
    "### Training\n",
    "\n",
    "Start training from the `DeepSpeech` top level directory:\n",
    "```\n",
    "bin/run-ldc93s1.sh\n",
    "```\n",
    "\n",
    "Even without a GPU, this should take less than 10 minutes to complete.\n",
    "\n",
    "```\n",
    "I FINISHED Optimization - training time: 0:01:08\n",
    "I Test of Epoch 50 - WER: 0.818182, loss: 12.298973083496094, mean edit distance: 0.173077\n",
    "I --------------------------------------------------------------------------------\n",
    "I WER: 0.818182, loss: 12.298973, mean edit distance: 0.173077\n",
    "I  - src: \"she had your dark suit in greasy wash water all year\"\n",
    "I  - res: \"she had yorodarksuitingreasywashwaterallyear\"\n",
    "I --------------------------------------------------------------------------------\n",
    "```\n",
    "\n",
    "### Training output and TensorBoard\n",
    "By default, training only saves checkpoint files. The default location is `$HOME/.local/share/deepspeech/ldc93s1`.\n",
    "\n",
    "This includes TensorBoard log files. To explore the TensorBoard graphs, you can run:\n",
    "\n",
    "```\n",
    "tensorboard --logdir $HOME/.local/share/deepspeech/ldc93s1\n",
    "```\n",
    "\n",
    "Follow the prompts to open the browser to http://localhost:6006. Discover something cool like the PCA projection of the gate weights of an LSTM cell:\n",
    "\n",
    "![pca_ldc93s1_lstm.gif](assets/speech/pca_ldc93s1_lstm.gif)\n",
    "\n",
    "\n",
    "### Exporting / freezing models\n",
    "\n",
    "To fine tune a model or release the model for others to use, convert the checkpoint files to a model file.  This is done by passing the --export_dir flag to the training script.\n",
    "\n",
    "Run this from the `DeepSpeech` top level directory:\n",
    "```\n",
    "mkdir models\n",
    "bin/run-ldc93s1.sh --export_dir models\n",
    "```\n",
    "\n",
    "When done, you should see `output_graph.pb` in the `models` directory. We will need it for the next section.\n",
    "\n",
    "### Checkpointing as a Best Practice\n",
    "Note that the training script stores state in the checkpoint directory, so that it resumes from the previous checkpointed state upon reruns. This is critical for long running training sessions where the system may crash. Checkpoints allow the training to recover and resume."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Now that we've trained our toy model, let's practice fine tuning it.  We'll try some new data from the NIE Singapore English Corpus.\n",
    "\n",
    "### Data Processing\n",
    "\n",
    "The data needs to be processed to 16kHz and chopped into 6-7 second utterances. \n",
    "\n",
    "The transcription also needs to be split and aligned with the data. This is done manually by listening to each chopped up wav file and finding the matching transcription. \n",
    "\n",
    "Full text:\n",
    "\n",
    "```\n",
    "The North Wind and the Sun were disputing which was the stronger when a traveler came along wrapped in a warm cloak. They agreed that the one who first succeeded in making the traveller take his cloak off should be considered stronger than the other. Then the North Wind blew as hard as he could, but the more he blew the more closely did the traveller fold his cloak around him; and at last the North Wind gave up the attempt. Then the Sun shone out warmly, and immediately the traveller took off his cloak. And so the North Wind was obliged to confess that the Sun was the stronger of the two.\n",
    "```\n",
    "\n",
    "In practice, getting the transcription truth is one of the more time-consuming parts of data gathering. This is usually done by humans through crowdsourcing efforts like the Common Voice Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.Audio(\"http://videoweb.nie.edu.sg/phonetic/niecsse/nws/nws-SE-f1.WAV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Running the import script\n",
    "\n",
    "Save the script below in the `DeepSpeech` top level directory and run it.\n",
    "```\n",
    "import_nie.py\n",
    "```\n",
    "\n",
    "When done, your data set should look like:\n",
    "```\n",
    "data/nws-SE-f1-test.csv      data/nws-SE-f1-train.csv\n",
    "data/nws-SE-f1_chunk001.wav  data/nws-SE-f1_chunk004.wav\n",
    "data/nws-SE-f1_chunk002.wav  data/nws-SE-f1_chunk005.wav\n",
    "data/nws-SE-f1_chunk003.wav  data/nws-SE-f1_chunk006.wav\n",
    "data/nws-SE-f1_chunk007.wav  \n",
    "```\n",
    "\n",
    "Note `chunk007` will not be used for training or testing (it contains only silence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"Importer script to create a sample NIE Singapore English Speech DeepSpeech dataset\n",
    "\n",
    "Save this script to the DeepSpeech directory and run it without any arguments:\n",
    "   import_nie.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pandas\n",
    "import subprocess\n",
    "\n",
    "def _resample_and_split_audio(audio_path, sample_rate=16000, chunksize_secs=6):\n",
    "    \"\"\"Resamples and splits the input audio into chunks.\n",
    "    Args:\n",
    "        audio_path: path to the input audio.\n",
    "        sample_rate: target sample rate (Hz).\n",
    "        chunksize_secs: target chunk size (seconds).\n",
    "    Returns:\n",
    "        Array of paths to the split audio.\n",
    "    \"\"\"\n",
    "    import glob\n",
    "    import scipy.io.wavfile as wav\n",
    "    audio_path_noext = os.path.splitext(audio_path)[0]\n",
    "\n",
    "    fs, _ = wav.read(audio_path)\n",
    "    if fs != sample_rate:\n",
    "        if fs < sample_rate:\n",
    "            print('Warning: original sample rate ({}) is lower than {}Hz. \\\n",
    "                Up-sampling might produce erratic speech recognition.'.\\\n",
    "                format(fs, sample_rate), file=sys.stderr)\n",
    "\n",
    "    print('Resampling audio to {}Hz'.format(sample_rate))\n",
    "\n",
    "    # SoX reference: http://sox.sourceforge.net/sox.html\n",
    "    # sox foo.wav --rate 16000 --bits 16 --channels 1 foo_resampled.wav\n",
    "    sox_cmd = 'sox {} --rate {} --bits 16 --channels 1 {}_resampled.wav'.\\\n",
    "        format(audio_path, sample_rate, audio_path_noext)\n",
    "    try:\n",
    "        p = subprocess.Popen(sox_cmd.split(),\n",
    "                             stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        _, err = p.communicate()\n",
    "\n",
    "        if p.returncode:\n",
    "            raise RuntimeError('SoX returned non-zero status: {}'.format(err))\n",
    "\n",
    "        print('Splitting audio into {}s chunks'.format(chunksize_secs))\n",
    "\n",
    "        # sox foo_resampled.wav foo_chunk.wav trim 0 6 : newfile : restart\n",
    "        sox_cmd = 'sox {}_resampled.wav {}_chunk.wav trim 0 {} : newfile : restart'.\\\n",
    "            format(audio_path_noext, audio_path_noext, chunksize_secs)\n",
    "        p = subprocess.Popen(sox_cmd.split(),\n",
    "                             stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n",
    "        _, err = p.communicate()\n",
    "\n",
    "        if p.returncode:\n",
    "            raise RuntimeError('SoX returned non-zero status: {}'.format(err))\n",
    "    except OSError as e:\n",
    "        raise OSError('SoX not found, use {}Hz files or install it: '.format(sample_rate), e)\n",
    "\n",
    "    # list of chunked files\n",
    "    return sorted(glob.glob('{}_chunk*.wav'.format(audio_path_noext)))\n",
    "\n",
    "def maybe_download(filename, work_directory, source_url):\n",
    "  \"\"\"Download the data from source url, unless it's already here.\n",
    "  Args:\n",
    "      filename: string, name of the file in the directory.\n",
    "      work_directory: string, path to working directory.\n",
    "      source_url: url to download from if file doesn't exist.\n",
    "  Returns:\n",
    "      Path to resulting file.\n",
    "  \"\"\"\n",
    "  import shutil\n",
    "  import urllib\n",
    "\n",
    "  if not os.path.exists(work_directory):\n",
    "      os.makedirs(work_directory)\n",
    "  filepath = os.path.join(work_directory, filename)\n",
    "\n",
    "  if not os.path.exists(filepath):\n",
    "    temp_file_name, _ = urllib.request.urlretrieve(source_url, filename)\n",
    "    shutil.move(temp_file_name, filepath)\n",
    "    print('Successfully downloaded', filename, os.path.getsize(filepath), 'bytes.')\n",
    "  return filepath\n",
    "\n",
    "def _download_and_preprocess_data(data_dir):\n",
    "    import re\n",
    "    NWS_SE_F1_WAV = \"nws-SE-f1.wav\"\n",
    "    NWS_SE_F1_WAV_URL = \"http://videoweb.nie.edu.sg/phonetic/niecsse/nws/nws-SE-f1.WAV\"\n",
    "\n",
    "    # transcript that's pre-chunked into 6 second speech.\n",
    "    # this is done by manually playing back each chunk.\n",
    "    NWS_SE_F1_TXT = [\n",
    "        \"The North Wind and the Sun were disputing which was the stronger when a traveler came along\",\n",
    "        \"wrapped in a warm cloak. They agreed that the one who first succeeded in making the traveller take his cloak\",\n",
    "        \"off should be considered stronger than the other. Then the North Wind blew as hard as he could, but the\",\n",
    "        \"more he blew the more closely did the traveller fold his cloak around him; and at last the North Wind gave up the\",\n",
    "        \"attempt. Then the Sun shone out warmly, and immediately the traveller\",\n",
    "        \"took off his cloak. And so the North Wind was obliged to confess that the Sun was the stronger of the two.\",\n",
    "        \" \"] # silence\n",
    "\n",
    "    local_file = maybe_download(NWS_SE_F1_WAV, data_dir, NWS_SE_F1_WAV_URL)\n",
    "\n",
    "    # lowercase, remove extra spacing\n",
    "    transcripts = [text.strip().lower() for text in NWS_SE_F1_TXT]\n",
    "\n",
    "    # remove anything other than characters and space\n",
    "    transcripts = [re.sub(r'[^\\w\\s]', '', text) for text in transcripts]\n",
    "\n",
    "    # process the audio files\n",
    "    audio_files = _resample_and_split_audio(local_file)\n",
    "\n",
    "    # sanity check\n",
    "    if len(audio_files) != len(transcripts):\n",
    "        raise RuntimeError('number of chunked files ({}) != number of transcripts ({})'. \\\n",
    "            format(len(audio_files) , len(transcripts)))\n",
    "\n",
    "    # drop silence (last transcript)\n",
    "    audio_files = [audio_files[i] for i in range(0, len(transcripts)) if len(transcripts[i]) > 0]\n",
    "\n",
    "    # create the data frame, reserving last 2 entries as test data\n",
    "    data = [(os.path.abspath(audio_files[i]), os.path.getsize(audio_files[i]), transcripts[i])\n",
    "        for i in range(0, len(audio_files))]\n",
    "\n",
    "    df = pandas.DataFrame(data=data[:len(data)-2],\n",
    "                          columns=[\"wav_filename\", \"wav_filesize\", \"transcript\"])\n",
    "    df.to_csv(os.path.join(data_dir, \"nws-SE-f1-train.csv\"), index=False)\n",
    "    print(\"Created:\", os.path.join(data_dir, \"nws-SE-f1-train.csv\"))\n",
    "\n",
    "    df = pandas.DataFrame(data=data[len(data)-2:],\n",
    "                          columns=[\"wav_filename\", \"wav_filesize\", \"transcript\"])\n",
    "    df.to_csv(os.path.join(data_dir, \"nws-SE-f1-test.csv\"), index=False)\n",
    "    print(\"Created:\", os.path.join(data_dir, \"nws-SE-f1-test.csv\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _download_and_preprocess_data(data_dir='data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Fine-tuning from a frozen model\n",
    "\n",
    "Ref: https://github.com/mozilla/DeepSpeech#continuing-training-from-a-frozen-graph\n",
    "\n",
    "Run these commands that will:\n",
    "- Initialize from the model that we saved earlier\n",
    "- Save the checkpoints to a new checkpoint location for this fine-tuning session\n",
    "- Run the fine-tuning session for 50 epochs with a small learning rate\n",
    "\n",
    "```\n",
    "mkdir fine_tuning_checkpoints\n",
    "\n",
    "python -u DeepSpeech.py \\\n",
    "  --n_hidden 494 \\\n",
    "  --initialize_from_frozen_model models/output_graph.pb \\\n",
    "  --checkpoint_dir fine_tuning_checkpoints \\\n",
    "  --train_files data/nws-SE-f1-train.csv \\\n",
    "  --dev_files data/nws-SE-f1-train.csv \\\n",
    "  --test_files data/nws-SE-f1-test.csv \\\n",
    "  --learning_rate 0.0001\n",
    "  --epoch 50 \\\n",
    "```\n",
    "\n",
    "You should see output that looks like:\n",
    "```\n",
    "(deepspeech-train) $ python -u DeepSpeech.py \\\n",
    ">   --n_hidden 494 \\\n",
    ">   --initialize_from_frozen_model models/output_graph.pb \\\n",
    ">   --checkpoint_dir fine_tuning_checkpoints \\\n",
    ">   --train_files data/nws-SE-f1-train.csv \\\n",
    ">   --dev_files data/nws-SE-f1-train.csv \\\n",
    ">   --test_files data/nws-SE-f1-test.csv \\\n",
    ">   --learning_rate 0.0001\n",
    "W Parameter --validation_step needs to be >0 for early stopping to work\n",
    "I Initializing from frozen model: models/output_graph.pb\n",
    "I STARTING Optimization\n",
    "I Training of Epoch 0 - loss: 748.212051\n",
    "I Training of Epoch 1 - loss: 630.433716\n",
    "I Training of Epoch 2 - loss: 566.069580\n",
    "I Training of Epoch 3 - loss: 512.083084\n",
    "I Training of Epoch 4 - loss: 472.369240\n",
    "I Training of Epoch 5 - loss: 433.883522\n",
    "I Training of Epoch 6 - loss: 403.633568\n",
    "I Training of Epoch 7 - loss: 373.490967\n",
    "I Training of Epoch 8 - loss: 346.342918\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Practice Exercises\n",
    "\n",
    "- Try fine-tuning with different hyper-parameters, such as learning rate. Note that the number of hidden layers is fixed by the frozen model\n",
    "- Try training an initial model with a Singapore English sample instead, then fine-tuning it with more Singapore English samples\n",
    "- Investigate creating a language model for Singapore English to improve the transcription result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
   "autolaunch": true,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
