{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "A survey of statistical and deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's a time series\n",
    "\n",
    "A set of observations at regular time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HDB Resale Price Index (Univariate)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"https://data.gov.sg/dataset/hdb-resale-price-index/resource/52e93430-01b7-4de0-80df-bc83d0afed40/view/14c47d07-1395-4661-8466-728abce27f5f\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x215f302cba8>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Source: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "print(\"HDB Resale Price Index (Univariate)\")\n",
    "IFrame('https://data.gov.sg/dataset/hdb-resale-price-index/resource/52e93430-01b7-4de0-80df-bc83d0afed40/view/14c47d07-1395-4661-8466-728abce27f5f', width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annual Average Weekly Paid Hours Worked Per Employee, By Industry And Type Of Employment (Multi-variate)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"600\"\n",
       "            height=\"400\"\n",
       "            src=\"https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual/resource/dd2147ce-20b7-401c-ac8f-8dbcc0d8e0d9/view/431f44b1-e58d-4131-998d-7e7aeee14479\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x215f302cbe0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Source: https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual\n",
    "print(\"Annual Average Weekly Paid Hours Worked Per Employee, By Industry And Type Of Employment (Multi-variate)\")\n",
    "IFrame('https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual/resource/dd2147ce-20b7-401c-ac8f-8dbcc0d8e0d9/view/431f44b1-e58d-4131-998d-7e7aeee14479', width='600', height='400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Series Decomposition\n",
    "\n",
    "- For analysis\n",
    "- Split into trend, cyclic, seasonal, noise\n",
    "- Multiplicative or additive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: Time Series Decomposition\n",
    "1. Go to https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "2. Click on the `Download` button\n",
    "3. Unzip and extract the `.csv` file. Note that path to that file for use later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use the [StatsModels](https://www.statsmodels.org/stable/index.html) Python library to perform the decomposition. \n",
    "\n",
    "This library will also be used later to perform statistical analysis. [Pandas](https://pandas.pydata.org/pandas-docs/stable/timeseries.html) is also useful for manipulating time series data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Walkthrough: Time Series Decomposition\"\"\"\n",
    "\n",
    "#!pip install statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = 'D:\\\\tmp\\\\hdb-resale-price-index\\\\housing-and-development-board-resale-price-index-1q2009-100-quarterly.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# print first few rows to make sure we loaded it properly\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### DateTimeIndex\n",
    "\n",
    "From the table above, we can confirm it is indeed a quarterly distribution. \n",
    "\n",
    "`seasonal_decompose` ([docs](http://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html)) expects a DateTimeIndex to indicate the frequency of the time series. \n",
    "\n",
    "To enable this, we've set `parse_dates=True` and `index_col=0` so that the `DataFrame.index` property is correctly set as a `pandas.DatetimeIndex` ([docs](https://pandas.pydata.org/pandas-docs/stable/timeseries.html#generating-ranges-of-timestamps)), with a quarterly frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the index.\n",
    "# note that this is different from the \"index\" column in the DataFrame,\n",
    "# which refers to the Resale Price Index.\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# plot the raw data, how that we have a DatetimeIndex\n",
    "ax = df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Additive vs. Multiplicative Decomposition\n",
    "\n",
    "Now we are ready to decompose.  Let's try two models: `additive` and `multiplicative`.\n",
    "\n",
    "You can find the definitions in the [StatsModels documentation](http://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html), but in a nutshell, the `additive` model \"adds up\" the components, while the `multiplicative` model \"multiplies\" the components together.\n",
    "\n",
    "Additive: $$y(t) = Trend + Seasonal + Residual$$\n",
    "\n",
    "Mulplicative: $$y(t) = Trend * Seasonal * Residual$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# it's a bit confusing, but `index` here refers to the HDB resale price index,\n",
    "# not the DataFrame.index (for time series)\n",
    "additive = seasonal_decompose(df, model='additive')\n",
    "additive.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "multiplicative = seasonal_decompose(df, model='multiplicative')\n",
    "multiplicative.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Observations\n",
    "\n",
    "- There is a strong seasonal component. The component values are available as part of the object returned by `seasonal_decompose`\n",
    "- The multiplicative model seems like a better fit, because the resulting residuals are smaller. In general, residuals (noise) are difficult to fit statistically.\n",
    "\n",
    "## Why decompose?\n",
    "- The purpose of decomposition is to \"subtract\" known variability and simpilify creating models. For example, we can ignore the seasonal component and focus on predicting the trend.\n",
    "- `series_decompose` uses a naive implementation. For a more advanced decomposition library, see [STLDecompose]( https://github.com/jrmontag/STLDecompose/blob/master/STL%20usage%20example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# print out the first few elements of each seasonal component\n",
    "print('Multiplicative model: Seasonal component')\n",
    "print(multiplicative.seasonal[:10])\n",
    "\n",
    "print('Multiplicative model: Trend component')\n",
    "print(multiplicative.trend[:10])\n",
    "\n",
    "print('Additive model: Seasonal component')\n",
    "print(additive.seasonal[:10])\n",
    "\n",
    "print('Additive model: Trend component')\n",
    "print(additive.trend[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Statistical: Auto-regressive, Moving average models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Deep Learning: LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Auto-regressive models\n",
    "\n",
    "$AR(p): X_t=c + \\sum_{i=1}^p \\varphi_iX_{t-i} + \\varepsilon_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output ($X_t$) depends on past values ($X_{t-i}$) + white noise ($\\varepsilon_t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting auto-regressive models\n",
    "\n",
    "1. Use Autocorrelation to pick parameter $p$, which indicates how far back in time $X_{t-i}$ should go\n",
    "2. Train model to fit data, minimizing mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Auto-regression\n",
    "\n",
    "In this workshop, we'll take the dataset we've been working with and see if we can train an auto-regressive model for it.\n",
    "\n",
    "Credits: borrowed heavily from https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workshop: Auto-regression\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = 'D:\\\\tmp\\\\hdb-resale-price-index\\\\housing-and-development-board-resale-price-index-1q2009-100-quarterly.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# print first few rows to make sure we loaded it properly\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Lag plots\n",
    "\n",
    "A quick first test is to check if the data is random. If random, the data will not exhibit a structure in the lag plot.\n",
    "\n",
    "Let's try [`pandas.plotting.lag_plot`](https://pandas.pydata.org/pandas-docs/stable/visualization.html#lag-plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# try to see if there are correlations between X(t) and X(t-1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20,10))\n",
    "df.plot(ax=ax1) # series plot\n",
    "pd.plotting.lag_plot(df['index']) # lag plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As a comparison, here's what the lag plot will look like for a random series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# generate a random series\n",
    "random_df = pd.DataFrame(np.random.randint(0, 100, size=(100, 1)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20,10))\n",
    "random_df.plot(ax=ax1) # series plot\n",
    "pd.plotting.lag_plot(random_df, ax=ax2) # lag plot (shows no correlation!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, it does look like there is correlation with the current and previous value of the HDB resale price index.\n",
    "\n",
    "We are ready to move to the next step.\n",
    "\n",
    "## Auto-correlation plots\n",
    "The next test is use auto-correlation to pick a good value of `p` to use for the equation:\n",
    "\n",
    "$AR(p): X_t=c + \\sum_{i=1}^p \\varphi_iX_{t-i} + \\varepsilon_t$\n",
    "\n",
    "We will use the `pandas.plotting.autocorrelation_plot`: https://pandas.pydata.org/pandas-docs/stable/visualization.html#autocorrelation-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(df['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Again, it is helpful to compare with a random series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Interpreting Auto-correlation plots\n",
    "\n",
    "The [docs](https://pandas.pydata.org/pandas-docs/stable/visualization.html#autocorrelation-plot) have a good explanation that we'll summarize here:\n",
    "\n",
    "- Auto-correlation plots show the auto-correlation at different time lags (`p`)\n",
    "- Random time series: the auto-correlations always hover around zero\n",
    "- Non-random time series: find values of Lag where auto-correlations are outside of the 95% or 99% confidence band.\n",
    "  - 95%: solid line\n",
    "  - 99%: dashed line\n",
    "- Both negative and positive auto-correlations are valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Auto-regressive model\n",
    "\n",
    "Since the dataset is about HDB quarterly resale prices, we can develop a model to predict the resale prices for the next year (4 quarters).\n",
    "\n",
    "Recall that an Auto-regessive model uses the past values to predict the next value in the series:\n",
    "\n",
    "$$AR(p): X_t=c + \\sum_{i=1}^p \\varphi_iX_{t-i} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "Before we fit the AR model, it's a good idea to create a baseline model.\n",
    "\n",
    "The simplest possible one is a \"Persistence Model\", which just predicts the current value based on the last observation.\n",
    "\n",
    "$$X_t = X_{t-1}$$\n",
    "\n",
    "### Predictions using a Persistence Model\n",
    "1. The time series is a vector of (t, X) values. To model it, we will time-shift the dataset to produce two vectors (x, y).\n",
    "    - input: $x = X_t$\n",
    "    - output: $y = X_{t+1}$\n",
    "2. We'll then split the (x, y) dataset into train and test\n",
    "3. Run the baseline model to get predictions:\n",
    "$$\\hat{y} = BaselineModel(x)$$ Where: $$BaselineModel(x) = x$$\n",
    "\n",
    "4. Then compute the means square error:\n",
    "$$score = MSE(y, \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_lagged_dataset(data):\n",
    "    \"\"\"Creates the dataset used for time-series modeling\n",
    "    Args:\n",
    "        data: the time series data (X)\n",
    "    Returns:\n",
    "        the lagged dataset (X[t], X[t+1]) as a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    values = pd.DataFrame(data)\n",
    "    df = pd.concat([values.shift(1), values], axis=1) # concat in the column axis\n",
    "    df.columns = ['t-1', 't']\n",
    "    return df\n",
    "    \n",
    "def split_train_test(data, split_percent=10):\n",
    "    \"\"\"Splits the dataset into train and test sets\n",
    "    Note that for time series data, we should not use random split\n",
    "    because sequence is important.\n",
    "    We'll just split by taking the last split_percent values\n",
    "    \n",
    "    Args:\n",
    "        data: the dataset (x, y)\n",
    "    Returns:\n",
    "        tuple: (train, test) as pandas.DataFrames\n",
    "    \"\"\"\n",
    "    split_index = int(((1-split_percent)/100) * data.shape[0])\n",
    "    \n",
    "    x = data.values\n",
    "    train, test = x[1:split_index], x[split_index:]\n",
    "    \n",
    "    train_df = pd.DataFrame.from_records(train, columns=['x', 'y'])\n",
    "    test_df = pd.DataFrame.from_records(test, columns=['x', 'y'])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def baseline_model(x):\n",
    "    \"\"\"Baseline model which gives the provided value\n",
    "    It implements: f(x) = x\n",
    "    Args:\n",
    "        x: the value\n",
    "    Returns:\n",
    "        the provided value\n",
    "    \"\"\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create datasets\n",
    "dataset = create_lagged_dataset(df['index'])\n",
    "print(\"\\nDataset:\\n\", dataset.head(5))\n",
    "\n",
    "train, test = split_train_test(dataset)\n",
    "print(\"\\nTrain:\\n\", train.head(5))\n",
    "print(\"\\nTest:\\n\", test.head(5))\n",
    "\n",
    "# walk-forward validation using baseline model: f(x) = x\n",
    "baseline_predictions = [baseline_model(x) for x in test['x']]\n",
    "\n",
    "# plot the predictions vs truth\n",
    "_, ax = plt.subplots(figsize=(20,10))\n",
    "f1, = ax.plot(test['y'])\n",
    "f2, = ax.plot(baseline_predictions, color='red')\n",
    "ax.legend([f1, f2], ['truth', 'baseline prediction'])\n",
    "plt.show()\n",
    "\n",
    "score = mean_squared_error(test['y'], baseline_predictions)\n",
    "print(\"MSE:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Auto-regression model\n",
    "\n",
    "Now that we have our baseline model in place, we can proceed with Auto-regression.\n",
    "\n",
    "We'll use the [`statsmodels.tsa.ar_model.AR`](http://www.statsmodels.org/devel/generated/statsmodels.tsa.ar_model.AR.html)\n",
    "\n",
    "Unlike the baseline model where we have to create a lagged time series, the AR model takes in a time series directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test_timeseries(timeseries, split_percent=10):\n",
    "    \"\"\"Splits a time series DataFrame into train and test sets\n",
    "    Note that for time series data, we should not use random split\n",
    "    because sequence is important.\n",
    "    We'll just split by taking the last split_percent values\n",
    "    \n",
    "    Args:\n",
    "        timeseries: the time series DataFrame (index=date_time, X)\n",
    "    Returns:\n",
    "        tuple: (train, test) as pandas.DataFrames\n",
    "    \"\"\"\n",
    "    split_index = int(((1-split_percent)/100) * timeseries.shape[0])\n",
    "    \n",
    "    train = timeseries.iloc[:split_index, :]\n",
    "    test = timeseries.iloc[split_index:, :]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "# create the Auto-regression model using the training set\n",
    "train_ts, test_ts = split_train_test_timeseries(df)\n",
    "print(\"\\nTrain:\\n\", train_ts.head(5))\n",
    "print(\"\\nTest:\\n\", test_ts.head(5))\n",
    "\n",
    "model = AR(train_ts)\n",
    "\n",
    "# fit the model\n",
    "%time ar_model = model.fit()\n",
    "\n",
    "print('Best lag: %s' % ar_model.k_ar)\n",
    "print('Coefficients: %s' % ar_model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The `statsmodels.tsa.ar_model.AR` implementation picked the 12-lag model as the best one.\n",
    "\n",
    "Next, we'll make some predictions using the AR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_start = pd.to_datetime(test_ts.index.values[0])\n",
    "print(\"Start date\", test_start)\n",
    "test_end = pd.to_datetime(test_ts.index.values[-1])\n",
    "print(\"End date\", test_end)\n",
    "\n",
    "ar_predictions = ar_model.predict(start=test_start, end=test_end)\n",
    "baseline_predictions_ts = pd.DataFrame({'index': baseline_predictions}, index=ar_predictions.index)\n",
    "\n",
    "# plot the predictions vs truth\n",
    "_, ax = plt.subplots(figsize=(20,10))\n",
    "f3, = ax.plot(test_ts)\n",
    "f4, = ax.plot(ar_predictions, color='red')\n",
    "f5, = ax.plot(baseline_predictions_ts, color='black')\n",
    "ax.legend([f3, f4, f5], ['truth', 'AR(12) prediction', 'baseline prediction'])\n",
    "plt.show()\n",
    "\n",
    "score_ts = mean_squared_error(test_ts, ar_predictions)\n",
    "print(\"MSE (AR 12):\", score_ts)\n",
    "print(\"MSE (baseline):\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## What happened?\n",
    "\n",
    "The AR fitted model performed MUCH WORSE than the baseline model.\n",
    "\n",
    "Can you think of reasons to explain this?\n",
    "\n",
    "Hint: look at the series, and think about what we are trying to predict here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Try Decomposition?\n",
    "\n",
    "Let's see if decomposition can help us fit a better model.\n",
    "\n",
    "We'll use the Multiplicative model, because it appears to result in a smaller random component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Exercise:\n",
    "# 1. Use multiplicative decomposition to extract the trend\n",
    "#    Hint: You'll need to drop the first 2 NaN values using iloc\n",
    "#    Otherwise, you'll get \"LinAlgError: SVD did not converge\"\n",
    "#\n",
    "#       df_trend = multiplicative.trend.dropna()\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Re-train the AR model\n",
    "#    Hint: use split_train_test_timeseries like above.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Compare the performance with the other models\n",
    "#    Hint: you should plot 4 lines\n",
    "#            truth\n",
    "#            baseline\n",
    "#            AR on original series\n",
    "#            AR on multiplicative trend (new model)\n",
    "#          you should also compute the MSE for predictions from the new model\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving Average Model\n",
    "\n",
    "We spent some time looking and trying out an Auto-regressive (AR) model. \n",
    "\n",
    "Next: Moving Average (MA) model, and the combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving Average Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$MA(p): X_t=\\mu + \\varepsilon_t + \\sum_{i=1}^p\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output hovers around an average ($\\mu$) and depends on past $p$ values of white noise ($\\varepsilon_{t-i}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Auto-regressive, Moving Average Model (ARMA)\n",
    "\n",
    "This is a combination of AR + MA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ARMA(p, q): AR(p) + MA(q)$\n",
    "\n",
    "$X_t=c + \\varepsilon_t + \\sum_{i=1}^p \\varphi_i X_{t-i} + \\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Auto-regressive, Integrated Moving Average Model (ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ARIMA(p, d, q): \\biggl(1-\\sum_{i=1}^p\\varphi_iB^i\\biggr)(1 - B)^dX_t = c + \\biggl(1 + \\sum_{i=1}^q\\theta_iB^i\\biggr)\\varepsilon_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src='assets/time-series/short_answers_headscratch.png' style='float:right'/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$ARIMA(p, d, q): X'_t = c + \\varepsilon_t + \\sum_{i=1}^p \\varphi_i X'_{t-i} + \\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ARMA(p, d, q): X_t=c + \\varepsilon_t + \\sum_{i=1}^p \\varphi_i X_{t-i} + \\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$X'_t$ = $X_t$, Differentiated $d$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$X_t$ = $X'_t$, Integrated $d$ times (hence the _I_ in ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think of ARIMA as something like:\n",
    "\n",
    "1. $X' = Differential(X, degree=d)$\n",
    "2. $X'_t = ARMA(X')$\n",
    "3. $X_t = Integral(X'_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stationary Time Series\n",
    "\n",
    "Mean, variance don't change over time (i.e. time-invariant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Easier to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Differentiation transforms a time series to become **Stationary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: Time Series Stationarity\n",
    "\n",
    "In this walkthrough, we will look at examples of stationary vs. non-stationary time series.\n",
    "\n",
    "We'll also try modeling a non-stationary series using the ARIMA algorithm to see the effects of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Walkthrough: Time Series Stationarity\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = 'D:\\\\tmp\\\\hdb-resale-price-index\\\\housing-and-development-board-resale-price-index-1q2009-100-quarterly.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# Plot a histogram of the observations\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "From the above histogram, this suggests a non-stationary series, where the mean and variance changes over time.\n",
    "\n",
    "Let's plot histograms of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "multiplicative = seasonal_decompose(df, model='multiplicative')\n",
    "\n",
    "# The seasonal component hovers around 3 values\n",
    "multiplicative.seasonal.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The noise component is fairly gaussian\n",
    "multiplicative.resid.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The trend follows the same pattern as the series\n",
    "multiplicative.trend.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## ARMA vs. ARIMA\n",
    "\n",
    "Since the trend looks fairly non-stationary, let's explore predictions from both the Auto-regressive Moving Average (ARMA) and the Auto-regressive Integrated Moving Average (ARIMA) models.\n",
    "\n",
    "This should also tell us if differentiation makes any difference in finding a stationary series that we can model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## ARMA: selecting best order\n",
    "\n",
    "We will use Akaike's Information Criterion (AIC) and the Bayesian Information Criterion (BIC) to select the order.\n",
    "\n",
    "More information about these metrics are at: www.otexts.org/fpp/8/6\n",
    "\n",
    "In general, the smaller the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA, ARIMA\n",
    "from statsmodels.tsa.stattools import arma_order_select_ic\n",
    "\n",
    "# We'll model the multiplicative trend\n",
    "mult = seasonal_decompose(df, model='multiplicative')\n",
    "trend = mult.trend.dropna()\n",
    "\n",
    "# http://www.statsmodels.org/dev/_modules/statsmodels/tsa/stattools.html#arma_order_select_ic\n",
    "# 'c' means constant\n",
    "train_ts, test_ts = split_train_test_timeseries(trend)\n",
    "res = arma_order_select_ic(train_ts, ic=['aic', 'bic'], trend='c')\n",
    "print(\"Order that minimizes AIC:\", res.aic_min_order)\n",
    "print(\"Order that minimizes BIC:\", res.bic_min_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using the order ($p = 4, q = 2$) we will now fit the ARMA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "order = res.aic_min_order\n",
    "model = ARMA(train_ts, order)\n",
    "%time arma_model = model.fit()\n",
    "print(\"AIC:\", arma_model.aic, \"BIC:\", arma_model.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AIC and BIC criteria are still quite large. This means that the model may not fit very well.\n",
    "\n",
    "Let's give it a try anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_start = pd.to_datetime(test_ts.index.values[0])\n",
    "test_end = pd.to_datetime(test_ts.index.values[-1])\n",
    "\n",
    "# plot the forecast\n",
    "_, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(trend)\n",
    "arma_model.plot_predict(start=test_start, end=test_end, dynamic=True, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# compute the MSE\n",
    "predictions = arma_model.predict(start=test_start, end=test_end)\n",
    "score = mean_squared_error(test_ts, predictions)\n",
    "print(\"MSE (ARMA{}):\".format(order), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The MSE is still significant, but *much* better than the AR model we looked at earlier.\n",
    "\n",
    "Still not as good as the baseline though.\n",
    "\n",
    "## ARIMA\n",
    "\n",
    "Finally, let's see what ARIMA can do. We'll use [pyramid](https://github.com/tgsmith61591/pyramid), which provides an order selector for ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pyramid-arima\n",
    "\n",
    "from pyramid.arima import auto_arima\n",
    "\n",
    "# auto_arima is a wrapper around R's auto.arima\n",
    "# https://www.rdocumentation.org/packages/forecast/versions/7.3/topics/auto.arima\n",
    "stepwise_fit = auto_arima(y=train_ts, start_p=1, start_q=1, max_p=10, max_q=10,\n",
    "                          seasonal=True, max_d=5, trace=True,\n",
    "                          error_action='ignore', # don't want to know if an order does not work\n",
    "                          suppress_warnings=True, # don't want convergence warnings\n",
    "                          stepwise=True)  # set to stepwise\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The best fit appears to be with $p = 2, d = 1, q = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "order_arima = (2, 1, 1)\n",
    "model = ARIMA(train_ts, order)\n",
    "%time arima_model = model.fit()\n",
    "print(\"AIC:\", arima_model.aic, \"BIC:\", arima_model.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Better AIC and BIC than the ARMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_start = pd.to_datetime(test_ts.index.values[0])\n",
    "test_end = pd.to_datetime(test_ts.index.values[-1])\n",
    "\n",
    "# plot the forecast\n",
    "_, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(trend)\n",
    "arima_model.plot_predict(start=test_start, end=test_end, dynamic=True, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# compute the MSE\n",
    "predictions = arima_model.predict(start=test_start, end=test_end, typ='levels')\n",
    "\n",
    "score = mean_squared_error(test_ts, predictions)\n",
    "print(\"MSE (ARMA{}):\".format(order), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "Additional topics that are not covered, but worth reading up on:\n",
    "- Vector Auto-regression (VAR) for multi-variate time series\n",
    "- Partial auto-correlation plots\n",
    "- Seasonal ARIMA (SARIMA)\n",
    "- ARIMA with eXogeneous variables (ARIMAX)\n",
    "- SARIMAX (basically a combo of the above two)\n",
    "\n",
    "statsmodels:\n",
    "- [SARIMAX](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html),  covers SARIMA, ARIMAX, and SARIMAX\n",
    "- [VAR](http://www.statsmodels.org/dev/vector_ar.html?highlight=var#module-statsmodels.tsa.vector_ar.var_model)\n",
    "- [Partial Auto-correlation plots](http://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_pacf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning models for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of Statistical Models\n",
    "\n",
    "- need to pick order\n",
    "- sometimes won't converge\n",
    "- complex to model non-linearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Networks for Time Series\n",
    "\n",
    "- LSTM unit + Fully Connected\n",
    "    - Train with enough data\n",
    "    - Profit!\n",
    "- Stack more LSTMs for more complicated series (e.g. multi-variate)\n",
    "- GRU? Why not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthough: RNN Time Series Predictor\n",
    "\n",
    "Dataset: Average Weekly Paid Hours Worked Per Employee By Industry And Type Of Employment, Annual\n",
    "\n",
    "Link: https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual?resource_id=ecc88c54-6d33-4d1c-9af5-896825386d56\n",
    "\n",
    "Download, unzip, and note the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nature_of_employment</th>\n",
       "      <th>total_paid_hours</th>\n",
       "      <th>standard_hours</th>\n",
       "      <th>overtime_paid_hours</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>full-time</td>\n",
       "      <td>47.5</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990-01-01</th>\n",
       "      <td>part-time</td>\n",
       "      <td>23.5</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>full-time</td>\n",
       "      <td>47.4</td>\n",
       "      <td>43.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1991-01-01</th>\n",
       "      <td>part-time</td>\n",
       "      <td>23.3</td>\n",
       "      <td>22.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1992-01-01</th>\n",
       "      <td>full-time</td>\n",
       "      <td>47.4</td>\n",
       "      <td>43.1</td>\n",
       "      <td>4.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           nature_of_employment  total_paid_hours  standard_hours  \\\n",
       "year                                                                \n",
       "1990-01-01            full-time              47.5            43.0   \n",
       "1990-01-01            part-time              23.5            23.0   \n",
       "1991-01-01            full-time              47.4            43.0   \n",
       "1991-01-01            part-time              23.3            22.7   \n",
       "1992-01-01            full-time              47.4            43.1   \n",
       "\n",
       "            overtime_paid_hours  \n",
       "year                             \n",
       "1990-01-01                  4.5  \n",
       "1990-01-01                  0.5  \n",
       "1991-01-01                  4.4  \n",
       "1991-01-01                  0.5  \n",
       "1992-01-01                  4.3  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Walkthrough: RNN Time Series Predictor\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = 'D:\\\\tmp\\\\weekly-paid-hours\\\\average-weekly-paid-hours-worked-per-employee-by-type-of-employment-topline.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# Inspect it\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
   "autolaunch": true,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
