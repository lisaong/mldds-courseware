{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Forecasting\n",
    "\n",
    "A survey of statistical and deep learning models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's a time series\n",
    "\n",
    "A set of observations at regular time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Source: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "print(\"HDB Resale Price Index (Univariate)\")\n",
    "IFrame('https://data.gov.sg/dataset/hdb-resale-price-index/resource/52e93430-01b7-4de0-80df-bc83d0afed40/view/14c47d07-1395-4661-8466-728abce27f5f', width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "# Source: https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual\n",
    "print(\"Annual Average Weekly Paid Hours Worked Per Employee, By Industry And Type Of Employment (Multi-variate)\")\n",
    "IFrame('https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual/resource/dd2147ce-20b7-401c-ac8f-8dbcc0d8e0d9/view/431f44b1-e58d-4131-998d-7e7aeee14479', width='600', height='400')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Time Series Decomposition\n",
    "\n",
    "- For analysis\n",
    "- Split into trend, cyclic, seasonal, noise\n",
    "- Multiplicative or additive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: Time Series Decomposition\n",
    "1. Go to https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "2. Click on the `Download` button\n",
    "3. Unzip and extract the `.csv` file. Note that path to that file for use later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will use the [StatsModels](https://www.statsmodels.org/stable/index.html) Python library to perform the decomposition. \n",
    "\n",
    "This library will also be used later to perform statistical analysis. [Pandas](https://pandas.pydata.org/pandas-docs/stable/timeseries.html) is also useful for manipulating time series data.\n",
    "\n",
    "```\n",
    "(mldds03) pip install statsmodels\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Walkthrough: Time Series Decomposition\"\"\"\n",
    "\n",
    "#!pip install statsmodels\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = '../data/hdb-resale-price-index/housing-and-development-board-resale-price-index-1q2009-100-quarterly.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# print first few rows to make sure we loaded it properly\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### DateTimeIndex\n",
    "\n",
    "From the table above, we can confirm it is indeed a quarterly distribution. \n",
    "\n",
    "`seasonal_decompose` ([docs](http://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html)) expects a DateTimeIndex to indicate the frequency of the time series. \n",
    "\n",
    "To enable this, we've set `parse_dates=True` and `index_col=0` so that the `DataFrame.index` property is correctly set as a `pandas.DatetimeIndex` ([docs](https://pandas.pydata.org/pandas-docs/stable/timeseries.html#generating-ranges-of-timestamps)), with a quarterly frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the index.\n",
    "# note that this is different from the \"index\" column in the DataFrame,\n",
    "# which refers to the Resale Price Index.\n",
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# plot the raw data, how that we have a DatetimeIndex\n",
    "ax = df.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Additive vs. Multiplicative Decomposition\n",
    "\n",
    "Now we are ready to decompose.  Let's try two models: `additive` and `multiplicative`.\n",
    "\n",
    "You can find the definitions in the [StatsModels documentation](http://www.statsmodels.org/dev/generated/statsmodels.tsa.seasonal.seasonal_decompose.html), but in a nutshell, the `additive` model \"adds up\" the components, while the `multiplicative` model \"multiplies\" the components together.\n",
    "\n",
    "Additive: $$y(t) = Trend + Seasonal + Residual$$\n",
    "\n",
    "Mulplicative: $$y(t) = Trend * Seasonal * Residual$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# it's a bit confusing, but `index` here refers to the HDB resale price index,\n",
    "# not the DataFrame.index (for time series)\n",
    "additive = seasonal_decompose(df, model='additive')\n",
    "additive.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "multiplicative = seasonal_decompose(df, model='multiplicative')\n",
    "multiplicative.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Observations\n",
    "\n",
    "- There is a strong seasonal component. The component values are available as part of the object returned by `seasonal_decompose`\n",
    "- The multiplicative model seems like a better fit, because the resulting residuals are smaller. In general, residuals (noise) are difficult to fit statistically.\n",
    "\n",
    "## Why decompose?\n",
    "- The purpose of decomposition is to \"subtract\" known variability and simpilify creating models. For example, we can ignore the seasonal component and focus on predicting the trend.\n",
    "- `series_decompose` uses a naive implementation. For a more advanced decomposition library, see [STLDecompose]( https://github.com/jrmontag/STLDecompose/blob/master/STL%20usage%20example.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# print out the first few elements of each seasonal component\n",
    "print('Multiplicative model: Seasonal component')\n",
    "print(multiplicative.seasonal[:10])\n",
    "\n",
    "print('Multiplicative model: Trend component')\n",
    "print(multiplicative.trend[:10])\n",
    "\n",
    "print('Additive model: Seasonal component')\n",
    "print(additive.seasonal[:10])\n",
    "\n",
    "print('Additive model: Trend component')\n",
    "print(additive.trend[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Statistical: Auto-regressive, Moving average models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Deep Learning: LSTMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Auto-regressive models\n",
    "\n",
    "$AR(p): X_t=c + \\sum_{i=1}^p \\varphi_iX_{t-i} + \\varepsilon_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output ($X_t$) depends on past values ($X_{t-i}$) + white noise ($\\varepsilon_t$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fitting auto-regressive models\n",
    "\n",
    "1. Use Autocorrelation to pick parameter $p$, which indicates how far back in time $X_{t-i}$ should go\n",
    "2. Train model to fit data, minimizing mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Auto-regressive Models\n",
    "\n",
    "In this workshop, we'll take the dataset we've been working with and see if we can train an auto-regressive model for it.\n",
    "\n",
    "Credits: borrowed heavily from https://machinelearningmastery.com/autoregression-models-time-series-forecasting-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Workshop: Auto-regressive Models\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = '../data/hdb-resale-price-index/housing-and-development-board-resale-price-index-1q2009-100-quarterly.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# print first few rows to make sure we loaded it properly\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Lag plots\n",
    "\n",
    "A quick first test is to check if the data is random. If random, the data will not exhibit a structure in the lag plot.\n",
    "\n",
    "Let's try [`pandas.plotting.lag_plot`](https://pandas.pydata.org/pandas-docs/stable/visualization.html#lag-plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# try to see if there are correlations between X(t) and X(t-1)\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20,10))\n",
    "df.plot(ax=ax1) # series plot\n",
    "pd.plotting.lag_plot(df['index']) # lag plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "As a comparison, here's what the lag plot will look like for a random series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# generate a random series\n",
    "random_df = pd.DataFrame(np.random.randint(0, 100, size=(100, 1)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize=(20,10))\n",
    "random_df.plot(ax=ax1) # series plot\n",
    "pd.plotting.lag_plot(random_df, ax=ax2) # lag plot (shows no correlation!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "So, it does look like there is correlation with the current and previous value of the HDB resale price index.\n",
    "\n",
    "We are ready to move to the next step.\n",
    "\n",
    "## Auto-correlation plots\n",
    "The next test is use auto-correlation to pick a good value of `p` to use for the equation:\n",
    "\n",
    "$AR(p): X_t=c + \\sum_{i=1}^p \\varphi_iX_{t-i} + \\varepsilon_t$\n",
    "\n",
    "We will use the `pandas.plotting.autocorrelation_plot`: https://pandas.pydata.org/pandas-docs/stable/visualization.html#autocorrelation-plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(df['index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Again, it is helpful to compare with a random series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pd.plotting.autocorrelation_plot(random_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Interpreting Auto-correlation plots\n",
    "\n",
    "The [docs](https://pandas.pydata.org/pandas-docs/stable/visualization.html#autocorrelation-plot) have a good explanation that we'll summarize here:\n",
    "\n",
    "- Auto-correlation plots show the auto-correlation at different time lags (`p`)\n",
    "- Random time series: the auto-correlations always hover around zero\n",
    "- Non-random time series: find values of Lag where auto-correlations are outside of the 95% or 99% confidence band.\n",
    "  - 95%: solid line\n",
    "  - 99%: dashed line\n",
    "- Both negative and positive auto-correlations are valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Auto-regressive model\n",
    "\n",
    "Since the dataset is about HDB quarterly resale prices, we can develop a model to predict the resale prices for the next year (4 quarters).\n",
    "\n",
    "Recall that an Auto-regessive model uses the past values to predict the next value in the series:\n",
    "\n",
    "$$AR(p): X_t=c + \\sum_{i=1}^p \\varphi_iX_{t-i} + \\varepsilon_t$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline model\n",
    "\n",
    "Before we fit the AR model, it's a good idea to create a baseline model.\n",
    "\n",
    "The simplest possible one is a \"Persistence Model\", which just predicts the current value based on the last observation.\n",
    "\n",
    "$$X_t = X_{t-1}$$\n",
    "\n",
    "### Predictions using a Persistence Model\n",
    "1. The time series is a vector of (t, X) values. To model it, we will time-shift the dataset to produce two vectors (x, y).\n",
    "    - input: $x = X_t$\n",
    "    - output: $y = X_{t+1}$\n",
    "2. We'll then split the (x, y) dataset into train and test\n",
    "3. Run the baseline model to get predictions:\n",
    "$$\\hat{y} = BaselineModel(x)$$ Where: $$BaselineModel(x) = x$$\n",
    "\n",
    "4. Then compute the means square error:\n",
    "$$score = MSE(y, \\hat{y})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def create_lagged_dataset(data):\n",
    "    \"\"\"Creates the dataset used for time-series modeling\n",
    "    Args:\n",
    "        data: the time series data (X)\n",
    "    Returns:\n",
    "        the lagged dataset (X[t], X[t+1]) as a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    values = pd.DataFrame(data)\n",
    "    df = pd.concat([values.shift(1), values], axis=1) # concat in the column axis\n",
    "    df.columns = ['t', 't+1']\n",
    "    return df\n",
    "    \n",
    "def split_train_test(data, split_percent=10):\n",
    "    \"\"\"Splits the dataset into train and test sets\n",
    "    Note that for time series data, we should not use random split\n",
    "    because sequence is important.\n",
    "    We'll just split by taking the last split_percent values\n",
    "    \n",
    "    Args:\n",
    "        data: the dataset (x, y)\n",
    "        split_percent: the percentage of the dataset to reserve\n",
    "                       for testing\n",
    "    Returns:\n",
    "        tuple: (train, test) as pandas.DataFrames\n",
    "    \"\"\"\n",
    "    split_index = int(((1-split_percent)/100) * data.shape[0])\n",
    "    \n",
    "    x = data.values\n",
    "    train, test = x[1:split_index], x[split_index:]\n",
    "    \n",
    "    train_df = pd.DataFrame.from_records(train, columns=['x', 'y'])\n",
    "    test_df = pd.DataFrame.from_records(test, columns=['x', 'y'])\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "def baseline_model(x):\n",
    "    \"\"\"Baseline model which gives the provided value\n",
    "    It implements: f(x) = x\n",
    "    Args:\n",
    "        x: the value\n",
    "    Returns:\n",
    "        the provided value\n",
    "    \"\"\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# create datasets\n",
    "dataset = create_lagged_dataset(df['index'])\n",
    "print(\"\\nDataset:\\n\", dataset.head(5))\n",
    "\n",
    "train, test = split_train_test(dataset)\n",
    "print(\"\\nTrain:\\n\", train.head(5))\n",
    "print(\"\\nTest:\\n\", test.head(5))\n",
    "\n",
    "# walk-forward validation using baseline model: f(x) = x\n",
    "baseline_predictions = [baseline_model(x) for x in test['x']]\n",
    "\n",
    "# plot the predictions vs truth\n",
    "_, ax = plt.subplots(figsize=(20,10))\n",
    "f1, = ax.plot(test['y'])\n",
    "f2, = ax.plot(baseline_predictions, color='red')\n",
    "ax.legend([f1, f2], ['truth', 'baseline prediction'])\n",
    "plt.show()\n",
    "\n",
    "score = mean_squared_error(test['y'], baseline_predictions)\n",
    "print(\"MSE:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Auto-regressive model\n",
    "\n",
    "Now that we have our baseline model in place, we can proceed with the AR model.\n",
    "\n",
    "We'll use the [`statsmodels.tsa.ar_model.AR`](http://www.statsmodels.org/devel/generated/statsmodels.tsa.ar_model.AR.html)\n",
    "\n",
    "Unlike the baseline model where we have to create a lagged time series, the AR model takes in a time series directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def split_train_test_timeseries(timeseries, split_percent=10):\n",
    "    \"\"\"Splits a time series DataFrame into train and test sets\n",
    "    Note that for time series data, we should not use random split\n",
    "    because sequence is important.\n",
    "    We'll just split by taking the last split_percent values\n",
    "    \n",
    "    Args:\n",
    "        timeseries: the time series DataFrame (index=date_time, X)\n",
    "        split_percent: the percentage of the time series to reserve\n",
    "                       for testing\n",
    "    Returns:\n",
    "        tuple: (train, test) as pandas.DataFrames\n",
    "    \"\"\"\n",
    "    split_index = int(((1-split_percent)/100) * timeseries.shape[0])\n",
    "    \n",
    "    train = timeseries.iloc[:split_index, :]\n",
    "    test = timeseries.iloc[split_index:, :]\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.ar_model import AR\n",
    "\n",
    "# create the Auto-regressive model using the training set\n",
    "train_ts, test_ts = split_train_test_timeseries(df)\n",
    "print(\"\\nTrain:\\n\", train_ts.head(5))\n",
    "print(\"\\nTest:\\n\", test_ts.head(5))\n",
    "\n",
    "model = AR(train_ts)\n",
    "\n",
    "# fit the model\n",
    "%time ar_model = model.fit()\n",
    "\n",
    "print('Best lag: %s' % ar_model.k_ar)\n",
    "print('Coefficients: %s' % ar_model.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The `statsmodels.tsa.ar_model.AR` implementation picked the 12-lag model as the best one.\n",
    "\n",
    "Next, we'll make some predictions using the AR model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_start = pd.to_datetime(test_ts.index.values[0])\n",
    "print(\"Start date\", test_start)\n",
    "test_end = pd.to_datetime(test_ts.index.values[-1])\n",
    "print(\"End date\", test_end)\n",
    "\n",
    "ar_predictions = ar_model.predict(start=test_start, end=test_end)\n",
    "baseline_predictions_ts = pd.DataFrame({'index': baseline_predictions}, index=ar_predictions.index)\n",
    "\n",
    "# plot the predictions vs truth\n",
    "_, ax = plt.subplots(figsize=(20,10))\n",
    "f3, = ax.plot(test_ts)\n",
    "f4, = ax.plot(ar_predictions, color='red')\n",
    "f5, = ax.plot(baseline_predictions_ts, color='black')\n",
    "ax.legend([f3, f4, f5], ['truth', 'AR(12) prediction', 'baseline prediction'])\n",
    "plt.show()\n",
    "\n",
    "score_ts = mean_squared_error(test_ts, ar_predictions)\n",
    "print(\"MSE (AR 12):\", score_ts)\n",
    "print(\"MSE (baseline):\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## What happened?\n",
    "\n",
    "The AR fitted model performed MUCH WORSE than the baseline model.\n",
    "\n",
    "Can you think of reasons to explain this?\n",
    "\n",
    "Hint: look at the series, and think about what we are trying to predict here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Try Decomposition?\n",
    "\n",
    "Let's see if decomposition can help us fit a better model.\n",
    "\n",
    "We'll use the Multiplicative model, because it appears to result in a smaller random component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================================\n",
    "# Exercise:\n",
    "# 1. Use multiplicative decomposition to extract the trend\n",
    "#    Hint: You'll need to drop the first 2 NaN values using iloc\n",
    "#    Otherwise, you'll get \"LinAlgError: SVD did not converge\"\n",
    "#\n",
    "#       df_trend = multiplicative.trend.dropna()\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Re-train the AR model\n",
    "#    Hint: use split_train_test_timeseries like above.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Compare the performance with the other models\n",
    "#    Hint: you should plot 4 lines\n",
    "#            truth\n",
    "#            baseline\n",
    "#            AR on original series\n",
    "#            AR on multiplicative trend (new model)\n",
    "#          you should also compute the MSE for predictions from the new model\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving Average Model\n",
    "\n",
    "We spent some time looking and trying out an Auto-regressive (AR) model. \n",
    "\n",
    "Next: Moving Average (MA) model, and the combination of both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Moving Average Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$MA(p): X_t=\\mu + \\varepsilon_t + \\sum_{i=1}^p\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Output hovers around an average ($\\mu$) and depends on past $p$ values of white noise ($\\varepsilon_{t-i}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Auto-regressive, Moving Average Model (ARMA)\n",
    "\n",
    "This is a combination of AR + MA:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ARMA(p, q): AR(p) + MA(q)$\n",
    "\n",
    "$X_t=c + \\varepsilon_t + \\sum_{i=1}^p \\varphi_i X_{t-i} + \\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Auto-regressive, Integrated Moving Average Model (ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ARIMA(p, d, q): \\biggl(1-\\sum_{i=1}^p\\varphi_iB^i\\biggr)(1 - B)^dX_t = c + \\biggl(1 + \\sum_{i=1}^q\\theta_iB^i\\biggr)\\varepsilon_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![???](assets/time-series/short_answers_headscratch.png)\n",
    "\n",
    "(image: xkcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$ARIMA(p, d, q): X'_t = c + \\varepsilon_t + \\sum_{i=1}^p \\varphi_i X'_{t-i} + \\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$ARMA(p, d, q): X_t=c + \\varepsilon_t + \\sum_{i=1}^p \\varphi_i X_{t-i} + \\sum_{i=1}^q\\theta_i\\varepsilon_{t-i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$X'_t$ = $X_t$, Differentiated $d$ times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$X_t$ = $X'_t$, Integrated $d$ times (hence the _I_ in ARIMA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Think of ARIMA as something like:\n",
    "\n",
    "1. $X' = Differential(X, degree=d)$\n",
    "2. $X'_t = ARMA(X')$\n",
    "3. $X_t = Integral(X'_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stationary Time Series\n",
    "\n",
    "Mean, variance don't change over time (i.e. time-invariant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Easier to model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Differentiation transforms a time series to become **Stationary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: Time Series Stationarity\n",
    "\n",
    "In this walkthrough, we will look at examples of stationary vs. non-stationary time series.\n",
    "\n",
    "We'll also try modeling a non-stationary series using the ARIMA algorithm to see the effects of differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Walkthrough: Time Series Stationarity\"\"\"\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/hdb-resale-price-index\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = '../data/hdb-resale-price-index/housing-and-development-board-resale-price-index-1q2009-100-quarterly.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# Plot a histogram of the observations\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "From the above histogram, this suggests a non-stationary series, where the mean and variance changes over time.\n",
    "\n",
    "Let's plot histograms of the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "multiplicative = seasonal_decompose(df, model='multiplicative')\n",
    "\n",
    "# The seasonal component hovers around 3 values, and is stationary (time invariant)\n",
    "multiplicative.seasonal.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The noise component is fairly gaussian and stationary (time invariant)\n",
    "multiplicative.resid.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# The trend changes with time, it's not stationary\n",
    "multiplicative.trend.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## ARMA vs. ARIMA\n",
    "\n",
    "Since the trend looks fairly non-stationary, let's explore predictions from both the Auto-regressive Moving Average (ARMA) and the Auto-regressive Integrated Moving Average (ARIMA) models.\n",
    "\n",
    "This should also tell us if differentiation makes any difference in finding a stationary series that we can model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## ARMA: selecting best order\n",
    "\n",
    "We will use Akaike's Information Criterion (AIC) and the Bayesian Information Criterion (BIC) to select the order.\n",
    "\n",
    "More information about these metrics are at: www.otexts.org/fpp/8/6\n",
    "\n",
    "In general, the smaller the better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA, ARIMA\n",
    "from statsmodels.tsa.stattools import arma_order_select_ic\n",
    "\n",
    "# We'll model the multiplicative trend\n",
    "mult = seasonal_decompose(df, model='multiplicative')\n",
    "trend = mult.trend.dropna()\n",
    "\n",
    "# http://www.statsmodels.org/dev/_modules/statsmodels/tsa/stattools.html#arma_order_select_ic\n",
    "# 'c' means constant\n",
    "train_ts, test_ts = split_train_test_timeseries(trend)\n",
    "res = arma_order_select_ic(train_ts, ic=['aic', 'bic'], trend='c')\n",
    "print(\"Order that minimizes AIC:\", res.aic_min_order)\n",
    "print(\"Order that minimizes BIC:\", res.bic_min_order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Using the order ($p = 4, q = 2$) we will now fit the ARMA model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# fit the model\n",
    "order = res.aic_min_order\n",
    "model = ARMA(train_ts, order)\n",
    "%time arma_model = model.fit()\n",
    "print(\"AIC:\", arma_model.aic, \"BIC:\", arma_model.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AIC and BIC criteria are still quite large. This means that the model may not fit very well.\n",
    "\n",
    "Let's give it a try anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_start = pd.to_datetime(test_ts.index.values[0])\n",
    "test_end = pd.to_datetime(test_ts.index.values[-1])\n",
    "\n",
    "# plot the forecast\n",
    "_, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(trend)\n",
    "arma_model.plot_predict(start=test_start, end=test_end, dynamic=True, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# compute the MSE\n",
    "predictions = arma_model.predict(start=test_start, end=test_end)\n",
    "score = mean_squared_error(test_ts, predictions)\n",
    "print(\"MSE (ARMA{}):\".format(order), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The MSE is still significant, but *much* better than the AR model we looked at earlier.\n",
    "\n",
    "Still not as good as the baseline though.\n",
    "\n",
    "## ARIMA\n",
    "\n",
    "Finally, let's see what ARIMA can do. We'll use [pyramid](https://github.com/tgsmith61591/pyramid), which provides an order selector for ARIMA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "#!pip install pyramid-arima\n",
    "\n",
    "from pyramid.arima import auto_arima\n",
    "\n",
    "# auto_arima is a wrapper around R's auto.arima\n",
    "# https://www.rdocumentation.org/packages/forecast/versions/7.3/topics/auto.arima\n",
    "stepwise_fit = auto_arima(y=train_ts, start_p=1, start_q=1, max_p=10, max_q=10,\n",
    "                          seasonal=True, max_d=5, trace=True,\n",
    "                          error_action='ignore', # don't want to know if an order does not work\n",
    "                          suppress_warnings=True, # don't want convergence warnings\n",
    "                          stepwise=True)  # set to stepwise\n",
    "\n",
    "stepwise_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The best fit appears to be with $p = 2, d = 1, q = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "order_arima = (2, 1, 1)\n",
    "model = ARIMA(train_ts, order)\n",
    "%time arima_model = model.fit()\n",
    "print(\"AIC:\", arima_model.aic, \"BIC:\", arima_model.bic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Better AIC and BIC than the ARMA model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_start = pd.to_datetime(test_ts.index.values[0])\n",
    "test_end = pd.to_datetime(test_ts.index.values[-1])\n",
    "\n",
    "# plot the forecast\n",
    "_, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(trend)\n",
    "arima_model.plot_predict(start=test_start, end=test_end, dynamic=True, ax=ax)\n",
    "plt.show()\n",
    "\n",
    "# compute the MSE\n",
    "predictions = arima_model.predict(start=test_start, end=test_end, typ='levels')\n",
    "\n",
    "score = mean_squared_error(test_ts, predictions)\n",
    "print(\"MSE (ARMA{}):\".format(order), score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "Additional topics that are not covered, but worth reading up on:\n",
    "- Vector Auto-regressive model (VAR) for multi-variate time series\n",
    "- Partial auto-correlation plots\n",
    "- Seasonal ARIMA (SARIMA)\n",
    "- ARIMA with eXogeneous variables (ARIMAX)\n",
    "- SARIMAX (basically a combo of the above two)\n",
    "\n",
    "statsmodels:\n",
    "- [SARIMAX](http://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html),  covers SARIMA, ARIMAX, and SARIMAX\n",
    "- [VAR](http://www.statsmodels.org/dev/vector_ar.html?highlight=var#module-statsmodels.tsa.vector_ar.var_model)\n",
    "- [Partial Auto-correlation plots](http://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_pacf.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep Learning models for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of Statistical Models\n",
    "\n",
    "- need to pick order\n",
    "- sometimes won't converge\n",
    "- complex to model non-linearity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Networks for Time Series\n",
    "\n",
    "- LSTM unit + Fully Connected\n",
    "    - Train with enough data\n",
    "    - Profit!\n",
    "- Stack more LSTMs for more complicated series (e.g. multi-variate)\n",
    "- GRU? Why not!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthough: RNN Time Series Predictor\n",
    "\n",
    "Dataset: Average Weekly Paid Hours Worked Per Employee By Industry And Type Of Employment, Annual\n",
    "\n",
    "Link: https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual?resource_id=ecc88c54-6d33-4d1c-9af5-896825386d56\n",
    "\n",
    "Download, unzip, and note the path and replace it below.\n",
    "\n",
    "Credits:\n",
    "https://machinelearningmastery.com/time-series-forecasting-long-short-term-memory-network-python/\n",
    "\n",
    "Cheatsheets:\n",
    "- https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf\n",
    "- https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Python_Matplotlib_Cheat_Sheet.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Walkthrough: RNN Time Series Predictor\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# ==================================================================\n",
    "# Dataset URL: https://data.gov.sg/dataset/average-weekly-paid-hours-worked-per-employee-by-industry-and-type-of-employment-annual\n",
    "#\n",
    "# Update this path to match your actual path\n",
    "data_path = '../data/weekly-paid-hours/average-weekly-paid-hours-worked-per-employee-by-type-of-employment-topline.csv'\n",
    "\n",
    "# load data\n",
    "df = pd.read_csv(data_path, index_col=0,\n",
    "                 parse_dates=True, infer_datetime_format=True)\n",
    "\n",
    "# Inspect\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# let's filter by full-time vs. part-time when we plot\n",
    "df_ft = df[df['nature_of_employment'] == 'full-time']\n",
    "df_pt = df[df['nature_of_employment'] == 'part-time']\n",
    "\n",
    "_, ax = plt.subplots(nrows=1, ncols=1, figsize=(20, 10))\n",
    "df_ft.plot(ax=ax, marker='o')\n",
    "df_pt.plot(ax=ax, marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Prediction Task\n",
    "\n",
    "Let's define our prediction task.\n",
    "\n",
    "- Task: Predict the **Full Time**, **Total Paid Hours** for **2018-2020**\n",
    "- Training set: Annual data for 1990-2011\n",
    "- Test set: Annual data for 2012-2016\n",
    "- Model: RNN with single LSTM layer\n",
    "- Baseline model: persistence model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Baseline model\n",
    "\n",
    "All performance is relative.  First, let's run the baseline model so that we have something to compare the performance of RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_split_row_index(data, split_date_str):\n",
    "    \"\"\"Gets the split index for the data, given a date string\n",
    "    Args:\n",
    "        data: the dataframe containing (t, X)\n",
    "        split_date_str: a date string format that's acceptable by\n",
    "                        pandas.to_datetime()\n",
    "    Returns:\n",
    "        the split row index into data\n",
    "    \"\"\"\n",
    "    split_date = pd.to_datetime(split_date_str)\n",
    "    split_data = data[data.index <= split_date]\n",
    "    return split_data.shape[0]\n",
    "\n",
    "def create_lagged_dataset(data):\n",
    "    \"\"\"Creates the dataset used for time-series modeling\n",
    "    Args:\n",
    "        data: the dataframe containing (t, X)\n",
    "    Returns:\n",
    "        the lagged dataset (X[t], X[t+1]) as a pandas.DataFrame\n",
    "    \"\"\"\n",
    "    values_df = pd.DataFrame(data.values)\n",
    "    df = pd.concat([values_df.shift(1), values_df],\n",
    "                   axis=1) # concat in the column axis\n",
    "    df.columns = ['t', 't+1']\n",
    "    return df.dropna()\n",
    "\n",
    "def create_train_test(data, split_date_str):\n",
    "    \"\"\"Splits the dataset into train and test sets\n",
    "    Note that for time series data, we should not use random split\n",
    "    because sequence is important.\n",
    "    We'll just split by reserving the values after the split_date\n",
    "    into the test set.\n",
    "    \n",
    "    Args:\n",
    "        data: the dataframe containing (t, X)\n",
    "        split_date_str: entries after this date string will be\n",
    "                        reserved in the test set. This should be\n",
    "                        a string format that's acceptable by\n",
    "                        pandas.to_datetime()\n",
    "    Returns:\n",
    "        tuple: (train, test) as pandas.DataFrames\n",
    "    \"\"\"\n",
    "    # first, convert the split date into a split index\n",
    "    split_index = get_split_row_index(data, split_date_str)\n",
    "    \n",
    "    # next, create the lagged dataset (X[t], X[t+1])\n",
    "    lagged_dataset = create_lagged_dataset(data['total_paid_hours'])\n",
    "\n",
    "    # finally, split the lagged dataset using the split index\n",
    "    train = lagged_dataset[:split_index]\n",
    "    test = lagged_dataset[split_index:]\n",
    "    return train, test\n",
    "\n",
    "def baseline_model(x):\n",
    "    \"\"\"Baseline model which gives the provided value\n",
    "    It implements: f(x) = x\n",
    "    Args:\n",
    "        x: the value\n",
    "    Returns:\n",
    "        the provided value\n",
    "    \"\"\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# use only the columns we care about\n",
    "df_ft = pd.DataFrame(df[df['nature_of_employment'] == 'full-time'],\n",
    "                     columns=['total_paid_hours'])\n",
    "\n",
    "train, test = create_train_test(df_ft, '2012')\n",
    "\n",
    "print(\"\\nTrain dataset:\\n\", train.head(5))\n",
    "print(\"\\nTest dataset:\\n\", test.head(5))\n",
    "\n",
    "# walk-forward validation using baseline model: f(x) = x\n",
    "baseline_predictions = [baseline_model(x) for x in test['t']]\n",
    "print(\"\\nPredictions:\\n\", baseline_predictions)\n",
    "\n",
    "# plot the predictions vs truth\n",
    "_, ax = plt.subplots(figsize=(20,10))\n",
    "f1, = ax.plot(test['t+1'].values, marker='o')\n",
    "f2, = ax.plot(baseline_predictions, color='red', marker='o')\n",
    "ax.legend([f1, f2], ['truth', 'baseline prediction'])\n",
    "plt.show()\n",
    "\n",
    "score = mean_squared_error(test['t+1'], baseline_predictions)\n",
    "print(\"MSE:\", score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## LSTM RNN\n",
    "\n",
    "Now we are ready to train our RNN to see if it can beat the baseline model.\n",
    "\n",
    "We will keep the architecture simple and create a single-layer LSTM that feeds into a fully-connected (Dense) layer.\n",
    "\n",
    "```\n",
    "model = Sequential()\n",
    "model.add(LSTM(...))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "```\n",
    "\n",
    "Note: As an exercise, you are encouraged to try the Statistical models as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## RNN: preparing data\n",
    "\n",
    "### Convert to Supervised dataset\n",
    "Unlike statistical models, RNN is a supervised learning problem. This means that we'll need to convert \n",
    "the data from a time series $(t, X)$ to a supervised learning dataset $(x, y)$.  \n",
    "\n",
    "Because the baseline model is also a supervised learning problem, we can reuse `create_train_test` as long as we keep the inputs and outputs in the same data type (e.g. pandas.DataFrame, same column names...).\n",
    "\n",
    "### Differencing\n",
    "To improve the modeling, we'll try to remove non-stationarity from the data. We'll use a technique called \"differencing,\" where computing the change in value for each time step, relative to the previous time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def difference(series):\n",
    "    \"\"\"Computes the difference of a series,\n",
    "    which contains the relative change in value from\n",
    "    the previous entry\n",
    "    Args:\n",
    "        series: the series\n",
    "    Returns:\n",
    "        the differenced values (change relative\n",
    "        to the previous step)\n",
    "    \"\"\"\n",
    "    values = series.values\n",
    "    diff = [values[i] - values[i - 1]\n",
    "            for i in range(1, len(values))]\n",
    "    return pd.DataFrame(diff)\n",
    "\n",
    "def inverse_difference(initial_value, differenced):\n",
    "    \"\"\"Computes the inverse difference of a series\n",
    "    Args:\n",
    "        initial_value: the initial value\n",
    "        differenced: the differenced series as a\n",
    "                     panda.DataFrame\n",
    "    Returns:\n",
    "        the original series as a panda.DataFrame\n",
    "    \"\"\"\n",
    "    diff = differenced.values\n",
    "\n",
    "    data = np.ndarray(differenced.shape[0] + 1)\n",
    "    data[0] = initial_value\n",
    "\n",
    "    for i in range(len(diff)):\n",
    "        data[i+1] = data[i] + diff[i]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "def difference_unittest():\n",
    "    series = pd.DataFrame([1., -2., 3., -4., 5.])\n",
    "    d = difference(series)\n",
    "    s = inverse_difference(series[0][0], d)\n",
    "    np.testing.assert_allclose(series.values, s.values)\n",
    "\n",
    "difference_unittest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Scaling\n",
    "LSTMs use `tanh` activations, so we will need to scale the data to -1 and 1.\n",
    "\n",
    "We'll use the training dataset to scale the data, so that there is no contamination from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale(train, test, scale_range=(-1, 1)):\n",
    "    \"\"\"Scales data to the feature_range\n",
    "    Args:\n",
    "        train: the training set\n",
    "        test: the test set\n",
    "        scale_range: the range (inclusive) to scale\n",
    "    Returns:\n",
    "        a tuple of scaler, scaled_train, scaled_test data\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler(feature_range=scale_range)\n",
    "    scaler = scaler.fit(train)\n",
    "\n",
    "    # sanity check, print the min, max observed by the scaler\n",
    "    print(\"Training min:\", scaler.data_min_,\n",
    "          \"Training max:\", scaler.data_max_)\n",
    "\n",
    "    train_scaled = scaler.transform(train)\n",
    "\n",
    "    # test dataset uses a scaler fit to the min & max of training set\n",
    "    test_scaled = scaler.transform(test)\n",
    "\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "def inverse_scale(scaler, scaled_data):\n",
    "    \"\"\"Restores the data back to the original scale\n",
    "    Args:\n",
    "        scaler: the scaler, used to scale the original data\n",
    "        scaled_data: the data to unscale\n",
    "    Returns:\n",
    "        the unscaled data        \n",
    "    \"\"\"\n",
    "    return scaler.inverse_transform(scaled_data)\n",
    "\n",
    "def scale_unittest():\n",
    "    train = pd.DataFrame([10., -2., 30., -4., 5.])\n",
    "    test = pd.DataFrame([1., -2., 3., -4., 50.])\n",
    "    scaler, train_scaled, test_scaled = scale(train, test)\n",
    "    test_unscaled = inverse_scale(scaler, test_scaled)\n",
    "    np.testing.assert_allclose(test.values, test_unscaled)\n",
    "    \n",
    "scale_unittest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Putting it together: Data Processing\n",
    "\n",
    "We'll now do the following:\n",
    "1. Compute the differenced series for `full-time`, `total_paid_hours`\n",
    "2. Scale the dataset based on the training set's max and min\n",
    "3. Convert the series into a supervised dataset, split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Starting point DataFrame\n",
    "df_ft = pd.DataFrame(df[df['nature_of_employment'] == 'full-time'],\n",
    "                     columns=['total_paid_hours'])\n",
    "\n",
    "df_ft.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the difference\n",
    "\n",
    "df_ft_diff = difference(df_ft['total_paid_hours'])\n",
    "df_ft_diff.index = df_ft.index[1:]\n",
    "df_ft_diff.columns = ['total_paid_hours']\n",
    "\n",
    "df_ft_diff.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Scale the dataset for training with LSTM\n",
    "split_index = get_split_row_index(df_ft_diff, \"2012\")\n",
    "df_train, df_test = df_ft_diff[:split_index], df_ft_diff[split_index:]\n",
    "\n",
    "scaler, train_scaled, test_scaled = scale(df_train.values, df_test.values)\n",
    "print(\"Train scaled:\")\n",
    "print(train_scaled[:5])\n",
    "\n",
    "print(\"Test scaled:\")\n",
    "print(test_scaled[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Convert to a supervised learning datasets\n",
    "train = create_lagged_dataset(pd.DataFrame(train_scaled))\n",
    "test = create_lagged_dataset(pd.DataFrame(test_scaled))\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Differencing makes a ... difference\n",
    "\n",
    "Since we are scaling the test dataset based on the training dataset, it is possible for the test truth values to fall completely outside of the [-1, 1] range.\n",
    "\n",
    "Truth values outside of the range will mean higher mean squared error with the RNN model. We would encounter this if we did *not* apply differencing to the series, because the test truth values were lower than the training values.\n",
    "\n",
    "In general, whenever you observe a time-dependent trend in the data, it is a good idea to try differencing as a pre-processing step. \n",
    "\n",
    "#### Optional Exercise - Differencing with Auto Regression\n",
    "Differencing can be helpful for modeling the *HDB Resale Price* dataset. Try it before feeding into the Statistical models to see if you get better predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setting up the LSTM RNN \n",
    "\n",
    "Using Keras, we'll now setup an LSTM Recurrent Neural Network with the this configuration:\n",
    "- Neurons (how many LSTM units): 1\n",
    "- Loss: mean square error \n",
    "- Optimizer: Adam\n",
    "- Input shape: batch size, # time steps, # features\n",
    "    - batch size: 1\n",
    "    - time steps: 1\n",
    "    - features: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM\n",
    "\n",
    "def create_LSTM_model(batch_size, time_steps, features, neurons):\n",
    "    \"\"\"Creates the LSTM model for predicting time-series\n",
    "    Args:\n",
    "        batch_size: number of samples per batch\n",
    "        time_steps: number of time steps per sample\n",
    "        features: number of features per sample\n",
    "        neurons: number of LSTM neurons\n",
    "    Returns:\n",
    "        the model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons,\n",
    "                   batch_input_shape=(batch_size, time_steps, features), \n",
    "                   stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### TensorBoard Callbacks\n",
    "\n",
    "Because the model is being re-fit on each epoch, we'll extend the Keras Tensorboard callback to log the losses for each epoch. \n",
    "\n",
    "This way we can visualize the losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "\n",
    "class LossHistory(TensorBoard):\n",
    "    writer = None\n",
    "    log_dir = None\n",
    "\n",
    "    def __init(self, log_dir='./logs', **kwargs):\n",
    "        \"\"\"Constructor\n",
    "        Args:\n",
    "            log_dir: the logs directory\n",
    "        \"\"\"\n",
    "        # make a subdirectory for our loss logs\n",
    "        self.log_dir = os.path.join(log_dir, 'loss')\n",
    "        super(LossHistory, self).__init__(log_dir, **kwargs)\n",
    "\n",
    "    def set_model(self, model):\n",
    "        \"\"\"Called by Keras to set the model\n",
    "        Args:\n",
    "            model: the model being set\n",
    "        \"\"\"\n",
    "        # setup the writer for the loss metric\n",
    "        self.writer = tf.summary.FileWriter(self.log_dir)\n",
    "        super(LossHistory, self).set_model(model)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        \"\"\"Called by Keras during the end of an epoch\n",
    "        Args:\n",
    "            epoch: the current epoch\n",
    "            logs: the statistics to be logged\n",
    "        \"\"\"\n",
    "        # record the loss in the summary so that\n",
    "        # TensorBoard will plot it\n",
    "        summary = tf.Summary()\n",
    "        summary_value = summary.value.add()\n",
    "        summary_value.simple_value = logs.get('loss')\n",
    "        summary_value.tag = 'loss'\n",
    "        self.writer.add_summary(summary, epoch)\n",
    "        self.writer.flush()\n",
    "        super(LossHistory, self).on_epoch_end(logs, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training\n",
    "\n",
    "We'll now train the RNN.\n",
    "\n",
    "If all goes well, you should see a loss curve like this in TensorBoard:\n",
    "\n",
    "![loss graph](assets/time-series/rnn_tensorboard_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "NEURONS = 1\n",
    "TIME_STEPS = 1\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS=3000\n",
    "FEATURES=1\n",
    "\n",
    "X_train = train['t']\n",
    "y_train = train['t+1']\n",
    "\n",
    "# reshape the data to (samples, timesteps, features)\n",
    "X_train = X_train.reshape(len(X_train), TIME_STEPS, FEATURES)\n",
    "print(\"(samples, timesteps, features):\", X_train.shape)\n",
    "\n",
    "# setup logging\n",
    "tensorboard = LossHistory(log_dir='./logs/lstm_ts_{}'.format(time()),\n",
    "                          write_graph=True, write_images=True)\n",
    "\n",
    "# create the model\n",
    "model = create_LSTM_model(batch_size=BATCH_SIZE, time_steps=TIME_STEPS,\n",
    "                          features=FEATURES, neurons=NEURONS)\n",
    "\n",
    "# train\n",
    "for i in range(EPOCHS):\n",
    "    # IMPORTANT: shuffle must be False because\n",
    "    # ordering of samples needs to be preserved and learnt\n",
    "    model.fit(X_train, y_train, epochs=1, batch_size=BATCH_SIZE,\n",
    "              verbose=True, shuffle=False, callbacks=[tensorboard])\n",
    "    \n",
    "    # Reset the state for each epoch because we are starting\n",
    "    # from the beginning of the sample\n",
    "    # This does not reset the weights, just the hidden states\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Making Predictions\n",
    "\n",
    "After training the LSTM, we can use it to make predictions:\n",
    "\n",
    "1. Initialize the state in the network using the training data\n",
    "2. Make the prediction for each test data\n",
    "3. Invert scaling\n",
    "4. Invert differencing\n",
    "5. Plot and compute mean square error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def initialize_model_state(model, train):\n",
    "    \"\"\"Initializes the model state using training data\n",
    "    Args:\n",
    "        model: the trained model\n",
    "        train: the training set use\n",
    "    Returns:\n",
    "        the initialized model\n",
    "    \"\"\"\n",
    "    # \"fast forward\" the model to the end of the\n",
    "    # training timesteps (X_train), discarding the\n",
    "    # predictions (yhat_train)\n",
    "    X_train = train['t']\n",
    "    \n",
    "    # reshape to samples, time steps, features\n",
    "    X_train = X_train.reshape(len(X_train), 1, 1)\n",
    "    \n",
    "    # predict with batch_size=1 to\n",
    "    # initialize the model to the last value of\n",
    "    # of the training set\n",
    "    model.predict(X_train, batch_size=1)\n",
    "\n",
    "def one_step_forecast(model, batch_size, X):\n",
    "    \"\"\"Makes a single-step forecast\n",
    "    Args:\n",
    "        model: the trained model\n",
    "        batch_size: the batch size\n",
    "        X: input for the next step as a numpy.ndarray\n",
    "    Returns:\n",
    "        forecast for the next step as a scalar\n",
    "    \"\"\"\n",
    "    # reshape to samples, time steps, features\n",
    "    X = X.reshape(1, 1, 1)\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_values = test['t'].values\n",
    "\n",
    "# make predictions as a column list of lists, to\n",
    "# match how the original scaler is fitted\n",
    "predictions = [[one_step_forecast(model, 1, test_values[i])] \n",
    "               for i in range(len(test_values))]\n",
    "\n",
    "# invert scaling\n",
    "predictions = inverse_scale(scaler, predictions)\n",
    "\n",
    "# invert differencing\n",
    "last_training_sample = df_ft.values[split_index-1]\n",
    "predictions = inverse_difference(last_training_sample,\n",
    "                                 pd.DataFrame(predictions))\n",
    "\n",
    "# shift predictions to exclude the last_training_sample\n",
    "# which was only used for inverting the differencing\n",
    "rnn_predictions = predictions.values[1:]\n",
    "print(\"RNN predictions:\", rnn_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# plot the predictions vs baseline vs truth\n",
    "_, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "_, truth = create_train_test(df_ft, \"2012\")\n",
    "truth_values = truth['t+1'].values\n",
    "f1, = ax.plot(truth_values, marker='o')\n",
    "f2, = ax.plot(baseline_predictions, color='red', marker='o')\n",
    "f3, = ax.plot(rnn_predictions, color='black', marker='o')\n",
    "ax.legend([f1, f2, f3], ['truth', 'baseline prediction', 'RNN prediction'])\n",
    "plt.show()\n",
    "\n",
    "score_rnn = mean_squared_error(truth_values, rnn_predictions)\n",
    "print(\"RNN MSE:\", score_rnn)\n",
    "\n",
    "score_baseline = mean_squared_error(truth_values, baseline_predictions)\n",
    "print(\"Baseline MSE:\", score_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Forecasting\n",
    "\n",
    "Assuming the RNN can be tuned to be sufficiently accurate, we can also use it to predict future readings.\n",
    "\n",
    "This is one advantage it offers over the baseline model, which will not be as useful for forecasting after time_step+1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================================\n",
    "# Exercise: use the RNN to predict the next 3 years (2018-2020)\n",
    "# and plot the forecasted values\n",
    "#\n",
    "# Hint: use np.asarray to convert a single prediction 'row'\n",
    "#       entry to a numpy array\n",
    "\n",
    "# predict based on original test values (up to 2017)\n",
    "predictions = [[one_step_forecast(model, 1, test_values[i])] \n",
    "               for i in range(len(test_values))]\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot the predictions vs baseline vs truth\n",
    "_, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "f1, = ax.plot(truth_values, marker='o')\n",
    "f2, = ax.plot(baseline_predictions, color='red', marker='o')\n",
    "f3, = ax.plot(rnn_predictions, color='green', marker='o')\n",
    "ax.legend([f1, f2, f3],\n",
    "          ['truth', 'baseline prediction', 'RNN forecast'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Practice Exercises:\n",
    "\n",
    "1. Try tuning the hyperparameters for the LSTM network to improve its score: more epochs, more neurons\n",
    "2. Try a GRU instead of LSTM, and compare the results\n",
    "3. Skip differencing to see what the effect is on accuracy\n",
    "4. Try using your own time series dataset. This is good practice for data processing and problem definition.\n",
    "\n",
    "When doing these exercises, practice using TensorBoard and the score to evaluate the difference in performance. Which method will you recommend most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Further Reading\n",
    "\n",
    "- [Engineering Extreme Event Forecasting at Uber with Recurrent Neural Networks](https://eng.uber.com/neural-networks/)\n",
    "- [Instability of Online Learning for Stateful LSTM for Time Series Forecasting](https://machinelearningmastery.com/instability-online-learning-stateful-lstm-time-series-forecasting/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
