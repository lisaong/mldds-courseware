{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Article Sentiment Classification\n",
    "\n",
    "In this hands-on workshop, we'll practice classifying text on a slightly larger dataset.\n",
    "\n",
    "Input: 8000 news articles that are labeled by relevance to the US Economy.\n",
    "\n",
    "Task: Fit a model that classifies the articles based on whether each is relevant to the US Economy. This is a binary classification task.\n",
    "\n",
    "Other uses of the same dataset (feel free to practice these on your owntime):\n",
    "- Multi-class classification on positivity score\n",
    "- Regression on relevance confidence, for the relevant articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "CSV: https://www.figure-eight.com/wp-content/uploads/2016/03/Full-Economic-News-DFE-839861.csv\n",
    "\n",
    "Source: https://www.figure-eight.com/data-for-everyone/\n",
    "\n",
    "Description:\n",
    "\n",
    ">Contributors read snippets of news articles. They then noted if the article was relevant to the US economy and, if so, what the tone of the article was. Tone was judged on a 9 point scale (from 1 to 9, with 1 representing the most negativity). Dataset contains these judgments as well as the dates, source titles, and text. Dates range from 1951 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration (20 minutes)\n",
    "\n",
    "1. Load the data into pandas\n",
    "2. Check for NaNs (if any) and decide what you would do with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.csv', encoding='latin1',\n",
    "                usecols=['relevance', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(df.loc[df.relevance=='not sure'].index, inplace=True)\n",
    "df.relevance.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation (60 minutes)\n",
    "\n",
    "1. Convert the relevance \"yes/no\" into numbers using LabelEncoder\n",
    "2. Perform text feature extraction on the \"text\" column:\n",
    "\n",
    "   a. Use spaCy to tokenize and lemmatize.\n",
    "   \n",
    "   b. Pass the tokenized text into TfidfVectorizer. You can decide to train with 1-gram or 2-gram (or more if your computer is faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['y'] = encoder.fit_transform(df.relevance)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# transformation function to use in pd.DataFrame.apply()\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizes the text by lemmatizing and removing stop words\n",
    "    Args:\n",
    "        text - the input text\n",
    "    Returns:\n",
    "        a list of tokens\n",
    "    \"\"\"\n",
    "    # process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # https://spacy.io/api/token\n",
    "    lemmas = [lemmatizer(token.text, token.pos_) for token in doc\n",
    "              if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # create a string\n",
    "    return ' '.join([item for sublist in lemmas for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "%time tokenize_text(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the column (takes about 10 minutes)\n",
    "# %time df_tokenized = df.text.apply(tokenize_text)\n",
    "\n",
    "# Save the tokenized columns to CSV, so that we don't have to tokenize again\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\n",
    "\n",
    "# df['tokenized'] = df_tokenized\n",
    "# df.to_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.tokenized.csv', encoding='latin1',\n",
    "#          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "      <td>1</td>\n",
       "      <td>new york yield certificate deposit offer major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>the wall street journal the morning brief look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>washington in effort achieve banking reform se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the statistic enormous cost employee drug abus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "      <td>1</td>\n",
       "      <td>new york indecision mark dollar tone trader pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                               text  y  \\\n",
       "0       yes  NEW YORK -- Yields on most certificates of dep...  1   \n",
       "1        no  The Wall Street Journal Online</br></br>The Mo...  0   \n",
       "2        no  WASHINGTON -- In an effort to achieve banking ...  0   \n",
       "3        no  The statistics on the enormous costs of employ...  0   \n",
       "4       yes  NEW YORK -- Indecision marked the dollar's ton...  1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  new york yield certificate deposit offer major...  \n",
       "1  the wall street journal the morning brief look...  \n",
       "2  washington in effort achieve banking reform se...  \n",
       "3  the statistic enormous cost employee drug abus...  \n",
       "4  new york indecision mark dollar tone trader pa...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.tokenized.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    new york yield certificate deposit offer major...\n",
       "1    the wall street journal the morning brief look...\n",
       "2    washington in effort achieve banking reform se...\n",
       "3    the statistic enormous cost employee drug abus...\n",
       "4    new york indecision mark dollar tone trader pa...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized = df.tokenized\n",
    "df_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['aa', 'aaa', 'aacer', 'aadvantage', 'aagrf', 'aairl', 'aalrl', 'aame', 'aap', 'aaron']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Apply the vectorizer on all tokenized rows\n",
    "vectorizer = TfidfVectorizer(lowercase=False, decode_error='ignore')\n",
    "\n",
    "X = vectorizer.fit_transform(df_tokenized)\n",
    "\n",
    "# print a few features\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    6571\n",
       "1    1420\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of the relevant / irrelevant articles\n",
    "df.groupby(['y']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize (20 minutes)\n",
    "\n",
    "Use `TSNE` to visualize the Tfidf vectors into a 2-D plot or 1-D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sample a portion of the dataset for t-SNE (otherwise too slow)\n",
    "density = 0.3\n",
    "\n",
    "df_display = pd.DataFrame(X.todense())\n",
    "df_display['y'] = df.y\n",
    "sample = df_display.sample(frac=density, random_state=42)\n",
    "sample_X = sample.iloc[:, :-1] # pick all but the last column ('y')\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "%time X_2d = tsne.fit_transform(sample_X)\n",
    "\n",
    "print(X_2d.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], c=sample.y)\n",
    "\n",
    "ax.set(title='t-SNE plot for News Articles (X) and Relevance (y), density %.2f' % density,\n",
    "       xlabel='X_2d[:, 0]', ylabel='X_2d[:, 1]')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=1, random_state=42)\n",
    "%time X_1d = tsne.fit_transform(sample_X)\n",
    "\n",
    "print(X_1d.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.scatter(X_1d, sample.y)\n",
    "\n",
    "ax.set(title='t-SNE 1D plot for News Articles (X) and Relevance (y), density %.2f' % density,\n",
    "       xlabel='X_1d', ylabel='y')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (30 minutes)\n",
    "\n",
    "1. Train/test split and shuffle the dataset\n",
    "2. Train your favourite models with the dataset.  Keep in mind that KNeighbors, SVM and MLPClassifier can be slow.\n",
    "3. Get classification_report metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_sparse, df.y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifiers = [\n",
    "    DummyClassifier(random_state=42),\n",
    "    LogisticRegression(random_state=42),\n",
    "    SGDClassifier(tol=1e-3, max_iter=1000, random_state=42),\n",
    "    GaussianNB(),\n",
    "    DecisionTreeClassifier(random_state=42),\n",
    "    MLPClassifier(random_state=42)\n",
    "]\n",
    "\n",
    "for clf in classifiers:\n",
    "    print(type(clf))\n",
    "    %time clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of plotting the ROC curves to compare classifiers\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(nrows=2, ncols=1, figsize=(20, 20))\n",
    "\n",
    "#\n",
    "# plot the dummy classifier\n",
    "#\n",
    "baseline = classifiers[0] # see list above\n",
    "y_confidence_baseline = baseline.predict_proba(X_test)\n",
    "\n",
    "# positive label = class 0\n",
    "fpr0, tpr0, _ = roc_curve(y_test, y_confidence_baseline[:, 0], pos_label=0)\n",
    "ax0.plot(fpr0, tpr0, label='Baseline (area = %f)' % auc(fpr0, tpr0), linestyle='dashed')\n",
    "\n",
    "# positive label = class 1\n",
    "fpr1, tpr1, _ = roc_curve(y_test, y_confidence_baseline[:, 1], pos_label=1)\n",
    "ax1.plot(fpr1, tpr1, label='Baseline (area = %f)' % auc(fpr1, tpr1), linestyle='dashed')\n",
    "\n",
    "#\n",
    "# plot the balanced classifiers\n",
    "#\n",
    "for clf in balanced_classifiers:\n",
    "    # some classifiers provide .decision_function(), others provide .predict_proba()\n",
    "    if hasattr(clf, 'decision_function'):\n",
    "        y_confidence = clf.decision_function(X_test)\n",
    "    \n",
    "        # positive label = class 0\n",
    "        fpr0, tpr0, _ = roc_curve(y_test, y_confidence, pos_label=0)\n",
    "\n",
    "        # positive label = class 1\n",
    "        fpr1, tpr1, _ = roc_curve(y_test, y_confidence, pos_label=1)    \n",
    "    else:\n",
    "        y_confidence = clf.predict_proba(X_test)\n",
    "\n",
    "        # positive label = class 0\n",
    "        fpr0, tpr0, _ = roc_curve(y_test, y_confidence[:, 0], pos_label=0)\n",
    "\n",
    "        # positive label = class 1\n",
    "        fpr1, tpr1, _ = roc_curve(y_test, y_confidence[:, 1], pos_label=1)    \n",
    "\n",
    "    ax1.plot(fpr1, tpr1, label='%s (area = %f)' % (clf.__class__, auc(fpr1, tpr1)))\n",
    "    ax0.plot(fpr0, tpr0, label='%s (area = %f)' % (clf.__class__, auc(fpr0, tpr0)))        \n",
    "\n",
    "\n",
    "# put the plots together\n",
    "ax0.set(xlabel='false positive rate', ylabel='true positive rate',\n",
    "       title='ROC curves (positive label=\"irrelevant\")')\n",
    "ax0.legend()\n",
    "ax0.grid()\n",
    "\n",
    "ax1.set(xlabel='false positive rate', ylabel='true positive rate',\n",
    "       title='ROC curves (positive label=\"relevant\")')\n",
    "ax1.legend()\n",
    "ax1.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions (30 minutes)\n",
    "\n",
    "1. Go to reuters.com or anywhere and find any news article.\n",
    "2. Get the text of the article and pass it to your best performing model.\n",
    "\n",
    "  a. You need to first tokenize and lemmatize your test text\n",
    "  \n",
    "  b. Apply `TfidfVectorizer` to get the test input vector\n",
    "  \n",
    "  c. Run `predict` to see if your model correctly classified the text as relevant to the US Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_news = [\"\"\"A sell-off in Chinese markets knocked Asian stocks on Wednesday as U.S. threats of tariffs on an additional $200 billion worth of Chinese goods pushed the world’s two biggest economies ever closer to a full-scale trade war.\n",
    "FILE PHOTO: People walk past an electronic board showing Japan's Nikkei average outside a brokerage in Tokyo, Japan, March 23, 2018. REUTERS/Toru Hanai\n",
    "Washington proposed the extra tariffs after efforts to negotiate a solution to the dispute failed to reach an agreement, senior administration officials said on Tuesday.\n",
    "\n",
    "The United States had just imposed tariffs on $34 billion worth of Chinese goods on Friday, drawing immediate retaliatory duties from Beijing on U.S. imports in the first shots of a heated trade war. U.S. President Donald Trump had warned then that his country may ultimately impose tariffs on more than $500 billion worth of Chinese imports - roughly the total amount of U.S. imports from China last year.\n",
    "\n",
    "“With no early end appearing to be in sight for the escalating ‘tit-for-tat’ world trade frictions and rising trade protectionism, global trade wars have become one of the key downside risks to world growth and trade in the second half of 2018 and for 2019,” wrote Rajiv Biswas, Asia Pacific chief economist at IHS Markit.\n",
    "\n",
    "MSCI’s broadest index of Asia-Pacific shares outside Japan fell 1.1 percent. The index had gained for the past two sessions, having enjoyed a lull from the trade war fears that lashed global markets last week.\"\"\"\n",
    ",\n",
    "\"\"\"TOKYO (Reuters) - Japanese regulators on Wednesday said Apple Inc (AAPL.O) may have breached antitrust rules by forcing mobile service providers to sell its iPhones cheaply and charge higher monthly fees, denying consumers a fair choice.\n",
    "\n",
    "FILE PHOTO: A member of Apple staff takes pictures as new iPhone X begins to sell at an Apple Store in Beijing, China November 3, 2017. REUTERS/Damir Sagolj/File Photo\n",
    "The Fair Trade Commission (FTC) said that Apple had forced NTT Docomo Inc (9437.T) , KDDI Corp (9433.T) and SoftBank Group Corp (9984.T) to offer subsidies and sell iPhones at a discount.\n",
    "\n",
    "Apple Inc\n",
    "190.35\n",
    "AAPL.ONASDAQ\n",
    "-0.00(-0.00%)\n",
    "AAPL.O\n",
    "AAPL.O9437.T9433.T9984.T\n",
    "The FTC, which began looking into Apple’s sales practices in 2016, did not punish Apple as the U.S. company had agreed to revise its contracts with the carriers, it said\n",
    "TOKYO (Reuters) - Japanese regulators on Wednesday said Apple Inc (AAPL.O) may have breached antitrust rules by forcing mobile service providers to sell its iPhones cheaply and charge higher monthly fees, denying consumers a fair choice.\n",
    "\n",
    "FILE PHOTO: A member of Apple staff takes pictures as new iPhone X begins to sell at an Apple Store in Beijing, China November 3, 2017. REUTERS/Damir Sagolj/File Photo\n",
    "The Fair Trade Commission (FTC) said that Apple had forced NTT Docomo Inc (9437.T) , KDDI Corp (9433.T) and SoftBank Group Corp (9984.T) to offer subsidies and sell iPhones at a discount.\n",
    "\n",
    "Apple Inc\n",
    "190.35\n",
    "AAPL.ONASDAQ\n",
    "-0.00(-0.00%)\n",
    "AAPL.O\n",
    "AAPL.O9437.T9433.T9984.T\n",
    "The FTC, which began looking into Apple’s sales practices in 2016, did not punish Apple as the U.S. company had agreed to revise its contracts with the carriers, it said\n",
    "\"\"\"\n",
    "]\n",
    "\n",
    "df_sample_news = pd.DataFrame(data={'text': sample_news})\n",
    "\n",
    "# first tokenize and lemmatize your test text\n",
    "df_sample_news['tokenized_text'] = df_sample_news.text.apply(tokenize_text)\n",
    "df_sample_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply TfidfVectorizer to get the test input vector\n",
    "sample_news = vectorizer.transform(df_sample_news.tokenized_text).todense()\n",
    "print(sample_news.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run predict to see if your model correctly classified the text as relevant to the US Economy\n",
    "\n",
    "for clf in balanced_classifiers:\n",
    "    print(clf.__class__)\n",
    "    predictions = clf.predict(sample_news)\n",
    "    \n",
    "    if hasattr(clf, 'predict_proba'):\n",
    "        probabilities = clf.predict_proba(sample_news)\n",
    "        for prediction, probability in (zip(predictions, probabilities)):\n",
    "            print(prediction, probability)\n",
    "    else:\n",
    "        distances = clf.decision_function(sample_news)\n",
    "        for prediction, distance in (zip(predictions, distances)):\n",
    "            print(prediction, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Improvements 1 - Manually Balance Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the distribution of the relevant / irrelevant articles\n",
    "df.groupby(['y']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's cut the irrelevant articles by 1/4\n",
    "# Alternatively, you can also collect and label more relevant articles\n",
    "df_irrelevant = df[df.y == 0].sample(frac=0.25, random_state=42)\n",
    "df_irrelevant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep all the relevant articles\n",
    "df_relevant = df[df.y == 1]\n",
    "df_relevant.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make our balanced dataset\n",
    "df_balanced = pd.concat([df_irrelevant, df_relevant], axis=0) # concat rows\n",
    "df_balanced.groupby(['y']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_balanced.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize, fit, metrics, predict, ...\n",
    "\n",
    "# featurize\n",
    "X = vectorizer.fit_transform(df_balanced.tokenized)\n",
    "\n",
    "# fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_balanced.y, random_state=42)\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# metrics\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "\n",
    "# predict\n",
    "sample_news = vectorizer.transform(df_sample_news.tokenized_text).todense()\n",
    "predictions = clf.predict(sample_news)\n",
    "if hasattr(clf, 'predict_proba'):\n",
    "    probabilities = clf.predict_proba(sample_news)\n",
    "    for prediction, probability in (zip(predictions, probabilities)):\n",
    "        print(prediction, probability)\n",
    "else:\n",
    "    distances = clf.decision_function(sample_news)\n",
    "    for prediction, distance in (zip(predictions, distances)):\n",
    "        print(prediction, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Improvements 2 - N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer_ngram = TfidfVectorizer(ngram_range=(2, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Featurize, fit, metrics, predict, ...\n",
    "\n",
    "# featurize\n",
    "X = vectorizer_ngram.fit_transform(df_balanced_sampled.tokenized)\n",
    "\n",
    "# fit\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_balanced_sampled.y, random_state=42)\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# metrics\n",
    "print(classification_report(y_test, clf.predict(X_test)))\n",
    "\n",
    "# predict\n",
    "sample_news = vectorizer_ngram.transform(df_sample_news.tokenized_text).todense()\n",
    "predictions = clf.predict(sample_news)\n",
    "if hasattr(clf, 'predict_proba'):\n",
    "    probabilities = clf.predict_proba(sample_news)\n",
    "    for prediction, probability in (zip(predictions, probabilities)):\n",
    "        print(prediction, probability)\n",
    "else:\n",
    "    distances = clf.decision_function(sample_news)\n",
    "    for prediction, distance in (zip(predictions, distances)):\n",
    "        print(prediction, distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
