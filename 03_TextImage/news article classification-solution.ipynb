{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# News Article Sentiment Classification\n",
    "\n",
    "In this hands-on workshop, we'll practice classifying text on a slightly larger dataset.\n",
    "\n",
    "Input: 8000 news articles that are labeled by relevance to the US Economy.\n",
    "\n",
    "Task: Fit a model that classifies the articles based on whether each is relevant to the US Economy. This is a binary classification task.\n",
    "\n",
    "Other uses of the same dataset (feel free to practice these on your owntime):\n",
    "- Multi-class classification on positivity score\n",
    "- Regression on relevance confidence, for the relevant articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "CSV: https://www.figure-eight.com/wp-content/uploads/2016/03/Full-Economic-News-DFE-839861.csv\n",
    "\n",
    "Source: https://www.figure-eight.com/data-for-everyone/\n",
    "\n",
    "Description:\n",
    "\n",
    ">Contributors read snippets of news articles. They then noted if the article was relevant to the US economy and, if so, what the tone of the article was. Tone was judged on a 9 point scale (from 1 to 9, with 1 representing the most negativity). Dataset contains these judgments as well as the dates, source titles, and text. Dates range from 1951 to 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration (20 minutes)\n",
    "\n",
    "1. Load the data into pandas\n",
    "2. Check for NaNs (if any) and decide what you would do with them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                               text\n",
       "0       yes  NEW YORK -- Yields on most certificates of dep...\n",
       "1        no  The Wall Street Journal Online</br></br>The Mo...\n",
       "2        no  WASHINGTON -- In an effort to achieve banking ...\n",
       "3        no  The statistics on the enormous costs of employ...\n",
       "4       yes  NEW YORK -- Indecision marked the dollar's ton..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.csv', encoding='latin1',\n",
    "                usecols=['relevance', 'text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['yes', 'no'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(df.loc[df.relevance=='not sure'].index, inplace=True)\n",
    "df.relevance.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation (60 minutes)\n",
    "\n",
    "1. Convert the relevance \"yes/no\" into numbers using LabelEncoder\n",
    "2. Perform text feature extraction on the \"text\" column:\n",
    "\n",
    "   a. Use spaCy to tokenize and lemmatize.\n",
    "   \n",
    "   b. Pass the tokenized text into TfidfVectorizer. You can decide to train with 1-gram or 2-gram (or more if your computer is faster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df['y'] = encoder.fit_transform(df.relevance)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "# transformation function to use in pd.DataFrame.apply()\n",
    "def tokenize_text(text):\n",
    "    \"\"\"Tokenizes the text by lemmatizing and removing stop words\n",
    "    Args:\n",
    "        text - the input text\n",
    "    Returns:\n",
    "        a list of tokens\n",
    "    \"\"\"\n",
    "    # process the text\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # https://spacy.io/api/token\n",
    "    lemmas = [lemmatizer(token.text, token.pos_) for token in doc\n",
    "              if not token.is_stop and token.is_alpha]\n",
    "\n",
    "    # create a string\n",
    "    return ' '.join([item for sublist in lemmas for item in sublist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 57.7 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'new york yield certificate deposit offer major bank drop tenth percentage point late week reflect overall decline short term interest on small denomination consumer cd sell directly bank average yield month deposit fall week end yesterday accord survey banxquote money markets wilmington information on month consumer deposit average yield sink week accord banxquote two bank banxquote survey citibank new york corestates pennsylvania pay threemonth small denomination decline somewhat small year consumer cd ease banxquote yield month month treasury bill sell monday auction plummet fifth percentage point previous week respectively'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "%time tokenize_text(df.text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the column (takes about 10 minutes)\n",
    "# %time df_tokenized = df.text.apply(tokenize_text)\n",
    "\n",
    "# Save the tokenized columns to CSV, so that we don't have to tokenize again\n",
    "# https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.to_csv.html\n",
    "\n",
    "# df['tokenized'] = df_tokenized\n",
    "# df.to_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.tokenized.csv', encoding='latin1',\n",
    "#          index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevance</th>\n",
       "      <th>text</th>\n",
       "      <th>y</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes</td>\n",
       "      <td>NEW YORK -- Yields on most certificates of dep...</td>\n",
       "      <td>1</td>\n",
       "      <td>new york yield certificate deposit offer major...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no</td>\n",
       "      <td>The Wall Street Journal Online&lt;/br&gt;&lt;/br&gt;The Mo...</td>\n",
       "      <td>0</td>\n",
       "      <td>the wall street journal the morning brief look...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>no</td>\n",
       "      <td>WASHINGTON -- In an effort to achieve banking ...</td>\n",
       "      <td>0</td>\n",
       "      <td>washington in effort achieve banking reform se...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>no</td>\n",
       "      <td>The statistics on the enormous costs of employ...</td>\n",
       "      <td>0</td>\n",
       "      <td>the statistic enormous cost employee drug abus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes</td>\n",
       "      <td>NEW YORK -- Indecision marked the dollar's ton...</td>\n",
       "      <td>1</td>\n",
       "      <td>new york indecision mark dollar tone trader pa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  relevance                                               text  y  \\\n",
       "0       yes  NEW YORK -- Yields on most certificates of dep...  1   \n",
       "1        no  The Wall Street Journal Online</br></br>The Mo...  0   \n",
       "2        no  WASHINGTON -- In an effort to achieve banking ...  0   \n",
       "3        no  The statistics on the enormous costs of employ...  0   \n",
       "4       yes  NEW YORK -- Indecision marked the dollar's ton...  1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  new york yield certificate deposit offer major...  \n",
       "1  the wall street journal the morning brief look...  \n",
       "2  washington in effort achieve banking reform se...  \n",
       "3  the statistic enormous cost employee drug abus...  \n",
       "4  new york indecision mark dollar tone trader pa...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('d:/tmp/news-article/Full-Economic-News-DFE-839861.tokenized.csv', encoding='latin1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    new york yield certificate deposit offer major...\n",
       "1    the wall street journal the morning brief look...\n",
       "2    washington in effort achieve banking reform se...\n",
       "3    the statistic enormous cost employee drug abus...\n",
       "4    new york indecision mark dollar tone trader pa...\n",
       "Name: tokenized, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tokenized = df.tokenized\n",
    "df_tokenized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7991, 31876)\n",
      "['aa', 'aaa', 'aacer', 'aadvantage', 'aagrf', 'aairl', 'aalrl', 'aame', 'aap', 'aaron']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Apply the vectorizer on all tokenized rows\n",
    "vectorizer = TfidfVectorizer(lowercase=False, decode_error='ignore')\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = vectorizer.fit_transform(df_tokenized).todense()\n",
    "print(X_dense.shape)\n",
    "\n",
    "# print a few features\n",
    "print(vectorizer.get_feature_names()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "y\n",
       "0    6571\n",
       "1    1420\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the distribution of the relevant / irrelevant articles\n",
    "df.groupby(['y']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize (20 minutes)\n",
    "\n",
    "Use `TSNE` to visualize the Tfidf vectors into a 2-D plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Sample a portion of the dataset for t-SNE (otherwise too slow)\n",
    "density = 0.2\n",
    "\n",
    "df_display = pd.DataFrame(X_dense)\n",
    "df_display['y'] = df.y\n",
    "sample = df_display.sample(frac=density, random_state=42)\n",
    "sample_X = sample.iloc[:, :-1] # pick all but the last column ('y')\n",
    "\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "%time X_2d = tsne.fit_transform(sample_X)\n",
    "\n",
    "print(X_2d.shape)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.scatter(X_2d[:, 0], X_2d[:, 1], c=sample.y)\n",
    "\n",
    "ax.set(title='t-SNE plot for News Articles (X) and Relevance (y), density %.2f' % density,\n",
    "       xlabel='X_2d[:, 0]', ylabel='X_2d[:, 1]')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train (30 minutes)\n",
    "\n",
    "1. Train/test split and shuffle the dataset\n",
    "2. Train your favourite models with the dataset.  Keep in mind that SVM and MLPClassifier can be slow.\n",
    "3. Get classification_report metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dense, df.y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classifiers = [\n",
    "    DummyClassifier(random_state=42),\n",
    "    LogisticRegression(random_state=42),\n",
    "    GaussianNB(),\n",
    "    Deci\n",
    "]\n",
    "\n",
    "for clf in classifiers:\n",
    "    print(type(clf))\n",
    "    clf.fit(X_train, y_train)\n",
    "    pred = clf.predict(X_test)\n",
    "    print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions (30 minutes)\n",
    "\n",
    "1. Go to reuters.com or anywhere and find any news article.\n",
    "2. Get the text of the article and pass it to your best performing model.\n",
    "  a. You need to first tokenize and lemmatize your test text\n",
    "  b. Apply `TfidfVectorizer` to get the test input vector\n",
    "  c. Run `predict` to see if your model correctly classified the text as relevant to the US Economy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
