{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Image Recognition\n",
    "\n",
    "(live long, prosper, and watch out for cats)\n",
    "\n",
    "![cat detectors rule](assets/image/meme.jpg)\n",
    "\n",
    "(image: https://towardsdatascience.com/the-whos-who-of-machine-learning-and-why-you-should-know-them-9cefbbc84f07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Input Representation\n",
    "- Convolutional Neural Networks\n",
    " - Convolution, Activation, Pooling\n",
    "- Architectures\n",
    " - Image Classification\n",
    " - Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Recognition Tasks\n",
    "![cat detectors](assets/image/cat_detectors.png)\n",
    "\n",
    "(image: analyticsindiamag.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Representation: Tensor\n",
    "\n",
    "- 3 channels: 'rgb'\n",
    "- rows: image height\n",
    "- columns: image width\n",
    "\n",
    "Ordering:\n",
    "- Channels-first: channel, rows, columns\n",
    "- Channels-last: rows, columns, channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Walkthrough - Image Tensors\n",
    "\n",
    "In this walkthrough, we will read an image from file and examine the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Install the Python image library from your conda environment:\n",
    "```\n",
    "conda install pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# read an image file\n",
    "demo = Image.open('assets/image/cat.jpg') # source: pxhere.com/en/photo/1337399\n",
    "\n",
    "# check whether this is RGB or BGR\n",
    "# so that we can input the images correctly to our neural network\n",
    "print('channel ordering:', demo.mode)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(demo)\n",
    "plt.title('moar food')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# examine the numpy array\n",
    "demo_arr = np.array(demo)\n",
    "\n",
    "print('shape:', demo_arr.shape)\n",
    "print('data type:', demo_arr.dtype)\n",
    "print('rank:', demo_arr.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# since the sides of the picture are the same boring color\n",
    "# inspect the (roughly) middle 5 rows and columns\n",
    "midpoint_row = int(demo_arr.shape[0] / 2)\n",
    "midpoint_col = int(demo_arr.shape[1] / 2)\n",
    "\n",
    "demo_arr[midpoint_row:midpoint_row+5, midpoint_col:midpoint_col+5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# resize the image to 224 by 224\n",
    "demo.thumbnail((224, 224), resample=Image.BICUBIC)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(demo, interpolation='nearest')\n",
    "plt.title('moar smaller (224x224)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# examine the numpy array again\n",
    "demo_arr = np.array(demo)\n",
    "\n",
    "print(demo_arr.shape)\n",
    "print(demo_arr.dtype)\n",
    "\n",
    "# notice the difference in values from previously\n",
    "midpoint_row = int(demo_arr.shape[0] / 2)\n",
    "midpoint_col = int(demo_arr.shape[1] / 2)\n",
    "demo_arr[midpoint_row:midpoint_row+5, midpoint_col:midpoint_col+5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A histogram is sometimes helpful to visualize the colour distribution of a given channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# order: RGB\n",
    "red_channel = demo_arr[:, :, 0]\n",
    "green_channel = demo_arr[:, :, 1]\n",
    "blue_channel = demo_arr[:, :, 2]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "\n",
    "# flatten the [row_size, col_size] matrix into a vector of [row_size * col_size]\n",
    "# we just need to count the raw pixel values for the histogram,\n",
    "# so it doesn't matter where they are located.\n",
    "ax[0].hist(red_channel.flatten(), 256, range=(0,256), color='red')\n",
    "ax[1].hist(green_channel.flatten(), 256, range=(0,256), color='green')\n",
    "ax[2].hist(blue_channel.flatten(), 256, range=(0,256), color='blue')\n",
    "\n",
    "fig.suptitle('Histogram of input image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output\n",
    "\n",
    "- Image Classification: labels\n",
    "- Object Detection: labels + bounding boxes\n",
    "- Instance Segmentation: labels + boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem: many input features $\\rightarrow$ many parameters\n",
    "\n",
    "224 x 224 pixel colour image: 224 x 224 x 3 = 150528 features\n",
    "\n",
    "Dimensionality reduction may help, but there's a better way...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "- Reduces parameter space\n",
    "- Looks at localized, spatial information\n",
    "\n",
    "![cnn](assets/image/cnn.png)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution - Hyperparmeters\n",
    "\n",
    "- Kernel size: size of the window (pixels)\n",
    "- Stride: how many pixels to slide the window\n",
    "- Depth: how many filters to use\n",
    "- Padding: whether to keep the output size the same as input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel size = (2, 2), Stride = 1\n",
    "\n",
    "![convolution](assets/image/2d_convolution.png)\n",
    "\n",
    "(image: http://www.deeplearningbook.org/contents/convnets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Depth = number of filters\n",
    "\n",
    "![depth col](assets/image/depthcol.jpg)\n",
    "\n",
    "(image: https://cs231n.github.io/convolutional-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding = same\n",
    "\n",
    "![same_padding_no_strides](assets/image/same_padding_no_strides.gif)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding = valid (none)\n",
    "\n",
    "![no_padding_no_strides](assets/image/no_padding_no_strides.gif)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity - Convolution Hyperparameters\n",
    "\n",
    "1. Run and watch the example in the next cell. \n",
    "\n",
    "2. Fill in the corresponding hyperparameters\n",
    "\n",
    " - Kernel Size = \n",
    " - Stride = \n",
    " - Depth = \n",
    " - Padding ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://cs231n.github.io/convolutional-networks\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe src=conv-demo.html width=800 height=700></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough - 2D Convolution\n",
    "\n",
    "In this walkthrough, we will convolve our demo image with a kernel that performs edge detection.\n",
    "\n",
    "Credits: http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# convert our input image to greyscale (1 channel)\n",
    "demo_grey = demo.convert(mode='L')\n",
    "\n",
    "plt.imshow(demo_grey, interpolation='nearest')\n",
    "plt.title('moar grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# edge-detection kernel\n",
    "kernel = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\n",
    "\n",
    "# we use 'valid' which means we do not add zero padding to our image\n",
    "edges = convolve2d(demo_grey, kernel, mode='valid')\n",
    "\n",
    "plt.imshow(edges)\n",
    "plt.title('moar edges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Block\n",
    "\n",
    "Generally, 3 stages:\n",
    "1. Convolutional Layer\n",
    "2. Activation Layer\n",
    "3. Pooling Layer\n",
    "\n",
    "Sometimes 1 & 2 will repeat a few times before 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![conv block](assets/image/convnet.jpg)\n",
    "\n",
    "(image: https://cs231n.github.io/convolutional-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Layer\n",
    "\n",
    "The output of convolution is typically passed through an \"activation\" function, so that it can model non-linearity:\n",
    "\n",
    "Examples:\n",
    "- linear (= no activation)\n",
    "- sigmoid\n",
    "- tanh\n",
    "- Rectified Linear Units, leaky ReLU, Parametric ReLU, Exponential Linear Units\n",
    "\n",
    "https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough -  Activation Functions\n",
    "\n",
    "Let's see what happens when we pass our convolved edge detected image through various activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_arr = edges\n",
    "input_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Sigmoid\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.imshow(sigmoid(edges))\n",
    "plt.title('edges + sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect somewhere in the middle of image (that's not black)\n",
    "input_arr[112:114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(input_arr[112:114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.tanh(input_arr))\n",
    "plt.title('edges + tanh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr[112:114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(input_arr[112:114])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU\n",
    "\n",
    "$f(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "plt.imshow(relu(input_arr))\n",
    "plt.title('edges + ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr[112:114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu(input_arr[112:114])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Leaky ReLU\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "x & if\\,x > 0 \\\\\n",
    "0.01x & otherwise\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "plt.imshow(leaky_relu(input_arr))\n",
    "plt.title('edges + Leaky ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "input_arr[112:114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaky_relu(input_arr[112:114])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layer\n",
    "\n",
    "- Summarizes the activations\n",
    " - Take the maximum of a window size: Max pooling\n",
    " - Take the average of a window size: Average pooling\n",
    "\n",
    "https://keras.io/layers/pooling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layer\n",
    "\n",
    "- Translation invariances:\n",
    "  - Robust to shifts in locations of pixels within that window\n",
    "- Downsampling:\n",
    "  - Compressing and summarizing inputs into next layers\n",
    "  - Pass the highest activation, or the average activations to the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough - Pooling\n",
    "\n",
    "Let's see what happens when we pass our convolved + activated image through various pooling functions.\n",
    "\n",
    "### Setup\n",
    "Install the skimage library in your conda environment:\n",
    "```\n",
    "conda install scikit-image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42463172/how-to-perform-max-mean-pooling-on-a-2d-array-using-numpy\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "def max_pool(x, pool_size=(2, 2)):\n",
    "    return block_reduce(x, pool_size, np.max)\n",
    "\n",
    "def average_pool(x, pool_size=(2, 2)):\n",
    "    return block_reduce(x, pool_size, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(max_pool(sigmoid(edges)))\n",
    "plt.title('edges + sigmoid + max pool')\n",
    "plt.show()\n",
    "\n",
    "print('Original shape:', edges.shape)\n",
    "print('After activation:', sigmoid(edges).shape)\n",
    "\n",
    "# (2, 2) will halve the size of the output\n",
    "print('After max pooling (2, 2):', max_pool(sigmoid(edges)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max Pool\n",
    "\n",
    "pool_size=(2, 2) means that it takes a 2x2 square and outputs 1 number.\n",
    "\n",
    "The number is the maximum of the 4 numbers in that 2x2 square.\n",
    "\n",
    "If the size is not divisible by pool_size, there are two choices:\n",
    "- With zero padding: pad by zeros to make the size divisible. Then perform MaxPooling operation\n",
    "- Without zero padding: behavior is not defined. Some libraries will drop these \"edge\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges[70:71] # somewhere in middle of image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(edges)[70:71] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pool(sigmoid(edges))[70:71]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Before maxpool', sigmoid(edges)[70:71].shape)\n",
    "print('After maxpool (shape / 2)', max_pool(sigmoid(edges))[70:71].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Pool\n",
    "\n",
    "pool_size=(2, 2) means that it takes a 2x2 square and outputs 1 number.\n",
    "\n",
    "The number is the average of the 4 numbers in that 2x2 square.\n",
    "\n",
    "If the size is not divisible by the pool size, there are two choices:\n",
    "- With zero padding: pad by zeros to make the size divisible. Then perform MaxPooling operation\n",
    "- Without zero padding: behavior is not defined. Some libraries will drop these \"edge\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(average_pool(sigmoid(edges)))\n",
    "plt.title('edges + sigmoid + average pool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(max_pool(relu(edges)))\n",
    "plt.title('edges + relu + max pool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(average_pool(relu(edges)))\n",
    "plt.title('edges + relu + average pool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization Layers\n",
    "\n",
    "These layers are common in Deep Convolutional Neural Networks:\n",
    "\n",
    "- Dropout\n",
    "- Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "- Randomly setting some layer inputs to 0 during training to reduce overfitting.\n",
    "- No-op during prediction\n",
    "\n",
    "https://keras.io/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "![dropout](assets/image/dropout.png)\n",
    "\n",
    "(image: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- Avoid saturating non-linearities by normalizing the input to the next layer\n",
    "- Normalizing is done per minibatch\n",
    "- Speeds up training\n",
    "\n",
    "https://keras.io/layers/normalization/#batchnormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization - Training\n",
    "\n",
    "![batch norm](assets/image/batchnorm.png)\n",
    "\n",
    "(image: [Batch Normalization Paper](https://arxiv.org/abs/1502.03167))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Batch Normalization - Prediction\n",
    "\n",
    "- Minibatch mean and variance don't apply at prediction time\n",
    "- Instead, \"population\" mean and variance is computed and stored from training\n",
    "- Batch Norm layer will use the population mean and variance for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Architectures\n",
    "\n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Instance Segmentation\n",
    "\n",
    "https://github.com/keras-team/keras-applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## ImageNet Large Scale Visual Recognition Challenge (ILSVRC)\n",
    "\n",
    "- http://image-net.org/index\n",
    "- 14 million crowdsourced, hand-annotated images\n",
    "  - http://image-net.org/explore\n",
    "  - http://image-net.org/download-bboxes\n",
    "- Annual challenge, since 2010\n",
    "  - Classifiy images into 1000 classes (including 90 dog breeds)\n",
    "  - CNN breakthrough in 2012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## ImageNet Progress\n",
    "\n",
    "![over the years](assets/image/pyconza17-deep-learning-for-computer-vision-60-638.jpg)\n",
    "\n",
    "(image: [Alex Conway](https://www.slideshare.net/AlexConway2/pyconza17-deep-learning-for-computer-vision))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Workshop: Image Classification CNNs\n",
    "\n",
    "In this workshop, we'll explore a couple of Convolutional Neural Networks (CNNs) that are trained for ImageNet to perform accurate image classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We'll classify an image with the VGG16 model.\n",
    "\n",
    "- Documentation: https://keras.io/applications/#vgg16\n",
    "\n",
    "- Implementation: https://github.com/keras-team/keras-applications/blob/master/keras_applications/vgg16.py\n",
    "\n",
    "Note that the weights are stored separately from the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import VGG16\n",
    "\n",
    "vgg16 = VGG16()\n",
    "print(vgg16.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Download Image\n",
    "1. Download an image from the web. It can be any well-known image format such as PNG or JPEG.\n",
    "2. Update the `path` variable to point to your image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update this path to your own image\n",
    "path = 'assets/image/cat.jpg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-process image\n",
    "\n",
    "1. Resize the crop the image into height: 224, width: 224\n",
    "  - These dimensions are specified by the neural network as the default input size\n",
    "2. Pre-process the image using `VGG16.preprocess_input`\n",
    "3. Prepend an extra axis to match the input shape that the model expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_crop_image(image_path, width, height):\n",
    "    \"\"\"Resizes and crops an image to the desired size\n",
    "    Args:\n",
    "        image_path: path to the image\n",
    "        width: image width\n",
    "        height: image height\n",
    "    Returns:\n",
    "        the resulting image\n",
    "    \"\"\"\n",
    "    from PIL import Image, ImageOps\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = ImageOps.fit(img, (width, height))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess image input\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "img = resize_and_crop_image(path, 224, 224)\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "print('Original image shape:', x.shape)\n",
    "\n",
    "# The model expect images in a batch, because it's trained that way\n",
    "# Add an extra first axis\n",
    "x = np.expand_dims(x, axis=0)\n",
    "print('Expected input shape', x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Predict and decode predictions\n",
    "\n",
    "1. Run the `VGG16.predict` method on the pre-processed image.\n",
    "2. Use the `VGG16.decode_predictions` method to translate the raw predicted values into class labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y = vgg16.predict(x)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(img, interpolation='nearest')\n",
    "plt.title('Test image')\n",
    "plt.show()\n",
    "\n",
    "# display the predictions\n",
    "print('Raw predictions:')\n",
    "print(y)\n",
    "\n",
    "print('Decoded predictions:')\n",
    "print(decode_predictions(y, top=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: Choose your own Image Classification CNN\n",
    "\n",
    "1. Pick another CNN from this list: https://keras.io/applications/#available-models\n",
    " - Note: we are using models from `keras.applications`. The next workshop will cover using a keras model from a third-party source.\n",
    "2. Repeat the steps above to:\n",
    " - Examine the model by getting its summary\n",
    " - Resize, crop, and preprocess the image using that model's `preprocess_input` method.\n",
    "   - For best results, stick to the input size that is documented for the model\n",
    " - Get raw predictions\n",
    " - Decode the predictions using that model's `decode_predictions` method.\n",
    "\n",
    "Can you find a model that performs better than `VGG16`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Workshop: Object Detection CNN\n",
    "\n",
    "In this workshop, we'll explore an Object Detection CNN.\n",
    "\n",
    "Object detection CNNs are not part of `keras.applications`, so we need to download the models separately, and then load it into keras.\n",
    "\n",
    "We'll try this model: https://github.com/qqwweee/keras-yolo3.\n",
    "\n",
    "This is a keras wrapper for the YOLO (you only look once) v3 model: https://pjreddie.com/darknet/yolo/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Clone the keras-yolo3 repository\n",
    "\n",
    "From a command window:\n",
    "```\n",
    "git clone https://github.com/qqwweee/keras-yolo3.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Download pre-trained YOLO v3 weights\n",
    "\n",
    "Download https://pjreddie.com/media/files/yolov3.weights into your `keras-yolo3` folder.\n",
    "\n",
    "Note: This file is 237MB, and may take a few minutes to download. While we are waiting, you can run the next cell (Examine YOLO v3 architecture)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Examine YOLO v3 architecture\n",
    "\n",
    "(This step can be done in parallel with the download.)\n",
    "\n",
    "Open yolov3.cfg in the `keras-yolo3` folder in a text editor.\n",
    "\n",
    "Scroll through the file, and you should recognize a few keywords. This config file defines the YOLO v3 architecture.\n",
    "\n",
    "Here's how the hyperparameters are set for one of the many convolutional blocks:\n",
    "\n",
    "```\n",
    "[convolutional]\n",
    "batch_normalize=1\n",
    "filters=64\n",
    "size=3\n",
    "stride=2\n",
    "pad=1\n",
    "activation=leaky\n",
    "```\n",
    "\n",
    "The above describes a convolutional block with:\n",
    "- depth 64\n",
    "- kernel size 3\n",
    "- stride 2\n",
    "- padding \"same\"\n",
    "- batch normalization\n",
    "- leaky ReLU activation\n",
    "- no pooling layer\n",
    "\n",
    "These design choices are specific to this architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Convert into Keras model\n",
    "\n",
    "#### Windows:\n",
    "\n",
    "Launch an Anaconda Prompt:\n",
    "\n",
    "```\n",
    "activate mldds03\n",
    "cd keras-yolo3\n",
    "python convert.py yolov3.cfg yolov3.weights model_data\\yolo.h5\n",
    "```\n",
    "\n",
    "#### MacOS / Ubuntu:\n",
    "\n",
    "From a terminal window:\n",
    "\n",
    "```\n",
    "source activate mldds03\n",
    "cd keras-yolo3\n",
    "python convert.py yolov3.cfg yolov3.weights model_data/yolo.h5\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If successful, you should see this:\n",
    "\n",
    "```\n",
    "Parsing section yolo_2\n",
    "__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to\n",
    "==================================================================================================\n",
    "input_1 (InputLayer)            (None, None, None, 3 0\n",
    "__________________________________________________________________________________________________\n",
    "conv2d_1 (Conv2D)               (None, None, None, 3 864         input_1[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "batch_normalization_1 (BatchNor (None, None, None, 3 128         conv2d_1[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 3 0           batch_normalization_1[0][0]\n",
    "\n",
    "...\n",
    "\n",
    "__________________________________________________________________________________________________\n",
    "batch_normalization_72 (BatchNo (None, None, None, 2 1024        conv2d_74[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "leaky_re_lu_58 (LeakyReLU)      (None, None, None, 1 0           batch_normalization_58[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "leaky_re_lu_65 (LeakyReLU)      (None, None, None, 5 0           batch_normalization_65[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "leaky_re_lu_72 (LeakyReLU)      (None, None, None, 2 0           batch_normalization_72[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "conv2d_59 (Conv2D)              (None, None, None, 2 261375      leaky_re_lu_58[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "conv2d_67 (Conv2D)              (None, None, None, 2 130815      leaky_re_lu_65[0][0]\n",
    "__________________________________________________________________________________________________\n",
    "conv2d_75 (Conv2D)              (None, None, None, 2 65535       leaky_re_lu_72[0][0]\n",
    "==================================================================================================\n",
    "Total params: 62,001,757\n",
    "Trainable params: 61,949,149\n",
    "Non-trainable params: 52,608\n",
    "__________________________________________________________________________________________________\n",
    "None\n",
    "Saved Keras model to model_data\\yolo.h5\n",
    "Read 62001757 of 62001757.0 from Darknet weights.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### A few interesting things:\n",
    "\n",
    "- This is a big model (62 million parameters)\n",
    "- The architect of this network decided to insert batch normalization **between** the convolutional layer and its activation\n",
    "- Pooling is not in use at all\n",
    "\n",
    "Next, let's use the model to perform object detection on images and videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Image detection\n",
    "\n",
    "```\n",
    "python yolo.py\n",
    "```\n",
    "\n",
    "Enter an input image filename, and you should get something like this:\n",
    "\n",
    "![memed](assets/image/yolo_v3_demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Video detection\n",
    "\n",
    "Video detection works by processing the image, one frame at a time.\n",
    "\n",
    "#### Setup - OpenCV\n",
    "To run video detection, you need to install OpenCV\n",
    "```\n",
    "conda install opencv\n",
    "```\n",
    "\n",
    "or\n",
    "```\n",
    "conda install -c conda-forge opencv\n",
    "```\n",
    "\n",
    "#### [Optional] Setup - openh264\n",
    "\n",
    "To display detection boxes embedded in the video, download the libopenh264 codec:\n",
    "1. Download https://github.com/cisco/openh264/releases\n",
    "2. Extract the .bz2 (on Windows, use Winzip or WinRAR)\n",
    "3. Copy openh264-1.6.0-win64msvc.dll to the `keras-yolo3` folder\n",
    "\n",
    "This is just for display purposes. The model will still detect objects without this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "1. Download a video (in the smallest resolution you can find), for example:\n",
    "https://pixabay.com/en/videos/street-it-s-raining-11067/\n",
    "\n",
    "2. Run the video detector\n",
    "\n",
    "```\n",
    "python yolo_video.py video.mp4 output.mp4\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example output for an example video:\n",
    "\n",
    "![video](assets/image/yolo_v3_video.png)\n",
    "\n",
    "```\n",
    "Found 7 boxes for img\n",
    "truck 0.50 (19, 282) (250, 538)\n",
    "car 0.46 (320, 374) (393, 435)\n",
    "car 0.57 (221, 355) (343, 435)\n",
    "car 0.65 (386, 381) (488, 442)\n",
    "car 0.74 (19, 282) (250, 538)\n",
    "car 0.92 (554, 354) (833, 526)\n",
    "person 0.72 (498, 354) (566, 464)\n",
    "3.7216578782751952\n",
    "(416, 416, 3)\n",
    "Found 6 boxes for img\n",
    "car 0.51 (322, 374) (391, 434)\n",
    "car 0.69 (386, 382) (485, 442)\n",
    "car 0.77 (219, 354) (340, 435)\n",
    "car 0.90 (16, 282) (260, 538)\n",
    "car 0.92 (553, 353) (835, 526)\n",
    "person 0.75 (499, 353) (566, 465)\n",
    "3.7170675099385093\n",
    "(416, 416, 3)\n",
    "Found 6 boxes for img\n",
    "car 0.70 (322, 375) (393, 434)\n",
    "car 0.78 (390, 382) (483, 443)\n",
    "car 0.86 (14, 285) (265, 539)\n",
    "car 0.91 (220, 354) (338, 440)\n",
    "car 0.92 (553, 353) (834, 527)\n",
    "person 0.91 (495, 349) (565, 466)\n",
    "3.7226787045414973\n",
    "(416, 416, 3)\n",
    "Found 7 boxes for img\n",
    "truck 0.58 (9, 290) (268, 536)\n",
    "car 0.68 (18, 304) (286, 524)\n",
    "car 0.71 (323, 375) (395, 434)\n",
    "car 0.86 (390, 382) (483, 444)\n",
    "car 0.90 (219, 355) (340, 440)\n",
    "car 0.92 (554, 354) (834, 527)\n",
    "person 0.87 (495, 349) (565, 464)\n",
    "3.609754515084603\n",
    "(416, 416, 3)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Object Detection using YOLO\n",
    "\n",
    "![yolo detection](assets/image/yolo-detection.png)\n",
    "\n",
    "(image: https://pjreddie.com/media/files/papers/yolo.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "keras_yolo3_path = 'D:/tmp/keras-yolo3/model_data' # update to your path\n",
    "\n",
    "# trained weights\n",
    "model_path = os.path.join(keras_yolo3_path, 'yolo.h5')\n",
    "\n",
    "# anchors\n",
    "anchors_path = os.path.join(keras_yolo3_path, 'yolo_anchors.txt')\n",
    "\n",
    "# classes\n",
    "classes_path = os.path.join(keras_yolo3_path, 'coco_classes.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These are the pre-defined anchors, chosen to be representative of the ground truth detection boxes.\n",
    "\n",
    "These are found using K-means clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(anchors_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the classes of objects that the detector will recognize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(classes_path, 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Finally, this is the model architecture, as seen by keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# This file is really huge, so may take some time to load\n",
    "yolo_model = load_model(model_path, compile=False)\n",
    "yolo_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "|Material|Read it for|URL\n",
    "|--|--|--|\n",
    "|Deep Learning - Chapter 9.2: Motivation (p 329-335)|3 motivations for convolution|http://www.deeplearningbook.org/contents/convnets.html|\n",
    "|Deep Learning - Chapter 9.3: Motivation (p 335-339)|The idea behind pooling|http://www.deeplearningbook.org/contents/convnets.html|\n",
    "|Bounding box object detectors: understanding YOLO, You Look Only Once|An overview of YOLO|https://christopher5106.github.io/object/detectors/2017/08/10/bounding-box-object-detectors-understanding-yolo.html|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
