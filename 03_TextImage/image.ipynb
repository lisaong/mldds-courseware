{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Image Recognition\n",
    "\n",
    "(live long, prosper, and watch out for cats)\n",
    "\n",
    "![cat detectors rule](assets/image/meme.jpg)\n",
    "\n",
    "(image: https://towardsdatascience.com/the-whos-who-of-machine-learning-and-why-you-should-know-them-9cefbbc84f07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Input Representation\n",
    "- Convolutional Neural Networks\n",
    " - Convolution, Activation, Pooling\n",
    "- Architectures\n",
    " - Image Classification\n",
    " - Object Detection\n",
    " - Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Recognition Tasks\n",
    "![cat detectors](assets/image/cat_detectors.png)\n",
    "\n",
    "(image: analyticsindiamag.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Representation: Tensor\n",
    "\n",
    "- 3 channels: 'rgb'\n",
    "- rows: image height\n",
    "- columns: image width\n",
    "\n",
    "Ordering:\n",
    "- Channels-first: channel, rows, columns\n",
    "- Channels-last: rows, columns, channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Walkthrough - Image Tensors\n",
    "\n",
    "In this walkthrough, we will read an image from file and examine the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Install the Python image library:\n",
    "```\n",
    "conda install pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# read an image file\n",
    "demo = Image.open('assets/image/cat.jpg') # source: pxhere.com/en/photo/1337399\n",
    "\n",
    "# check whether this is RGB or BGR\n",
    "# so that we can input the images correctly to our neural network\n",
    "print('channel ordering:', demo.mode)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(demo)\n",
    "plt.title('moar food')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# examine the numpy array\n",
    "demo_arr = np.array(demo)\n",
    "\n",
    "print('shape:', demo_arr.shape)\n",
    "print('data type:', demo_arr.dtype)\n",
    "print('rank:', demo_arr.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# since the sides of the picture are the same boring color\n",
    "# inspect the (roughly) middle 5 rows and columns\n",
    "midpoint_row = int(demo_arr.shape[0] / 2)\n",
    "midpoint_col = int(demo_arr.shape[1] / 2)\n",
    "\n",
    "demo_arr[midpoint_row:midpoint_row+5, midpoint_col:midpoint_col+5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# resize the image to 224 by 224\n",
    "demo.thumbnail((224, 224), resample=Image.BICUBIC)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(demo, interpolation='nearest')\n",
    "plt.title('moar food (224x224)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# examine the numpy array again\n",
    "demo_arr = np.array(demo)\n",
    "\n",
    "print(demo_arr.shape)\n",
    "print(demo_arr.dtype)\n",
    "\n",
    "# notice the difference in values from previously\n",
    "midpoint_row = int(demo_arr.shape[0] / 2)\n",
    "midpoint_col = int(demo_arr.shape[1] / 2)\n",
    "demo_arr[midpoint_row:midpoint_row+5, midpoint_col:midpoint_col+5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A histogram is sometimes helpful to visualize the colour distribution of a given channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# order: RGB\n",
    "red_channel = demo_arr[:, :, 0]\n",
    "green_channel = demo_arr[:, :, 1]\n",
    "blue_channel = demo_arr[:, :, 2]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "\n",
    "# flatten the [row_size, col_size] matrix into a vector of [row_size * col_size]\n",
    "# we just need to count the raw pixel values for the histogram,\n",
    "# so it doesn't matter where they are located.\n",
    "ax[0].hist(red_channel.flatten(), 256, range=(0,256), color='red')\n",
    "ax[1].hist(green_channel.flatten(), 256, range=(0,256), color='green')\n",
    "ax[2].hist(blue_channel.flatten(), 256, range=(0,256), color='blue')\n",
    "\n",
    "fig.suptitle('Histogram of input image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output\n",
    "\n",
    "- Image Classification: labels\n",
    "- Object Detection: labels + bounding boxes\n",
    "- Instance Segmentation: labels + boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem: many input features $\\rightarrow$ many parameters\n",
    "\n",
    "224 x 224 pixel colour image: 224 x 224 x 3 = 150528 features\n",
    "\n",
    "Dimensionality reduction may help, but there's a better way...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "- Reduces parameter space\n",
    "- Looks at localized, spatial information\n",
    "\n",
    "![cnn](assets/image/cnn.png)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution - Hyperparmeters\n",
    "\n",
    "- Kernel size: size of the window (pixels)\n",
    "- Stride: how many pixels to slide the window\n",
    "- Depth: how many filters to use\n",
    "- Padding: whether to keep the output size the same as input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel size = (2, 2), Stride = 1\n",
    "\n",
    "![convolution](assets/image/2d_convolution.png)\n",
    "\n",
    "(image: http://www.deeplearningbook.org/contents/convnets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Depth = number of filters\n",
    "\n",
    "![depth col](assets/image/depthcol.jpg)\n",
    "\n",
    "(image: https://cs231n.github.io/convolutional-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding = same\n",
    "\n",
    "![same_padding_no_strides](assets/image/same_padding_no_strides.gif)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding = valid (none)\n",
    "\n",
    "![no_padding_no_strides](assets/image/no_padding_no_strides.gif)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity - Convolution Hyperparameters\n",
    "\n",
    "1. Run and watch the example in the next cell. \n",
    "\n",
    "2. Fill in the corresponding hyperparameters\n",
    "\n",
    " - Kernel Size = \n",
    " - Stride = \n",
    " - Depth = \n",
    " - Padding ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://cs231n.github.io/convolutional-networks\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe src=conv-demo.html width=800 height=700></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough - 2D Convolution\n",
    "\n",
    "In this walkthrough, we will convolve our demo image with a kernel that performs edge detection.\n",
    "\n",
    "Credits: http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# convert our input image to greyscale (1 channel)\n",
    "demo_grey = demo.convert(mode='L')\n",
    "\n",
    "plt.imshow(demo_grey, interpolation='nearest')\n",
    "plt.title('moar grey')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# edge-detection kernel\n",
    "kernel = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\n",
    "\n",
    "# we use 'valid' which means we do not add zero padding to our image\n",
    "edges = convolve2d(demo_grey, kernel, mode='valid')\n",
    "\n",
    "plt.imshow(edges)\n",
    "plt.title('moar edges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Block\n",
    "\n",
    "Generally, 3 stages:\n",
    "1. Convolutional Layer\n",
    "2. Activation Layer\n",
    "3. Pooling Layer\n",
    "\n",
    "Sometimes 1 & 2 will repeat a few times before 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![conv block](assets/image/convnet.jpg)\n",
    "\n",
    "(image: https://cs231n.github.io/convolutional-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Layer\n",
    "\n",
    "The output of convolution is typically passed through an \"activation\" function, so that it can model non-linearity:\n",
    "\n",
    "Examples:\n",
    "- linear (= no activation)\n",
    "- sigmoid\n",
    "- tanh\n",
    "- Rectified Linear Units, leaky ReLU, Parametric ReLU, Exponential Linear Units\n",
    "\n",
    "https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough -  Activation Functions\n",
    "\n",
    "Let's see what happens when we pass our convolved edge detected image through various activation functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_arr = edges\n",
    "input_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Sigmoid\n",
    "\n",
    "$f(x) = \\frac{1}{1 + e^{-x}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "plt.imshow(sigmoid(edges))\n",
    "plt.title('edges + sigmoid')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.tanh(input_arr))\n",
    "plt.title('edges + tanh')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU\n",
    "\n",
    "$f(x) = max(0, x)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "plt.imshow(relu(input_arr))\n",
    "plt.title('edges + ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Leaky ReLU\n",
    "\n",
    "$f(x) = \\begin{cases}\n",
    "x & if\\,x > 0 \\\\\n",
    "0.01x & otherwise\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def leaky_relu(x):\n",
    "    return np.where(x > 0, x, x * 0.01)\n",
    "\n",
    "plt.imshow(leaky_relu(input_arr))\n",
    "plt.title('edges + Leaky ReLU')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layer\n",
    "\n",
    "- Summarizes the activations\n",
    " - Take the maximum of a window size: Max pooling\n",
    " - Take the average of a window size: Average pooling\n",
    "\n",
    "https://keras.io/layers/pooling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling Layer\n",
    "\n",
    "- Translation invariances:\n",
    "  - Robust to shifts in locations of pixels within that window\n",
    "- Downsampling:\n",
    "  - Compressing and summarizing inputs into next layers\n",
    "  - Pass the highest activation, or the average activations to the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough - Pooling\n",
    "\n",
    "Let's see what happens when we pass our convolved + activated image through various pooling functions.\n",
    "\n",
    "### Setup\n",
    "Install the skimage library:\n",
    "```\n",
    "conda install scikit-image\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/42463172/how-to-perform-max-mean-pooling-on-a-2d-array-using-numpy\n",
    "from skimage.measure import block_reduce\n",
    "\n",
    "def max_pool(x, pool_size=(2, 2)):\n",
    "    return block_reduce(x, pool_size, np.max)\n",
    "\n",
    "def average_pool(x, pool_size=(2, 2)):\n",
    "    return block_reduce(x, pool_size, np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(max_pool(sigmoid(edges)))\n",
    "plt.title('edges + sigmoid + max pool')\n",
    "plt.show()\n",
    "\n",
    "print('Original shape:', edges.shape)\n",
    "print('After activation:', sigmoid(edges).shape)\n",
    "\n",
    "# (2, 2) will halve the size of the output\n",
    "print('After pooling (2, 2):', max_pool(sigmoid(edges)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(average_pool(sigmoid(edges)))\n",
    "plt.title('edges + sigmoid + average pool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.imshow(max_pool(relu(edges)))\n",
    "plt.title('edges + relu + max pool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(average_pool(relu(edges)))\n",
    "plt.title('edges + relu + average pool')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization Layers\n",
    "\n",
    "These layers are common in Deep Convolutional Neural Networks:\n",
    "\n",
    "- Dropout\n",
    "- Batch Normalization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "- Randomly setting some layer inputs to 0 during training to reduce overfitting.\n",
    "- No-op during prediction\n",
    "\n",
    "https://keras.io/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "![dropout](assets/image/dropout.png)\n",
    "\n",
    "(image: http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization\n",
    "\n",
    "- Avoid saturating non-linearities by normalizing the input to the next layer\n",
    "- Normalizing is done per minibatch\n",
    "- Speeds up training\n",
    "\n",
    "https://keras.io/layers/normalization/#batchnormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Normalization - Training\n",
    "\n",
    "![batch norm](assets/image/batchnorm.png)\n",
    "\n",
    "(image: [Batch Normalization Paper](https://arxiv.org/abs/1502.03167))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Batch Normalization - Prediction\n",
    "\n",
    "- Minibatch mean and variance don't apply at prediction time\n",
    "- Instead, \"population\" mean and variance is computed and stored from training\n",
    "- Batch Norm layer will use the population mean and variance for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Architectures\n",
    "\n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Instance Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "|Material|Read it for|URL\n",
    "|--|--|--|\n",
    "|Deep Learning - Chapter 9.2: Motivation (p 329-335)|3 motivations for convolution|http://www.deeplearningbook.org/contents/convnets.html|\n",
    "|Deep Learning - Chapter 9.3: Motivation (p 335-339)|The idea behind pooling|http://www.deeplearningbook.org/contents/convnets.html|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
