{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Image Recognition\n",
    "\n",
    "(live long, prosper, and watch out for cats)\n",
    "\n",
    "![cat detectors rule](assets/image/meme.jpg)\n",
    "\n",
    "(image: https://towardsdatascience.com/the-whos-who-of-machine-learning-and-why-you-should-know-them-9cefbbc84f07)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Input Representation\n",
    "- Convolutional Neural Networks\n",
    "- Image Classification\n",
    "- Object Detection\n",
    "- Instance Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image Recognition Tasks\n",
    "![cat detectors](assets/image/cat_detectors.png)\n",
    "\n",
    "(image: analyticsindiamag.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Image Representation: Tensor\n",
    "\n",
    "- 3 channels: 'rgb'\n",
    "- rows: image height\n",
    "- columns: image width\n",
    "\n",
    "Ordering:\n",
    "- Channels-first: channel, rows, columns\n",
    "- Channels-last: rows, columns, channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Walkthrough - Image Tensors\n",
    "\n",
    "In this walkthrough, we will read an image from file and examine the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup\n",
    "\n",
    "Install the Python image library:\n",
    "```\n",
    "conda install pillow\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# read an image file\n",
    "demo = Image.open('assets/image/cat.jpg') # source: pxhere.com/en/photo/1337399\n",
    "\n",
    "# check whether this is RGB or BGR\n",
    "# so that we can input the images correctly to our neural network\n",
    "print('channel ordering:', demo.mode)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(demo)\n",
    "plt.title('moar food')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# examine the numpy array\n",
    "demo_arr = np.array(demo)\n",
    "\n",
    "print('shape:', demo_arr.shape)\n",
    "print('data type:', demo_arr.dtype)\n",
    "print('rank:', demo_arr.ndim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# since the sides of the picture are the same boring color\n",
    "# inspect the (roughly) middle 5 rows and columns\n",
    "midpoint_row = int(demo_arr.shape[0] / 2)\n",
    "midpoint_col = int(demo_arr.shape[1] / 2)\n",
    "\n",
    "demo_arr[midpoint_row:midpoint_row+5, midpoint_col:midpoint_col+5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# resize the image to 224 by 224\n",
    "demo.thumbnail((224, 224), resample=Image.BICUBIC)\n",
    "\n",
    "# display the image\n",
    "plt.imshow(demo, interpolation='nearest')\n",
    "plt.title('moar food (224x224)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# examine the numpy array again\n",
    "demo_arr = np.array(demo)\n",
    "\n",
    "print(demo_arr.shape)\n",
    "print(demo_arr.dtype)\n",
    "\n",
    "# notice the difference in values from previously\n",
    "midpoint_row = int(demo_arr.shape[0] / 2)\n",
    "midpoint_col = int(demo_arr.shape[1] / 2)\n",
    "demo_arr[midpoint_row:midpoint_row+5, midpoint_col:midpoint_col+5, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A histogram is sometimes helpful to visualize the colour distribution of a given channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# order: RGB\n",
    "red_channel = demo_arr[:, :, 0]\n",
    "green_channel = demo_arr[:, :, 1]\n",
    "blue_channel = demo_arr[:, :, 2]\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 10))\n",
    "\n",
    "# flatten the [row_size, col_size] matrix into a vector of [row_size * col_size]\n",
    "# we just need to count the raw pixel values for the histogram,\n",
    "# so it doesn't matter where they are located.\n",
    "ax[0].hist(red_channel.flatten(), 256, range=(0,256), color='red')\n",
    "ax[1].hist(green_channel.flatten(), 256, range=(0,256), color='green')\n",
    "ax[2].hist(blue_channel.flatten(), 256, range=(0,256), color='blue')\n",
    "\n",
    "fig.suptitle('Histogram of input image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output\n",
    "\n",
    "- Image Classification: labels\n",
    "- Object Detection: labels + bounding boxes\n",
    "- Instance Segmentation: labels + boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem: many input features $\\rightarrow$ many parameters\n",
    "\n",
    "224 x 224 pixel colour image: 224 x 224 x 3 = 150528 features\n",
    "\n",
    "Dimensionality reduction may help, but there's a better way...."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution\n",
    "\n",
    "- Reduces parameter space\n",
    "- Looks at localized, spatial information\n",
    "\n",
    "![cnn](assets/image/cnn.png)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolution - Hyperparmeters\n",
    "\n",
    "- Kernel size: size of the window (pixels)\n",
    "- Stride: how many pixels to slide the window\n",
    "- Depth: how many filters to use\n",
    "- Padding: whether to keep the output size the same as input size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kernel size = (2, 2), Stride = 1\n",
    "\n",
    "![convolution](assets/image/2d_convolution.png)\n",
    "\n",
    "(image: http://www.deeplearningbook.org/contents/convnets.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Depth = number of filters\n",
    "\n",
    "![depth col](assets/image/depthcol.jpg)\n",
    "\n",
    "(image: https://cs231n.github.io/convolutional-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding = same\n",
    "\n",
    "![same_padding_no_strides](assets/image/same_padding_no_strides.gif)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Padding = valid (none)\n",
    "\n",
    "![no_padding_no_strides](assets/image/no_padding_no_strides.gif)\n",
    "\n",
    "(image: [leonardoaraujosantos.gitbooks.io](https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/convolutional_neural_networks.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activity - Convolution Hyperparameters\n",
    "\n",
    "Fill in the hyperparameter values for the following example.\n",
    "\n",
    "1. Kernel Size = \n",
    "2. Stride = \n",
    "3. Depth = \n",
    "4. Padding ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Source: https://cs231n.github.io/convolutional-networks\n",
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe src=conv-demo.html width=800 height=700></iframe>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough - 2D Convolution\n",
    "\n",
    "In this walkthrough, we will convolve our demo image with a kernel that performs edge detection.\n",
    "\n",
    "Credits: http://machinelearninguru.com/computer_vision/basics/convolution/image_convolution_1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# convert our input image to greyscale (1 channel)\n",
    "demo_grey = demo.convert(mode='L')\n",
    "\n",
    "plt.imshow(demo_grey, interpolation='nearest')\n",
    "plt.title('moar grey (224x224)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.convolve2d.html\n",
    "from scipy.signal import convolve2d\n",
    "\n",
    "# edge-detection kernel\n",
    "kernel = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\n",
    "\n",
    "# we use 'valid' which means we do not add zero padding to our image\n",
    "edges = convolve2d(demo_grey, kernel, mode='valid')\n",
    "\n",
    "plt.imshow(edges)\n",
    "plt.title('moar edges (224x224)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Convolutional Block\n",
    "\n",
    "1. Convolutional Layer\n",
    "2. Activation Layer\n",
    "3. Pooling Layer\n",
    "\n",
    "Note: 1 and 2 may be repeated before 3 is applied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![conv block](assets/image/convnet.jpg)\n",
    "\n",
    "(image: https://cs231n.github.io/convolutional-networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "The output of convolution is typically passed through an \"activation\" function to i\n",
    "\n",
    "Examples:\n",
    "- linear (= no activation)\n",
    "- sigmoid\n",
    "- tanh\n",
    "- Rectified Linear Units, leaky ReLU, Parametric ReLU, Exponential Linear Units\n",
    "\n",
    "https://keras.io/activations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough -  Activations\n",
    "\n",
    "Let's see what happens when we pass our convolved edge detected image through different activations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Pooling\n",
    "\n",
    "https://keras.io/layers/pooling/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization Layers\n",
    "\n",
    "- Dropout\n",
    "- Batch normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://keras.io/layers/core/#dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "https://keras.io/layers/normalization/#batchnormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "|Material|Read it for|URL\n",
    "|--|--|--|\n",
    "|Deep Learning - Chapter 9.2: Motivation (p 329-335)|3 motivations for convolution|http://www.deeplearningbook.org/contents/convnets.html|\n",
    "|Deep Learning - Chapter 9.3: Motivation (p 335-339)|The idea behind pooling|http://www.deeplearningbook.org/contents/convnets.html|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
