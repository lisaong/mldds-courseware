{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "- Overview\n",
    "- Parsing, Stemming, Lemmatization\n",
    "- Named Entity Recognition\n",
    "- Stop Words\n",
    "- Frequency Analysis\n",
    "- Workshop: Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toolkits\n",
    "\n",
    "### NLTK: NLP toolkit\n",
    "\n",
    "Book: http://www.nltk.org/book/\n",
    "\n",
    "Wiki: https://github.com/nltk/nltk/wiki\n",
    "\n",
    "Corpus: http://www.nltk.org/nltk_data/\n",
    "\n",
    "### spaCy: another NLP toolkit\n",
    "\n",
    "Simpler to use than NLTK (but usually fewer knobs)\n",
    "\n",
    "API: https://spacy.io/api/\n",
    "\n",
    "Models: https://spacy.io/usage/models\n",
    "\n",
    "Tutorial: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run this command from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) conda install nltk spacy scikit-learn pandas\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# What is Text Processing?\n",
    "\n",
    "- A sub-field of Natural Language Processing (NLP)\n",
    "- Natural Language Processing is ...\n",
    " - Teaching machines to understand and produce language (text, speech)\n",
    " - A combination of computer science and computational linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text Processing Tasks\n",
    "\n",
    "- Word categorization and tagging: part of speech, type of entity\n",
    "- Semantic Analysis: finding meanings of documents\n",
    "- Topic Modeling: finding topics from documents\n",
    "- Document similarity: comparing if two documents are semantically similar\n",
    "- etc.\n",
    "\n",
    "Note: Speech is text processing + acoustic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing, Stemming & Lemmatization\n",
    "\n",
    "- Tokenization: splitting text into words\n",
    "- Sentence boundary detection: splitting text into sentences\n",
    "- Stemming: finding word stems\n",
    "   - stating => state, reference => refer\n",
    "- Lemmatization: finding the base form of words\n",
    "   - was => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "- Segmenting text into words, punctuation, etc.\n",
    "- Rule-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization with spaCy\n",
    "\n",
    "![tokenization in spaCy](assets/text/tokenization.svg)\n",
    "\n",
    "(image: https://spacy.io/usage/spacy-101#annotations-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download the English model\n",
    "# You can find other models here: https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u\"This is a test. A quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# sentence tokenizer\n",
    "for sent in doc.sents:\n",
    "    print()\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizer\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain('DET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/token\n",
    "\n",
    "https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tokenization with NLTK\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "nltk.tokenize\n",
    " - sent_tokenize\n",
    " - word_tokenize\n",
    " - wordpunc_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# list of sentences\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# flat list of words and punctuations\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# list of lists\n",
    "[word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text2 = \"'The time is now 5.30am,' he said.\"\n",
    "\n",
    "print(word_tokenize(text2))\n",
    "\n",
    "print(wordpunct_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "[nltk.pos_tag(word) for word in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Twitter-aware tokenizer\n",
    "\n",
    "`nltk.tokenize.TweetTokenizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming vs. Lemmatization\n",
    "\n",
    "- Stemming uses rule-based heuristics\n",
    "  - ponies => poni\n",
    "  - Quicker, but less precision\n",
    "- Lemmatization uses vocabulary and morphological analysis\n",
    "  - ponies => pony\n",
    "  - For English, not much improvement over stemming because context of word use is more important\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "- 5 sequential phases of word reductions\n",
    "- Applies rules such as \"sses -> ss\", \"ies => i\"\n",
    "\n",
    "![stemmers](assets/text/stemmers.png)\n",
    "\n",
    "(image: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with spaCy\n",
    "\n",
    "`spacy.lemmatizer.Lemmatizer`\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(lemmatizer(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with NLTK\n",
    "\n",
    "`nltk.stem`\n",
    "- `PorterStemmer`\n",
    "- `WordNetLemmatizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "- Find and classify entities within text\n",
    "  - Persons\n",
    "  - Organizations\n",
    "  - Locations\n",
    "  - Time expressions\n",
    "  - Quantities\n",
    "  - Phone numbers\n",
    "  - etc\n",
    "  \n",
    "- Grammar-based models, trained classifiers\n",
    "\n",
    "- Can be corpus-dependent, see https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with spaCy\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text3 = u\"Flight 224 is scheduled to arrive in Frankfurt at 4pm July 5th, 2018.\"\n",
    "doc = nlp(text3)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, entity.start_char, entity.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "\n",
    "```\n",
    "nltk.ne_chunk()\n",
    "```\n",
    "\n",
    "https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text3)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Input to ne_chunk needs to be a part-of-speech tagged word\n",
    "sentences_pos_tagged = [nltk.pos_tag(word) for word in sentences]\n",
    "\n",
    "[nltk.ne_chunk(word_pos) for word_pos in sentences_pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are high-frequency words that don't contribute much lexical content:\n",
    "\n",
    "- the\n",
    "- a\n",
    "- to\n",
    "\n",
    "NLP libraries usually include a corpus of stop words.\n",
    "\n",
    "Stop word lists:\n",
    "- http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "- https://www.semantikoz.com/blog/free-stop-word-lists-in-23-languages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with spaCy\n",
    "\n",
    "`spacy.lang.en.stop_words`\n",
    "\n",
    "`token.is_stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text3)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS.add('MLDDS')\n",
    "\n",
    "doc = nlp(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with NLTK\n",
    "\n",
    "```\n",
    "nltk.corpus.stopwords\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Download corpus\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text3)\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop words\n",
    "stops = stopwords.words('english')\n",
    "stops.append(\"MLDDS\")\n",
    "stops = set(stops)\n",
    "\n",
    "tokens = nltk.word_tokenize(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frequency Analysis\n",
    "\n",
    "Answers two questions:\n",
    "\n",
    "1. How often does a word appear in a document?\n",
    "\n",
    "2. How important is a word in a document?\n",
    "\n",
    "Measure: Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Term Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$\\frac{f_{t, d}}{\\sum_{t' \\in d} \\, f_{t',d}}$$\n",
    "\n",
    "$f_{t, d}$: count of term $t$ in document $d$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inverse Document Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$log\\frac{N}{\\mid\\{d \\in D : t \\in d \\}\\mid}$$\n",
    "\n",
    "$N$: number of documents\n",
    "\n",
    "$\\mid\\{d \\in D : t \\in d \\}\\mid$: number of documents containing term $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## TD-IDF\n",
    "\n",
    "$$tfidf(t, d, D) = tf(t, d) * idf(t, D)$$\n",
    "\n",
    "|term|tf|idf|tf-idf|\n",
    "|--|--|--|--|--|\n",
    "|to|large|very small|closer to 0|\n",
    "|coffee|small|large|not closer to 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Computing TF-IDF\n",
    "\n",
    "#### Scikit-learn:\n",
    "\n",
    "```\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "```\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "\n",
    "#### NLTK\n",
    "Supports tf-idf, but less popular\n",
    "```\n",
    "nltk.text.TextCollection\n",
    "```\n",
    "\n",
    "http://www.nltk.org/api/nltk.html#nltk.text.TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_wc = pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())\n",
    "df_wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer is a combination of\n",
    "#   CountVectorizer + TfidfTransformer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "print(X_dense.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# for each sentence, get the highest tf-idf\n",
    "import numpy as np\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "tfidf_arr = np.array(X_dense)\n",
    "\n",
    "for i in np.arange(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    sorted_idx = np.argsort(tfidf_arr[i])[::-1]\n",
    "    [print(terms[j], tfidf_arr[i][j]) for j in sorted_idx]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1. Get 3-5 of your own sample sentences\n",
    "2. Compute the TF-IDF\n",
    "3. Compute the TF-IDF with stop_words filtered out:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "```\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOP_WORDS)\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## N-grams\n",
    "\n",
    "TF-IDF can be applied to N-grams (N words at a time), to try to capture some context information.\n",
    "\n",
    "```\n",
    "CountVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "\n",
    "TfidfVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences using 1 and 2-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "print(X_dense.shape)\n",
    "\n",
    "pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: TF-IDF with Trigrams\n",
    "\n",
    "- Compute the TF-IDF for trigrams (1 to 3-grams), using your sample text.\n",
    "- Try with and without stop words included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer is a combination of\n",
    "#   CountVectorizer + TfidfTransformer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "print(X_dense.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "tfidf_arr = np.array(X_dense)\n",
    "\n",
    "for i in np.arange(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    sorted_idx = np.argsort(tfidf_arr[i])[::-1]\n",
    "    [print(terms[j], tfidf_arr[i][j]) for j in sorted_idx]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLTK N-gram support\n",
    "\n",
    "You can also split text into trigrams and bigrams using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'a'), ('a', 'field'), ('field', 'one'), ('one', 'summer'), ('summer', \"'s\"), (\"'s\", 'day'), ('day', 'a'), ('a', 'Grasshopper'), ('Grasshopper', 'was'), ('was', 'hopping'), ('hopping', 'about'), ('about', ','), (',', 'chirping'), ('chirping', 'and'), ('and', 'singing'), ('singing', 'to'), ('to', 'its'), ('its', 'heart'), ('heart', \"'s\"), (\"'s\", 'content'), ('content', '.')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import bigrams, trigrams, ngrams, word_tokenize\n",
    "\n",
    "# http://www.taleswithmorals.com/aesop-fable-the-ant-and-the-grasshopper.htm\n",
    "text6 = \"In a field one summer's day a Grasshopper was hopping about, \" \\\n",
    "        \"chirping and singing to its heart's content.\"\n",
    "\n",
    "words = word_tokenize(text6)\n",
    "\n",
    "print(list(bigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'a', 'field'), ('a', 'field', 'one'), ('field', 'one', 'summer'), ('one', 'summer', \"'s\"), ('summer', \"'s\", 'day'), (\"'s\", 'day', 'a'), ('day', 'a', 'Grasshopper'), ('a', 'Grasshopper', 'was'), ('Grasshopper', 'was', 'hopping'), ('was', 'hopping', 'about'), ('hopping', 'about', ','), ('about', ',', 'chirping'), (',', 'chirping', 'and'), ('chirping', 'and', 'singing'), ('and', 'singing', 'to'), ('singing', 'to', 'its'), ('to', 'its', 'heart'), ('its', 'heart', \"'s\"), ('heart', \"'s\", 'content'), (\"'s\", 'content', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(trigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('In', 'a', 'field', 'one'), ('a', 'field', 'one', 'summer'), ('field', 'one', 'summer', \"'s\"), ('one', 'summer', \"'s\", 'day'), ('summer', \"'s\", 'day', 'a'), (\"'s\", 'day', 'a', 'Grasshopper'), ('day', 'a', 'Grasshopper', 'was'), ('a', 'Grasshopper', 'was', 'hopping'), ('Grasshopper', 'was', 'hopping', 'about'), ('was', 'hopping', 'about', ','), ('hopping', 'about', ',', 'chirping'), ('about', ',', 'chirping', 'and'), (',', 'chirping', 'and', 'singing'), ('chirping', 'and', 'singing', 'to'), ('and', 'singing', 'to', 'its'), ('singing', 'to', 'its', 'heart'), ('to', 'its', 'heart', \"'s\"), ('its', 'heart', \"'s\", 'content'), ('heart', \"'s\", 'content', '.')]\n"
     ]
    }
   ],
   "source": [
    "print(list(ngrams(words, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Document Summarization\n",
    "\n",
    "- assign a score to each word in a document corresponding to its level of \"importance\"\n",
    "- rank each sentence in the document\n",
    "  - by summing the individual word scores and dividing by the number of tokens in the sentence\n",
    "- extract the top N highest scoring sentences and return them as our \"summary\"\n",
    "\n",
    "Credits: \n",
    "- https://github.com/charlieg/A-Smattering-of-NLP-in-Python\n",
    "- http://anthology.aclweb.org/P/P11/P11-3014.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "building term-document matrix... [process started: 2018-06-23 22:34:29.859958]\n",
      "done! [process finished: 2018-06-23 22:35:25.331088]\n"
     ]
    }
   ],
   "source": [
    "import datetime, re, sys\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import reuters\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "import nltk\n",
    "nltk.download('reuters')\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "token_dict = {}\n",
    "for article in reuters.fileids():\n",
    "    token_dict[article] = reuters.raw(article)\n",
    "        \n",
    "tfidf = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english', decode_error='ignore')\n",
    "print('building term-document matrix... [process started: ' + str(datetime.datetime.now()) + ']')\n",
    "\n",
    "tdm = tfidf.fit_transform(token_dict.values()) # this can take some time (about 60 seconds)\n",
    "print('done! [process finished: ' + str(datetime.datetime.now()) + ']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDM contains 26304 terms and 10788 documents\n",
      "first term: 'ali\n",
      "last term: zzzz\n",
      "random term: significnt\n",
      "random term: maud\n",
      "random term: pasta\n",
      "random term: realism\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "feature_names = tfidf.get_feature_names()\n",
    "print ('TDM contains ' + str(len(feature_names)) + ' terms and ' + str(tdm.shape[0]) + ' documents')\n",
    "\n",
    "print('first term: ' + feature_names[0])\n",
    "print('last term: ' + feature_names[len(feature_names) - 1])\n",
    "\n",
    "for i in range(0, 4):\n",
    "    print('random term: ' + feature_names[randint(1,len(feature_names) - 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** SUMMARY ***\n",
      "Comdata said in the merger each share of the company's\n",
      "  stock would be converted at the holders election into either 15\n",
      "  dlrs in cash or a combination of 10 dlrs in cash and a unit of\n",
      "  securities including common stock.\n",
      "Comdata said WCAS and its affiliate investors would commit\n",
      "  50 mln dlrs to buy the securities comprising the new entities\n",
      "  units of securities resulting from the merger in the same\n",
      "  proportions and at the same price as the company shareholders.\n",
      "\n",
      "*** ORIGINAL ***\n",
      "COMDATA &lt;CDN> IN MERGER AGREEMENT\n",
      "  Comdata Network Inc said\n",
      "  it has entered into a letter of intent with a limited\n",
      "  partnership managed by Welsh, Carson, Anderson and Stowe (WCAS)\n",
      "  to merge Comdata into a corproration to be formed by WCAS.\n",
      "      Comdata said in the merger each share of the company's\n",
      "  stock would be converted at the holders election into either 15\n",
      "  dlrs in cash or a combination of 10 dlrs in cash and a unit of\n",
      "  securities including common stock.\n",
      "      Comdata said the terms are subject to the condition that\n",
      "  WCAS' affiliate investors would own a minimum of 60 pct of the\n",
      "  fully diluted stock of the new entity.\n",
      "      Comdata said WCAS and its affiliate investors would commit\n",
      "  50 mln dlrs to buy the securities comprising the new entities\n",
      "  units of securities resulting from the merger in the same\n",
      "  proportions and at the same price as the company shareholders.\n",
      "      Comdata said the move is subject to execution of definitive\n",
      "  agreement and approval by Comdata shareholders as well as\n",
      "  obtaining up to 200 mln dlrs in debt financing.\n",
      "      WCAS told Comdata it believes that it can get commitments\n",
      "  for this financing.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "article_id = random.randint(0, tdm.shape[0] - 1)\n",
    "article_text = reuters.raw(reuters.fileids()[article_id])\n",
    "\n",
    "sent_scores = []\n",
    "for sentence in nltk.sent_tokenize(article_text):\n",
    "    score = 0\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    for token in (t for t in sent_tokens if t in feature_names):\n",
    "        score += tdm[article_id, feature_names.index(token)]\n",
    "    sent_scores.append((score / len(sent_tokens), sentence))\n",
    "\n",
    "summary_length = int(math.ceil(len(sent_scores) / 5))\n",
    "sent_scores.sort(key=lambda sent: sent[0])\n",
    "\n",
    "print('*** SUMMARY ***')\n",
    "for summary_sentence in sent_scores[:summary_length]:\n",
    "    print(summary_sentence[1])\n",
    "\n",
    "print('\\n*** ORIGINAL ***')\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
