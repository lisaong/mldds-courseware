{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "- Overview\n",
    "- Parsing, Stemming, Lemmatization\n",
    "- Named Entity Recognition\n",
    "- Stop Words\n",
    "- Frequency Analysis\n",
    "- Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Toolkits\n",
    "\n",
    "### NLTK: NLP toolkit\n",
    "\n",
    "Book: http://www.nltk.org/book/\n",
    "\n",
    "Wiki: https://github.com/nltk/nltk/wiki\n",
    "\n",
    "Corpus: http://www.nltk.org/nltk_data/\n",
    "\n",
    "### spaCy: another NLP toolkit\n",
    "\n",
    "Simpler to use than NLTK (but usually fewer knobs)\n",
    "\n",
    "API: https://spacy.io/api/\n",
    "\n",
    "Models: https://spacy.io/usage/models\n",
    "\n",
    "Tutorial: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "Run this command from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) conda install nltk spacy scikit-learn pandas\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# What is Text Processing?\n",
    "\n",
    "- A sub-field of Natural Language Processing (NLP)\n",
    "- Natural Language Processing is ...\n",
    " - Teaching machines to understand and produce language (text, speech)\n",
    " - A combination of computer science and computational linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text Processing Tasks\n",
    "\n",
    "- Word categorization and tagging: part of speech, type of entity\n",
    "- Semantic Analysis: finding meanings of documents\n",
    "- Topic Modeling: finding topics from documents\n",
    "- Document similarity: comparing if two documents are semantically similar\n",
    "- etc.\n",
    "\n",
    "Note: Speech is text processing + acoustic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing, Stemming & Lemmatization\n",
    "\n",
    "- Tokenization: splitting text into words\n",
    "- Sentence boundary detection: splitting text into sentences\n",
    "- Stemming: finding word stems\n",
    "   - stating => state, reference => refer\n",
    "- Lemmatization: finding the base form of words\n",
    "   - was => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "- Segmenting text into words, punctuation, etc.\n",
    "- Rule-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization with spaCy\n",
    "\n",
    "![tokenization in spaCy](assets/text/tokenization.svg)\n",
    "\n",
    "(image: https://spacy.io/usage/spacy-101#annotations-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download the English model\n",
    "# You can find other models here: https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u\"This is a test. A quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# sentence tokenizer\n",
    "for sent in doc.sents:\n",
    "    print()\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizer\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain('DET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/token\n",
    "\n",
    "https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tokenization with NLTK\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "nltk.tokenize\n",
    " - sent_tokenize\n",
    " - word_tokenize\n",
    " - wordpunc_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# list of sentences\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# flat list of words and punctuations\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# list of lists\n",
    "[word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text2 = \"'The time is now 5.30am,' he said.\"\n",
    "\n",
    "print(word_tokenize(text2))\n",
    "\n",
    "print(wordpunct_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "[nltk.pos_tag(word) for word in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Twitter-aware tokenizer\n",
    "\n",
    "`nltk.tokenize.TweetTokenizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming vs. Lemmatization\n",
    "\n",
    "- Stemming uses rule-based heuristics\n",
    "  - ponies => poni\n",
    "  - Quicker, but less precision\n",
    "- Lemmatization uses vocabulary and morphological analysis\n",
    "  - ponies => pony\n",
    "  - For English, not much improvement over stemming because context of word use is more important\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "- 5 sequential phases of word reductions\n",
    "- Applies rules such as \"sses -> ss\", \"ies => i\"\n",
    "\n",
    "![stemmers](assets/text/stemmers.png)\n",
    "\n",
    "(image: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with spaCy\n",
    "\n",
    "`spacy.lemmatizer.Lemmatizer`\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(lemmatizer(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with NLTK\n",
    "\n",
    "`nltk.stem`\n",
    "- `PorterStemmer`\n",
    "- `WordNetLemmatizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "- Find and classify entities within text\n",
    "  - Persons\n",
    "  - Organizations\n",
    "  - Locations\n",
    "  - Time expressions\n",
    "  - Quantities\n",
    "  - Phone numbers\n",
    "  - etc\n",
    "  \n",
    "- Grammar-based models, trained classifiers\n",
    "\n",
    "- Can be corpus-dependent, see https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with spaCy\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text3 = u\"Flight 224 is scheduled to arrive in Frankfurt at 4pm July 5th, 2018.\"\n",
    "doc = nlp(text3)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, entity.start_char, entity.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "\n",
    "```\n",
    "nltk.ne_chunk()\n",
    "```\n",
    "\n",
    "https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text3)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Input to ne_chunk needs to be a part-of-speech tagged word\n",
    "sentences_pos_tagged = [nltk.pos_tag(word) for word in sentences]\n",
    "\n",
    "[nltk.ne_chunk(word_pos) for word_pos in sentences_pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are high-frequency words that don't contribute much lexical content:\n",
    "\n",
    "- the\n",
    "- a\n",
    "- to\n",
    "\n",
    "NLP libraries usually include a corpus of stop words.\n",
    "\n",
    "Stop word lists:\n",
    "- http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "- https://www.semantikoz.com/blog/free-stop-word-lists-in-23-languages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with spaCy\n",
    "\n",
    "`spacy.lang.en.stop_words`\n",
    "\n",
    "`token.is_stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text3)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS.add('MLDDS')\n",
    "\n",
    "doc = nlp(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with NLTK\n",
    "\n",
    "```\n",
    "nltk.corpus.stopwords\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Download corpus\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text3)\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop words\n",
    "stops = stopwords.words('english')\n",
    "stops.append(\"MLDDS\")\n",
    "stops = set(stops)\n",
    "\n",
    "tokens = nltk.word_tokenize(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frequency Analysis\n",
    "\n",
    "Answers two questions:\n",
    "\n",
    "1. How often does a word appear in a document?\n",
    "\n",
    "2. How important is a word in a document?\n",
    "\n",
    "Measure: Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Term Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$\\frac{f_{t, d}}{\\sum_{t' \\in d} \\, f_{t',d}}$$\n",
    "\n",
    "$f_{t, d}$: count of term $t$ in document $d$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inverse Document Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$log\\frac{N}{\\mid\\{d \\in D : t \\in d \\}\\mid}$$\n",
    "\n",
    "$N$: number of documents\n",
    "\n",
    "$\\mid\\{d \\in D : t \\in d \\}\\mid$: number of documents containing term $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## TD-IDF\n",
    "\n",
    "$$tfidf(t, d, D) = tf(t, d) * idf(t, D)$$\n",
    "\n",
    "|term|tf|idf|tf-idf|\n",
    "|--|--|--|--|--|\n",
    "|to|large|very small|closer to 0|\n",
    "|coffee|small|large|not closer to 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Computing TF-IDF\n",
    "\n",
    "#### Scikit-learn:\n",
    "\n",
    "```\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "```\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "\n",
    "#### NLTK\n",
    "Supports tf-idf, but less popular\n",
    "```\n",
    "nltk.text.TextCollection\n",
    "```\n",
    "\n",
    "http://www.nltk.org/api/nltk.html#nltk.text.TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_wc = pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())\n",
    "df_wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer is a combination of\n",
    "#   CountVectorizer + TfidfTransformer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "print(X_dense.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# for each sentence, get the highest tf-idf\n",
    "import numpy as np\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "tfidf_arr = np.array(X_dense)\n",
    "\n",
    "for i in np.arange(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    sorted_idx = np.argsort(tfidf_arr[i])[::-1]\n",
    "    [print(terms[j], tfidf_arr[i][j]) for j in sorted_idx]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1. Get 3-5 of your own sample sentences\n",
    "2. Compute the TF-IDF\n",
    "3. Compute the TF-IDF with stop_words filtered out:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "```\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOP_WORDS)\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## N-grams\n",
    "\n",
    "TF-IDF can be applied to N-grams (N words at a time), to try to capture some context information.\n",
    "\n",
    "```\n",
    "CountVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "\n",
    "TfidfVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences using 1 and 2-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "print(X_dense.shape)\n",
    "\n",
    "pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: TF-IDF with Trigrams\n",
    "\n",
    "- Compute the TF-IDF for trigrams (1 to 3-grams), using your sample text.\n",
    "- Try with and without stop words included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### NLTK N-gram support\n",
    "\n",
    "You can also split text into trigrams and bigrams using NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams, ngrams, word_tokenize\n",
    "\n",
    "# http://www.taleswithmorals.com/aesop-fable-the-ant-and-the-grasshopper.htm\n",
    "text6 = \"In a field one summer's day a Grasshopper was hopping about, \" \\\n",
    "        \"chirping and singing to its heart's content.\"\n",
    "\n",
    "words = word_tokenize(text6)\n",
    "\n",
    "print(list(bigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(list(trigrams(words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ngrams(words, 4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# NLP Datasets\n",
    "\n",
    "http://www.nltk.org/nltk_data/\n",
    "\n",
    "https://github.com/niderhoff/nlp-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Paragraph Summarization\n",
    "\n",
    "- Download a corpus from NLTK\n",
    "- Split the corpus into paragraphs\n",
    "- Compute TF-IDF score for each word in a paragraph corresponding to its level of \"importance\"\n",
    "- Rank each sentence using (sum of TF-IDF(words) / number of tokens)\n",
    "- Extract the top N highest scoring sentences and return them as our \"summary\"\n",
    "\n",
    "Credits: https://github.com/charlieg/A-Smattering-of-NLP-in-Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a corpus\n",
    "\n",
    "Select a corpus from http://www.nltk.org/nltk_data/. \n",
    "\n",
    "Suggestions:\n",
    "- reuters\n",
    "- abc\n",
    "- gutenberg\n",
    "\n",
    "Example\n",
    "```\n",
    "# download the corpus you selected\n",
    "import nltk\n",
    "nltk.download('abc')\n",
    "\n",
    "# update the import with the corpus you selected\n",
    "from nltk.corpus import abc as corpus\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore the corpus\n",
    "\n",
    "For example, try printing the raw text of one of the files:\n",
    "\n",
    "```\n",
    "fileids = corpus.fileids()\n",
    "\n",
    "print(fileids)\n",
    "\n",
    "print(corpus.raw(fileids[0])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Split the text into paragraphs\n",
    "\n",
    "NLTK doesn't include a paragraph tokenizer, so we'll try to create our own.\n",
    "\n",
    "One logic that may work is this:\n",
    "- a paragraph is detected if there are consecutive newline characters\n",
    "\n",
    "Adapt this function to your corpus, and adjust the logic if necessary to get paragraphs.\n",
    "\n",
    "```\n",
    "def tokenize_paragraph(text):\n",
    "    \"\"\"Tokenizes text into paragraphs\n",
    "    Args:\n",
    "        text - the raw text\n",
    "    Returns:\n",
    "        A list of paragraphs for the raw text\n",
    "    \"\"\"\n",
    "    # Note: you may need to customize this logic for the \n",
    "    # corpus you selected\n",
    "    return [p for p in text.split('\\n\\n') if p]\n",
    "\n",
    "# test code\n",
    "paragraphs = tokenize_paragraph(corpus.raw(fileids[0]))\n",
    "print(paragraphs[:5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Collect all paragraphs in your corpus\n",
    "\n",
    "Using the paragraph tokenizer, create a list containing all paragraphs for all the files in the corpus.\n",
    "\n",
    "We will be using this to train TF-IDF.\n",
    "\n",
    "Some starter code:\n",
    "\n",
    "```\n",
    "all_paragraphs = []\n",
    "for fileid in corpus.fileids():\n",
    "    text = corpus.raw(fileid)\n",
    "\n",
    "    # Split text into paragraphs and add to all_paragraphs\n",
    "    # You can use the syntax list1 = list1 + list2\n",
    "    #\n",
    "    # Your code here\n",
    "    ...\n",
    "    \n",
    "\n",
    "# test code\n",
    "print(len(all_paragraphs))\n",
    "print(all_paragraphs[:5])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    \"\"\"Helper function to tokenize and stem words in text\n",
    "    Arg:\n",
    "        text: the input text\n",
    "    Return:\n",
    "        the tokenized stem words\n",
    "    \"\"\"\n",
    "    import re\n",
    "    from nltk.stem import PorterStemmer\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compute TF-IDF\n",
    "\n",
    "- Treating a paragraph as a document, compute the TF-IDF using TfidfVectorizer\n",
    "- Pass `tokenize_and_stem` as a tokenizer to TfidfVectorizer\n",
    "- Filter out stop words in TfidfVectorizer\n",
    "- `fit_transform` the TfidfVectorizer (this may take about a minute or two)\n",
    "\n",
    "```\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize_and_stem, stop_words='english')\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Explore the TF-IDF matrix\n",
    "\n",
    "Explore the TF-IDF matrix, counting the terms, documents, and printing the first few terms\n",
    "\n",
    "```\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "\n",
    "# number of terms\n",
    "print(\"Number of terms:\", len(feature_names))\n",
    "\n",
    "# number of documents (paragraphs)\n",
    "print(\"Number of paragraphs:\", tfidf.shape[0])\n",
    "\n",
    "# first 20 terms\n",
    "print(feature_names[:20])\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Paragraph Summarization\n",
    "\n",
    "- Pick a random paragraph\n",
    "- Tokenize the paragraph into sentences\n",
    "- Rank each sentence by getting the average word score for it\n",
    "- Extract the top N highest scoring sentences and return them as our \"summary\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Get a random index for all_paragraphs\n",
    "paragraph_index = random.randint(0, len(all_paragraphs)-1)\n",
    "paragraph = all_paragraphs[paragraph_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the selected paragraph into sentences\n",
    "# for each sentence, compute the sum of TF-IDF divided by tokens\n",
    "sentence_scores = []\n",
    "for sentence in sent_tokenize(paragraph):\n",
    "    tfidf_sum = 0\n",
    "\n",
    "    sent_tokens = tokenize_and_stem(sentence)\n",
    "    feature_tokens = [t for t in sent_tokens if t in feature_names]\n",
    "\n",
    "    for ft in feature_tokens:\n",
    "        tfidf_sum += tfidf[paragraph_index, feature_names.index(ft)]\n",
    "    \n",
    "    sentence_score = tfidf_sum / len(sent_tokens)\n",
    "    sentence_scores.append((sentence_score, sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top-N scores and create the summary\n",
    "\n",
    "# in case paragraph has less than 2 sentences\n",
    "n = min(2, len(sentence_scores))\n",
    "\n",
    "# sort by sentence_score\n",
    "sentence_scores.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "print('*** SUMMARY ***')\n",
    "for summary_sentence in sentence_scores[:n]:\n",
    "    print(summary_sentence[1], '(score: %.2f)' % summary_sentence[0])\n",
    "\n",
    "print('\\n*** ORIGINAL ***')\n",
    "print(paragraph)\n",
    "\n",
    "print('\\n*** SENTENCE SCORES ***')\n",
    "for (score, sentence) in sentence_scores:\n",
    "    print(score, sentence)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
