{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "- Overview\n",
    "- Parsing, Stemming, Lemmatization\n",
    "- Named Entity Recognition\n",
    "- Stop Words\n",
    "- Frequency Analysis\n",
    "- Workshop: Document Summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "NLTK book: http://www.nltk.org/book/\n",
    "\n",
    "spaCy 101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run this command from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) conda install nltk spacy scikit-learn pandas\n",
    "```\n",
    "\n",
    "### NLTK: NLP toolkit\n",
    "https://www.nltk.org/install.html\n",
    "\n",
    "### spaCy: another NLP toolkit\n",
    "\n",
    "Simpler to use than NLTK (but usually fewer knobs)\n",
    "\n",
    "https://spacy.io/api/\n",
    "\n",
    "https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# What is Text Processing?\n",
    "\n",
    "- A sub-field of Natural Language Processing (NLP)\n",
    "- Natural Language Processing is ...\n",
    " - Teaching machines to understand and produce language (text, speech)\n",
    " - A combination of computer science and computational linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text Processing Tasks\n",
    "\n",
    "- Word categorization and tagging: part of speech, type of entity\n",
    "- Semantic Analysis: finding meanings of documents\n",
    "- Topic Modeling: finding topics from documents\n",
    "- Document similarity: comparing if two documents are semantically similar\n",
    "- etc.\n",
    "\n",
    "Note: Speech is text processing + acoustic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing, Stemming & Lemmatization\n",
    "\n",
    "- Tokenization: splitting text into words\n",
    "- Sentence boundary detection: splitting text into sentences\n",
    "- Stemming: finding word stems\n",
    "   - stating => state, reference => refer\n",
    "- Lemmatization: finding the base form of words\n",
    "   - was => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "- Segmenting text into words, punctuation, etc.\n",
    "- Rule-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization with spaCy\n",
    "\n",
    "![tokenization in spaCy](assets/text/tokenization.svg)\n",
    "\n",
    "(image: https://spacy.io/usage/spacy-101#annotations-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Download the English model\n",
    "# You can find other models here: https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = u\"This is a test. A quick brown fox jumps over the lazy dog.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "# sentence tokenizer\n",
    "for sent in doc.sents:\n",
    "    print()\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word tokenizer\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "spacy.explain('DET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/token\n",
    "\n",
    "https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tokenization with NLTK\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "nltk.tokenize\n",
    " - sent_tokenize\n",
    " - word_tokenize\n",
    " - wordpunc_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# list of sentences\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# flat list of words and punctuations\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# list of lists\n",
    "[word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text2 = \"'The time is now 5.30am,' he said.\"\n",
    "\n",
    "print(word_tokenize(text2))\n",
    "\n",
    "print(wordpunct_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "[nltk.pos_tag(word) for word in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('JJ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Twitter-aware tokenizer\n",
    "\n",
    "`nltk.tokenize.TweetTokenizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming vs. Lemmatization\n",
    "\n",
    "- Stemming uses rule-based heuristics\n",
    "  - ponies => poni\n",
    "  - Quicker, but less precision\n",
    "- Lemmatization uses vocabulary and morphological analysis\n",
    "  - ponies => pony\n",
    "  - For English, not much improvement over stemming because context of word use is more important\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "- 5 sequential phases of word reductions\n",
    "- Applies rules such as \"sses -> ss\", \"ies => i\"\n",
    "\n",
    "![stemmers](assets/text/stemmers.png)\n",
    "\n",
    "(image: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with spaCy\n",
    "\n",
    "`spacy.lemmatizer.Lemmatizer`\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(lemmatizer(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with NLTK\n",
    "\n",
    "`nltk.stem`\n",
    "- `PorterStemmer`\n",
    "- `WordNetLemmatizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "- Find and classify entities within text\n",
    "  - Persons\n",
    "  - Organizations\n",
    "  - Locations\n",
    "  - Time expressions\n",
    "  - Quantities\n",
    "  - Phone numbers\n",
    "  - etc\n",
    "  \n",
    "- Grammar-based models, trained classifiers\n",
    "\n",
    "- Can be corpus-dependent, see https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with spaCy\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "text3 = u\"Flight 224 is scheduled to arrive in Frankfurt at 4pm July 5th, 2018.\"\n",
    "doc = nlp(text3)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, entity.start_char, entity.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "\n",
    "```\n",
    "nltk.ne_chunk()\n",
    "```\n",
    "\n",
    "https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text3)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Input to ne_chunk needs to be a part-of-speech tagged word\n",
    "sentences_pos_tagged = [nltk.pos_tag(word) for word in sentences]\n",
    "\n",
    "[nltk.ne_chunk(word_pos) for word_pos in sentences_pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are high-frequency words that don't contribute much lexical content:\n",
    "\n",
    "- the\n",
    "- a\n",
    "- to\n",
    "\n",
    "NLP libraries usually include a corpus of stop words.\n",
    "\n",
    "Stop word lists:\n",
    "- http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "- https://www.semantikoz.com/blog/free-stop-word-lists-in-23-languages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with spaCy\n",
    "\n",
    "`spacy.lang.en.stop_words`\n",
    "\n",
    "`token.is_stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deutsch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text3)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS.add('MLDDS')\n",
    "\n",
    "doc = nlp(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with NLTK\n",
    "\n",
    "```\n",
    "nltk.corpus.stopwords\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Download corpus\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text3)\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding stop words\n",
    "stops = stopwords.words('english')\n",
    "stops.append(\"MLDDS\")\n",
    "stops = set(stops)\n",
    "\n",
    "tokens = nltk.word_tokenize(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frequency Analysis\n",
    "\n",
    "Answers two questions:\n",
    "\n",
    "1. How often does a word appear in a document?\n",
    "\n",
    "2. How important is a word in a document?\n",
    "\n",
    "Measure: Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Term Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$\\frac{f_{t, d}}{\\sum_{t' \\in d} \\, f_{t',d}}$$\n",
    "\n",
    "$f_{t, d}$: count of term $t$ in document $d$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inverse Document Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$log\\frac{N}{\\mid\\{d \\in D : t \\in d \\}\\mid}$$\n",
    "\n",
    "$N$: number of documents\n",
    "\n",
    "$\\mid\\{d \\in D : t \\in d \\}\\mid$: number of documents containing term $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## TD-IDF\n",
    "\n",
    "$$tfidf(t, d, D) = tf(t, d) * idf(t, D)$$\n",
    "\n",
    "|term|tf|idf|tf-idf|\n",
    "|--|--|--|--|--|\n",
    "|to|large|very small|closer to 0|\n",
    "|coffee|small|large|not closer to 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Computing TF-IDF\n",
    "\n",
    "#### Scikit-learn:\n",
    "\n",
    "```\n",
    "sklearn.feature_extraction.text.CountVectorizer\n",
    "\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "```\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "\n",
    "#### NLTK\n",
    "Supports tf-idf, but less popular\n",
    "```\n",
    "nltk.text.TextCollection\n",
    "```\n",
    "\n",
    "http://www.nltk.org/api/nltk.html#nltk.text.TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing Word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display as a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "df_wc = pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())\n",
    "df_wc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Computing TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/modules/feature_extraction.html\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer is a combination of\n",
    "#   CountVectorizer + TfidfTransformer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "print(X_dense.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "X_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# for each sentence, get the highest tf-idf\n",
    "import numpy as np\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "tfidf_arr = np.array(X_dense)\n",
    "\n",
    "for i in np.arange(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    sorted_idx = np.argsort(tfidf_arr[i])[::-1]\n",
    "    [print(terms[j], tfidf_arr[i][j]) for j in sorted_idx]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise\n",
    "\n",
    "1. Get 3-5 of your own sample sentences\n",
    "2. Compute the TF-IDF\n",
    "3. Compute the TF-IDF with stop_words filtered out:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "```\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=STOP_WORDS)\n",
    "\n",
    "...\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## N-grams\n",
    "\n",
    "TF-IDF can be applied to N-grams (N words at a time), to try to capture some context information.\n",
    "\n",
    "```\n",
    "CountVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "\n",
    "TfidfVectorizer(ngram_range=(minN, maxN)), ..)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 30)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bird</th>\n",
       "      <th>bird gets</th>\n",
       "      <th>brown</th>\n",
       "      <th>brown fox</th>\n",
       "      <th>dog</th>\n",
       "      <th>early</th>\n",
       "      <th>early bird</th>\n",
       "      <th>fox</th>\n",
       "      <th>fox jumps</th>\n",
       "      <th>gets</th>\n",
       "      <th>...</th>\n",
       "      <th>quick brown</th>\n",
       "      <th>test</th>\n",
       "      <th>the</th>\n",
       "      <th>the early</th>\n",
       "      <th>the lazy</th>\n",
       "      <th>the quick</th>\n",
       "      <th>the worm</th>\n",
       "      <th>this</th>\n",
       "      <th>this is</th>\n",
       "      <th>worm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   bird  bird gets  brown  brown fox  dog  early  early bird  fox  fox jumps  \\\n",
       "0     0          0      0          0    0      0           0    0          0   \n",
       "1     0          0      1          1    1      0           0    1          1   \n",
       "2     1          1      0          0    0      1           1    0          0   \n",
       "\n",
       "   gets  ...   quick brown  test  the  the early  the lazy  the quick  \\\n",
       "0     0  ...             0     1    0          0         0          0   \n",
       "1     0  ...             1     0    2          0         1          1   \n",
       "2     1  ...             0     0    2          1         0          0   \n",
       "\n",
       "   the worm  this  this is  worm  \n",
       "0         0     1        1     0  \n",
       "1         0     0        0     0  \n",
       "2         1     0        0     1  \n",
       "\n",
       "[3 rows x 30 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "# Count word occurrences using 1 and 2-grams\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "print(X_dense.shape)\n",
    "\n",
    "pd.DataFrame(X_dense, columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: TF-IDF with Trigrams\n",
    "\n",
    "- Compute the TF-IDF for trigrams (1 to 3-grams), using your sample text.\n",
    "- Try with and without stop words included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 42)\n",
      "['bird', 'bird gets', 'bird gets the', 'brown', 'brown fox', 'brown fox jumps', 'dog', 'early', 'early bird', 'early bird gets', 'fox', 'fox jumps', 'fox jumps over', 'gets', 'gets the', 'gets the worm', 'is', 'is test', 'jumps', 'jumps over', 'jumps over the', 'lazy', 'lazy dog', 'over', 'over the', 'over the lazy', 'quick', 'quick brown', 'quick brown fox', 'test', 'the', 'the early', 'the early bird', 'the lazy', 'the lazy dog', 'the quick', 'the quick brown', 'the worm', 'this', 'this is', 'this is test', 'worm']\n",
      "This is a test.\n",
      "\n",
      "is test 0.4082482904638631\n",
      "this is 0.4082482904638631\n",
      "this 0.4082482904638631\n",
      "this is test 0.4082482904638631\n",
      "is 0.4082482904638631\n",
      "test 0.4082482904638631\n",
      "early bird gets 0.0\n",
      "gets the worm 0.0\n",
      "gets the 0.0\n",
      "gets 0.0\n",
      "fox jumps over 0.0\n",
      "fox jumps 0.0\n",
      "fox 0.0\n",
      "worm 0.0\n",
      "jumps 0.0\n",
      "early 0.0\n",
      "dog 0.0\n",
      "brown fox jumps 0.0\n",
      "brown fox 0.0\n",
      "brown 0.0\n",
      "bird gets the 0.0\n",
      "bird gets 0.0\n",
      "early bird 0.0\n",
      "jumps over the 0.0\n",
      "jumps over 0.0\n",
      "lazy 0.0\n",
      "the worm 0.0\n",
      "the quick brown 0.0\n",
      "the quick 0.0\n",
      "the lazy dog 0.0\n",
      "the lazy 0.0\n",
      "the early bird 0.0\n",
      "the early 0.0\n",
      "the 0.0\n",
      "quick brown fox 0.0\n",
      "quick brown 0.0\n",
      "quick 0.0\n",
      "over the lazy 0.0\n",
      "over the 0.0\n",
      "over 0.0\n",
      "lazy dog 0.0\n",
      "bird 0.0\n",
      "\n",
      "The quick brown fox jumps over the lazy dog.\n",
      "\n",
      "the 0.30847453550092424\n",
      "jumps over the 0.20280347192512718\n",
      "quick brown fox 0.20280347192512718\n",
      "fox 0.20280347192512718\n",
      "jumps 0.20280347192512718\n",
      "jumps over 0.20280347192512718\n",
      "lazy 0.20280347192512718\n",
      "lazy dog 0.20280347192512718\n",
      "over 0.20280347192512718\n",
      "over the 0.20280347192512718\n",
      "over the lazy 0.20280347192512718\n",
      "quick 0.20280347192512718\n",
      "fox jumps over 0.20280347192512718\n",
      "quick brown 0.20280347192512718\n",
      "dog 0.20280347192512718\n",
      "brown fox jumps 0.20280347192512718\n",
      "brown fox 0.20280347192512718\n",
      "the lazy 0.20280347192512718\n",
      "the lazy dog 0.20280347192512718\n",
      "the quick 0.20280347192512718\n",
      "the quick brown 0.20280347192512718\n",
      "brown 0.20280347192512718\n",
      "fox jumps 0.20280347192512718\n",
      "early 0.0\n",
      "bird gets the 0.0\n",
      "early bird 0.0\n",
      "early bird gets 0.0\n",
      "bird gets 0.0\n",
      "worm 0.0\n",
      "gets 0.0\n",
      "gets the 0.0\n",
      "gets the worm 0.0\n",
      "is 0.0\n",
      "is test 0.0\n",
      "this is test 0.0\n",
      "test 0.0\n",
      "the early 0.0\n",
      "the early bird 0.0\n",
      "the worm 0.0\n",
      "this 0.0\n",
      "this is 0.0\n",
      "bird 0.0\n",
      "\n",
      "The early bird gets the worm.\n",
      "\n",
      "the 0.3886917427512738\n",
      "worm 0.2555414657180043\n",
      "bird gets 0.2555414657180043\n",
      "bird gets the 0.2555414657180043\n",
      "early 0.2555414657180043\n",
      "early bird 0.2555414657180043\n",
      "early bird gets 0.2555414657180043\n",
      "gets 0.2555414657180043\n",
      "gets the 0.2555414657180043\n",
      "gets the worm 0.2555414657180043\n",
      "bird 0.2555414657180043\n",
      "the early 0.2555414657180043\n",
      "the worm 0.2555414657180043\n",
      "the early bird 0.2555414657180043\n",
      "quick brown 0.0\n",
      "quick 0.0\n",
      "this is 0.0\n",
      "this 0.0\n",
      "brown 0.0\n",
      "brown fox 0.0\n",
      "brown fox jumps 0.0\n",
      "dog 0.0\n",
      "quick brown fox 0.0\n",
      "the quick brown 0.0\n",
      "the quick 0.0\n",
      "fox 0.0\n",
      "fox jumps 0.0\n",
      "fox jumps over 0.0\n",
      "the lazy dog 0.0\n",
      "the lazy 0.0\n",
      "test 0.0\n",
      "is 0.0\n",
      "is test 0.0\n",
      "jumps 0.0\n",
      "jumps over 0.0\n",
      "this is test 0.0\n",
      "lazy 0.0\n",
      "lazy dog 0.0\n",
      "over 0.0\n",
      "over the 0.0\n",
      "over the lazy 0.0\n",
      "jumps over the 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# TfidfVectorizer is a combination of\n",
    "#   CountVectorizer + TfidfTransformer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(sentences)\n",
    "\n",
    "# convert sparse matrix to dense matrix\n",
    "X_dense = X.todense()\n",
    "\n",
    "print(X_dense.shape)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "terms = vectorizer.get_feature_names()\n",
    "tfidf_arr = np.array(X_dense)\n",
    "\n",
    "for i in np.arange(len(sentences)):\n",
    "    print(sentences[i])\n",
    "    sorted_idx = np.argsort(tfidf_arr[i])[::-1]\n",
    "    [print(terms[j], tfidf_arr[i][j]) for j in sorted_idx]\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workshop: Document Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
