{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "- Parsing, Stemming, Lemmatization\n",
    "- Named Entity Recognition\n",
    "- Frequency Analysis\n",
    "- Stop Words\n",
    "- Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "Run this command from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) conda install gensim cython nltk spacy\n",
    "```\n",
    "\n",
    "### gensim: for training word2vec\n",
    "\n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "\n",
    "### Cython: to speed up training word2vec\n",
    "http://docs.cython.org/en/latest/src/quickstart/install.html\n",
    "\n",
    "\n",
    "### NLTK: NLP toolkit\n",
    "Installation: https://www.nltk.org/install.html\n",
    "\n",
    "Book: http://www.nltk.org/book\n",
    "\n",
    "### spaCy: another NLP toolkit\n",
    "\n",
    "Simpler to use than NLTK (but usually fewer knobs)\n",
    "\n",
    "https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing, Stemming Lemmatization\n",
    "\n",
    "- Tokenization: splitting text into words\n",
    "- Sentence boundary detection: splitting text into sentences\n",
    "- Stemming: finding word stems\n",
    "   - stating => state, reference => refer\n",
    "- Lemmatization: finding the base form of words\n",
    "   - was => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "- Segmenting text into words, punctuation, etc.\n",
    "- Rule-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization with spaCy\n",
    "\n",
    "![tokenization in spaCy](assets/text/tokenization.svg)\n",
    "\n",
    "(image: https://spacy.io/usage/spacy-101#annotations-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 14.2MB/s ta 0:00:01    50% |████████████████▎               | 19.0MB 12.2MB/s eta 0:00:02    62% |███████████████████▉            | 23.2MB 12.4MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied (use --upgrade to upgrade): en-core-web-sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz in /home/lisaong/miniconda3/envs/mldds03/lib/python3.6/site-packages\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/lisaong/miniconda3/envs/mldds03/lib/python3.6/site-packages/en_core_web_sm\n",
      "    -->\n",
      "    /home/lisaong/miniconda3/envs/mldds03/lib/python3.6/site-packages/spacy/data/en_core_web_sm\n",
      "\n",
      "    You can now load the model via spacy.load('en_core_web_sm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the English model\n",
    "# You can find other models here: https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DET det\n",
      "Bukit PROPN compound\n",
      "Panjang PROPN compound\n",
      "Light PROPN compound\n",
      "Rail PROPN compound\n",
      "Transit PROPN nsubj\n",
      "( PUNCT punct\n",
      "BPLRT PROPN appos\n",
      ") PUNCT punct\n",
      "will VERB aux\n",
      "resume VERB ROOT\n",
      "operations NOUN dobj\n",
      "on ADP prep\n",
      "Sundays NOUN pobj\n",
      "starting VERB advcl\n",
      "from ADP prep\n",
      "July PROPN pobj\n",
      ", PUNCT punct\n",
      "although ADP mark\n",
      "operating VERB compound\n",
      "hours NOUN nsubjpass\n",
      "will VERB aux\n",
      "be VERB auxpass\n",
      "shortened VERB advcl\n",
      ", PUNCT punct\n",
      "announced VERB conj\n",
      "SMRT PROPN dobj\n",
      "on ADP prep\n",
      "Thursday PROPN pobj\n",
      "( PUNCT punct\n",
      "Jun PROPN appos\n",
      "21 NUM nummod\n",
      ") PUNCT punct\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"The Bukit Panjang Light Rail Transit (BPLRT) will resume operations on Sundays starting from July, although operating hours will be shortened, announced SMRT on Thursday (Jun 21).\")\n",
    "\n",
    "for token in doc:\n",
    "    # text, part-of-speech, syntactic dependencies\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'determiner'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('DET')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"3830\" height=\"557.0\" style=\"max-width: none; height: 557.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">The</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">Bukit</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">Panjang</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">Light</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">Rail</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">Transit (</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">BPLRT)</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1030\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1030\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">resume</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">operations</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1590\">Sundays</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1590\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">starting</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1870\">from</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1870\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2010\">July,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2010\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">although</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2290\">operating</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2290\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2430\">hours</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2430\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2570\">will</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2570\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2710\">be</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2710\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">shortened,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2990\">announced</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2990\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3130\">SMRT</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3130\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3270\">on</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3270\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3410\">Thursday (</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3410\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">Jun</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"467.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3690\">21).</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3690\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,422.0 C70,72.0 745.0,72.0 745.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,424.0 L62,412.0 78,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M210,422.0 C210,142.0 740.0,142.0 740.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M210,424.0 L202,412.0 218,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M350,422.0 C350,352.0 445.0,352.0 445.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350,424.0 L342,412.0 358,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M490,422.0 C490,282.0 730.0,282.0 730.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M490,424.0 L482,412.0 498,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M630,422.0 C630,352.0 725.0,352.0 725.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M630,424.0 L622,412.0 638,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M770,422.0 C770,212.0 1155.0,212.0 1155.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M770,424.0 L762,412.0 778,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M770,422.0 C770,352.0 865.0,352.0 865.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M865.0,424.0 L873.0,412.0 857.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1050,422.0 C1050,352.0 1145.0,352.0 1145.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1050,424.0 L1042,412.0 1058,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1190,422.0 C1190,352.0 1285.0,352.0 1285.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1285.0,424.0 L1293.0,412.0 1277.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M1190,422.0 C1190,282.0 1430.0,282.0 1430.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1430.0,424.0 L1438.0,412.0 1422.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M1470,422.0 C1470,352.0 1565.0,352.0 1565.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1565.0,424.0 L1573.0,412.0 1557.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-11\" stroke-width=\"2px\" d=\"M1190,422.0 C1190,142.0 1720.0,142.0 1720.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-11\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1720.0,424.0 L1728.0,412.0 1712.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-12\" stroke-width=\"2px\" d=\"M1750,422.0 C1750,352.0 1845.0,352.0 1845.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-12\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1845.0,424.0 L1853.0,412.0 1837.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-13\" stroke-width=\"2px\" d=\"M1890,422.0 C1890,352.0 1985.0,352.0 1985.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-13\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1985.0,424.0 L1993.0,412.0 1977.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-14\" stroke-width=\"2px\" d=\"M2170,422.0 C2170,72.0 2845.0,72.0 2845.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-14\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,424.0 L2162,412.0 2178,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-15\" stroke-width=\"2px\" d=\"M2310,422.0 C2310,352.0 2405.0,352.0 2405.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-15\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2310,424.0 L2302,412.0 2318,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-16\" stroke-width=\"2px\" d=\"M2450,422.0 C2450,212.0 2835.0,212.0 2835.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-16\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubjpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2450,424.0 L2442,412.0 2458,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-17\" stroke-width=\"2px\" d=\"M2590,422.0 C2590,282.0 2830.0,282.0 2830.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-17\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2590,424.0 L2582,412.0 2598,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-18\" stroke-width=\"2px\" d=\"M2730,422.0 C2730,352.0 2825.0,352.0 2825.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-18\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">auxpass</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2730,424.0 L2722,412.0 2738,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-19\" stroke-width=\"2px\" d=\"M1190,422.0 C1190,2.0 2850.0,2.0 2850.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-19\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2850.0,424.0 L2858.0,412.0 2842.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-20\" stroke-width=\"2px\" d=\"M2870,422.0 C2870,352.0 2965.0,352.0 2965.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-20\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2965.0,424.0 L2973.0,412.0 2957.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-21\" stroke-width=\"2px\" d=\"M3010,422.0 C3010,352.0 3105.0,352.0 3105.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-21\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3105.0,424.0 L3113.0,412.0 3097.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-22\" stroke-width=\"2px\" d=\"M3150,422.0 C3150,352.0 3245.0,352.0 3245.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-22\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3245.0,424.0 L3253.0,412.0 3237.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-23\" stroke-width=\"2px\" d=\"M3290,422.0 C3290,352.0 3385.0,352.0 3385.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-23\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3385.0,424.0 L3393.0,412.0 3377.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-24\" stroke-width=\"2px\" d=\"M3150,422.0 C3150,212.0 3535.0,212.0 3535.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-24\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3535.0,424.0 L3543.0,412.0 3527.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-25\" stroke-width=\"2px\" d=\"M3570,422.0 C3570,352.0 3665.0,352.0 3665.0,422.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-25\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3665.0,424.0 L3673.0,412.0 3657.0,412.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"The Bukit Panjang Light Rail Transit (BPLRT) will resume operations on Sundays starting from July, although operating hours will be shortened, announced SMRT on Thursday (Jun 21).\")\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tokenization with NLTK\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "nltk.tokenize\n",
    " - sent_tokenize\n",
    " - word_tokenize\n",
    " - wordpunc_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/lisaong/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMRT advised commuters to plan their journeys ahead while operating hours are shortened.',\n",
       " 'It will deploy staff to assist commuters during the affected Sundays, it said.']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = 'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "\n",
    "# list of words sentences\n",
    "sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SMRT',\n",
       " 'advised',\n",
       " 'commuters',\n",
       " 'to',\n",
       " 'plan',\n",
       " 'their',\n",
       " 'journeys',\n",
       " 'ahead',\n",
       " 'while',\n",
       " 'operating',\n",
       " 'hours',\n",
       " 'are',\n",
       " 'shortened',\n",
       " '.',\n",
       " 'It',\n",
       " 'will',\n",
       " 'deploy',\n",
       " 'staff',\n",
       " 'to',\n",
       " 'assist',\n",
       " 'commuters',\n",
       " 'during',\n",
       " 'the',\n",
       " 'affected',\n",
       " 'Sundays',\n",
       " ',',\n",
       " 'it',\n",
       " 'said',\n",
       " '.']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "\n",
    "# flat list of words\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['SMRT',\n",
       "  'advised',\n",
       "  'commuters',\n",
       "  'to',\n",
       "  'plan',\n",
       "  'their',\n",
       "  'journeys',\n",
       "  'ahead',\n",
       "  'while',\n",
       "  'operating',\n",
       "  'hours',\n",
       "  'are',\n",
       "  'shortened',\n",
       "  '.'],\n",
       " ['It',\n",
       "  'will',\n",
       "  'deploy',\n",
       "  'staff',\n",
       "  'to',\n",
       "  'assist',\n",
       "  'commuters',\n",
       "  'during',\n",
       "  'the',\n",
       "  'affected',\n",
       "  'Sundays',\n",
       "  ',',\n",
       "  'it',\n",
       "  'said',\n",
       "  '.']]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = 'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# list of lists\n",
    "[word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'The\", 'time', 'is', 'now', '5.30am', ',', \"'\", 'he', 'said', '.']\n",
      "[\"'\", 'The', 'time', 'is', 'now', '5', '.', '30am', \",'\", 'he', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"'The time is now 5.30am,' he said.\"\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/lisaong/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('SMRT', 'NNP'),\n",
       "  ('advised', 'VBD'),\n",
       "  ('commuters', 'NNS'),\n",
       "  ('to', 'TO'),\n",
       "  ('plan', 'VB'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('journeys', 'NNS'),\n",
       "  ('ahead', 'RB'),\n",
       "  ('while', 'IN'),\n",
       "  ('operating', 'NN'),\n",
       "  ('hours', 'NNS'),\n",
       "  ('are', 'VBP'),\n",
       "  ('shortened', 'VBN'),\n",
       "  ('.', '.')],\n",
       " [('It', 'PRP'),\n",
       "  ('will', 'MD'),\n",
       "  ('deploy', 'VB'),\n",
       "  ('staff', 'NN'),\n",
       "  ('to', 'TO'),\n",
       "  ('assist', 'VB'),\n",
       "  ('commuters', 'NNS'),\n",
       "  ('during', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('affected', 'JJ'),\n",
       "  ('Sundays', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('it', 'PRP'),\n",
       "  ('said', 'VBD'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = 'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "[nltk.pos_tag(word) for word in sentences]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Twitter-aware tokenizer\n",
    "\n",
    "`nltk.tokenize.TweetTokenizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "text = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "text = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "tknzr.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming vs. Lemmatization\n",
    "\n",
    "- Stemming uses rule-based heuristics\n",
    "  - ponies => poni\n",
    "  - Quicker, but less precision\n",
    "- Lemmatization uses vocabulary and morphological analysis\n",
    "  - ponies => pony\n",
    "  - For English, not much improvement over stemming because context of word use is more important\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "- 5 sequential phases of word reductions\n",
    "- Applies rules such as \"sses -> ss\", \"ies => i\"\n",
    "\n",
    "![stemmers](assets/text/stemmers.png)\n",
    "\n",
    "(image: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with spaCy\n",
    "\n",
    "`spacy.lemmatizer.Lemmatizer`\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smrt']\n",
      "['advise']\n",
      "['commuter']\n",
      "['to']\n",
      "['plan']\n",
      "['their']\n",
      "['journey']\n",
      "['ahead']\n",
      "['while']\n",
      "['operate']\n",
      "['hour']\n",
      "['be']\n",
      "['shorten']\n",
      "['.']\n",
      "['it']\n",
      "['will']\n",
      "['deploy']\n",
      "['staff']\n",
      "['to']\n",
      "['assist']\n",
      "['commuter']\n",
      "['during']\n",
      "['the']\n",
      "['affect']\n",
      "['sunday']\n",
      "[',']\n",
      "['it']\n",
      "['say']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "text = u'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    print(lemmatizer(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with NLTK\n",
    "\n",
    "`nltk.stem`\n",
    "- `PorterStemmer`\n",
    "- `WordNetLemmatizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smrt\n",
      "advis\n",
      "commut\n",
      "to\n",
      "plan\n",
      "their\n",
      "journey\n",
      "ahead\n",
      "while\n",
      "oper\n",
      "hour\n",
      "are\n",
      "shorten\n",
      ".\n",
      "It\n",
      "will\n",
      "deploy\n",
      "staff\n",
      "to\n",
      "assist\n",
      "commut\n",
      "dure\n",
      "the\n",
      "affect\n",
      "sunday\n",
      ",\n",
      "it\n",
      "said\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "text = u'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/lisaong/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "SMRT\n",
      "advised\n",
      "commuter\n",
      "to\n",
      "plan\n",
      "their\n",
      "journey\n",
      "ahead\n",
      "while\n",
      "operating\n",
      "hour\n",
      "are\n",
      "shortened\n",
      ".\n",
      "It\n",
      "will\n",
      "deploy\n",
      "staff\n",
      "to\n",
      "assist\n",
      "commuter\n",
      "during\n",
      "the\n",
      "affected\n",
      "Sundays\n",
      ",\n",
      "it\n",
      "said\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "text = u'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "for token in tokens:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "- Find and classify entities within text\n",
    "  - Persons\n",
    "  - Organizations\n",
    "  - Locations\n",
    "  - Time expressions\n",
    "  - Quantities\n",
    "  - Phone numbers\n",
    "  - etc\n",
    "  \n",
    "- Grammar-based models, trained classifiers\n",
    "\n",
    "- Can be corpus-dependent, see https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with spaCy\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BPLRT ORG 38 43\n",
      "Sundays DATE 71 78\n",
      "July DATE 93 97\n",
      "hours TIME 118 123\n",
      "SMRT ORG 153 157\n",
      "Thursday DATE 161 169\n",
      "Jun 21 PERSON 171 177\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"The Bukit Panjang Light Rail Transit (BPLRT) will resume operations on Sundays starting from July, although operating hours will be shortened, announced SMRT on Thursday (Jun 21).\")\n",
    "# doc = nlp(u\"The Bukit Panjang Light Rail Transit (BPLRT) will resume operations on Sundays starting from July, although operating hours will be shortened, announced SMRT on Thursday (June 21).\")\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, entity.start_char, entity.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'People, including fictional'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PERSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">The Bukit Panjang Light Rail Transit (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    BPLRT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ") will resume operations on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Sundays\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " starting from \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    July\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", although operating \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    hours\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">TIME</span>\n",
       "</mark>\n",
       " will be shortened, announced \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    SMRT\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " on \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Thursday\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " (\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Jun 21\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ").</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"The Bukit Panjang Light Rail Transit (BPLRT) will resume operations on Sundays starting from July, although operating hours will be shortened, announced SMRT on Thursday (Jun 21).\")\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "\n",
    "```\n",
    "nltk.ne_chunk()\n",
    "```\n",
    "\n",
    "https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/lisaong/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/lisaong/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tree('S', [Tree('ORGANIZATION', [('SMRT', 'NNP')]), ('advised', 'VBD'), ('commuters', 'NNS'), ('to', 'TO'), ('plan', 'VB'), ('their', 'PRP$'), ('journeys', 'NNS'), ('ahead', 'RB'), ('while', 'IN'), ('operating', 'NN'), ('hours', 'NNS'), ('are', 'VBP'), ('shortened', 'VBN'), ('.', '.')]),\n",
       " Tree('S', [('It', 'PRP'), ('will', 'MD'), ('deploy', 'VB'), ('staff', 'NN'), ('to', 'TO'), ('assist', 'VB'), ('commuters', 'NNS'), ('during', 'IN'), ('the', 'DT'), ('affected', 'JJ'), Tree('ORGANIZATION', [('Sundays', 'NNP')]), (',', ','), ('it', 'PRP'), ('said', 'VBD'), ('.', '.')])]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "text = 'SMRT advised commuters to plan their journeys ahead while operating hours are shortened. It will deploy staff to assist commuters during the affected Sundays, it said.'\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Input to ne_chunk needs to be a part-of-speech tagged word\n",
    "sentences_pos_tagged = [nltk.pos_tag(word) for word in sentences]\n",
    "\n",
    "[nltk.ne_chunk(word_pos) for word_pos in sentences_pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Workshop: Creating Word2Vec Models\n",
    "\n",
    "Credits:\n",
    "- https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/\n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors\n",
    "\n",
    "Word2Vec\n",
    "- Semantic learning of text representations\n",
    "- Neural network \n",
    "- Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Download text\n",
    "\n",
    "For demonstration purposes, we'll start with Wikipedia articles.\n",
    "\n",
    "We'll use a python library that wraps the Wikipedia APIs.\n",
    "\n",
    "https://pypi.org/project/wikipedia/\n",
    "\n",
    "Run this from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) pip install wikipedia\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia import search, page\n",
    "\n",
    "# Get our documents: wikipedia articles\n",
    "topic = 'singapore'\n",
    "\n",
    "titles = search(topic)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# retrieve all pages\n",
    "wikipages = [page(title) for title in titles]\n",
    "\n",
    "# inspect the first page\n",
    "wikipages[0].summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Process text\n",
    "\n",
    "- Split into sentences\n",
    "- Remove special characters\n",
    "- Convert to lowercase\n",
    "- Tokenize the text into words\n",
    "- Optionally remove stop words such as 'a', 'the'\n",
    "- Stem each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # python regular expressions library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK corpora\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "\n",
    "# 1. Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Download the Stop Words corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 3. Helper function to convert text\n",
    "def text_to_sentence_wordlists(text, remove_stopwords=True):\n",
    "    \"\"\"Cleans and converts text to a list of lists of tokens\n",
    "    Args:\n",
    "        text: input text\n",
    "        remove_stopwords: whether to remove stopwords\n",
    "    Returns: a tuple\n",
    "        A list of lists of tokens that looks like:\n",
    "           [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "        Total of words\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    # Reference: http://www.nltk.org/api/nltk.tokenize.html\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # set of stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    wordcount = 0\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        # Remove non-letters and numbers\n",
    "        sentence = re.sub('[^a-zA-Z0-9]', ' ', sentence)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Tokenize the sentence into words\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "        if remove_stopwords:\n",
    "            # Remove stop words\n",
    "            tokens = [token for token in tokens if not token in stops]\n",
    "    \n",
    "        # Stem the words\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        result += [tokens]\n",
    "        wordcount += len(tokens)\n",
    "    \n",
    "    return (result, wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Test our helper function to see what it does\n",
    "text = wikipages[0].summary\n",
    "print('===== Original text for first article =====')\n",
    "print(text)\n",
    "\n",
    "wordlist, count = text_to_sentence_wordlists(text,\n",
    "                                             remove_stopwords=False)\n",
    "print('\\n===== Stem words [%d words] =====' % count)\n",
    "print(wordlist)\n",
    "\n",
    "wordlist, count = text_to_sentence_wordlists(text)\n",
    "print('\\n===== Stem words - stopwords [%d words] =====' % count)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Convert all articles to sentence word lists\n",
    "\n",
    "Let's now convert all articles on our topic to sentence word lists.\n",
    "\n",
    "We were examining the summary for each article, let's see how we can get to the content.\n",
    "\n",
    "Looking at the wikipedia library's documentation, we can use `WikipediaPage.content` to get to the plain text content for each page: https://wikipedia.readthedocs.io/en/latest/code.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Converting %d articles to training set...' % len(titles))\n",
    "\n",
    "training_set = []\n",
    "training_set_size = 0\n",
    "\n",
    "for wikipage in wikipages:\n",
    "    wordlist, count = text_to_sentence_wordlists(wikipage.content)\n",
    "\n",
    "    training_set_size += count\n",
    "    training_set += wordlist\n",
    "    \n",
    "print('Training set size: %d stem words, %d sentences' \\\n",
    "      % (training_set_size, len(training_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Question to ponder:\n",
    "\n",
    "Should we randomize the training set?\n",
    "\n",
    "Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Train a word2vec model\n",
    "\n",
    "(Credits: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors)\n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced.\n",
    "\n",
    "For details on the algorithms below, see the [word2vec API documentation](https://radimrehurek.com/gensim/models/word2vec.html) as well as the [Google documentation](https://code.google.com/archive/p/word2vec/)(Performance section).\n",
    "\n",
    "### Domain characteristics\n",
    "\n",
    "Our training set is:\n",
    "- Small (under 25k words). Typically, word2vec training sets can go in hundreds of thousands.\n",
    "- Wikipedia articles about a common topic. We'll expect some words (e.g. singapore) to appear more frequently about that topic. Whether this is something we need to worry about is unclear.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Architecture:\n",
    "Architecture options are skip-gram (the default: slower, better for infrequent words) or continuous bag of words (fast). \n",
    "\n",
    "#### Training algorithm:\n",
    "This controls which algorithm to use.\n",
    "\n",
    "Hierarchical softmax (the default: better for infrequent words) or negative sampling (better for frequent words, better with low dimensional vectors). Start with the default first.\n",
    "\n",
    "#### Downsampling of frequent words:\n",
    "This controls the threshold for frequent words to be removed randomly. \n",
    "\n",
    "Randomly removing frequent words in large datasets can improve both accuracy and speed.\n",
    "\n",
    "$$p = \\frac{f-t}{f} - \\sqrt{\\frac{t}{f}}$$\n",
    "\n",
    "Where:\n",
    "- $p$: probabability that word is present\n",
    "- $f$: frequency of word in corpus\n",
    "- $t$: the threshold (our downsampling hyperparameter)\n",
    "\n",
    "A smaller $t$ means more words will be randomly removed.\n",
    "\n",
    "(Source: https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf)\n",
    "\n",
    "The [Google documentation](https://code.google.com/archive/p/word2vec/) recommends values between 1e-3 and 1e-5. Let's try 1e-3 and then iterate from there, since our training set is small.\n",
    "\n",
    "#### Word vector dimensionality:\n",
    "This controls how many features the word vector should have. Higher dimensionality (more features) usually result in better models, but also longer runtimes. Reasonable values can be in the tens to hundreds. We'll try 200.\n",
    "\n",
    "#### Context / window size:\n",
    "This defines the window-size to look for related words. For skip-gram usually around 10, for CBOW around 5. More is better, up to a point.\n",
    "\n",
    "### Worker threads:\n",
    "Number of parallel processes to run. This can significantly improve training speed.  \n",
    "\n",
    "The number to choose depends on how many logical CPU cores your computer has (on Windows, Start Menu -> System Information, look for Processors). \n",
    "\n",
    "Start with a number around 2-4, and then increase up if your computer is more powerful.\n",
    "\n",
    "### Minimum word count:\n",
    "This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. \n",
    "\n",
    "Reasonable values could be between 10 and 100. Higher values also help limit run time.\n",
    "\n",
    "For wikipedia articles, we'll try a minimum wordcount of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "word2vec.Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors\n",
    "\n",
    "# Set values for various parameters\n",
    "sg = 1                # Algorithm: 1: skip-gram, 0: CBOW\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 2       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Initialize and train the model.\n",
    "# This may take a while if your training set is large (e.g. 500,000 words)\n",
    "print('Training Word2Vec model...')\n",
    "%time model = word2vec.Word2Vec(training_set, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"wikipedia_{}features_{}minwords_{}context_{}downsampling.w2v\" \\\n",
    "    .format(num_features, min_word_count, context, str(downsampling))\n",
    "model.save(model_name)\n",
    "\n",
    "print('Saved model as %s' % model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Loading the saved model\n",
    "\n",
    "Here's how to load a previously saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"wikipedia_100features_50minwords_10context.w2v\"\n",
    "\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "The trained model contains a read-only `models.keyedvectors.Word2VecMeyedVectors` with methods for evaluating word relationships.\n",
    "\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "\n",
    "Here are some things to try with the word2vec model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Get the vocabulary of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in the vocab\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Check if a stem word is in the model's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('malaysia') in model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('korea') in model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Find a word that doesn't match in a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test = 'raffles indian chinese malay'\n",
    "\n",
    "# you can either use the helper function to convert to stem words\n",
    "# or call stemmer.stem() directly on each word\n",
    "wordlist, _ = text_to_sentence_wordlists(test)\n",
    "print('Input: %s' % wordlist[0])\n",
    "\n",
    "print(\"Word that doesn't match: %s\"\n",
    "      % model.wv.doesnt_match(wordlist[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top N most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = stemmer.stem('singapore')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = stemmer.stem('changi')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Measures the cosine distance and similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('changi')\n",
    "word2 = stemmer.stem('aircraft')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('changi')\n",
    "word2 = stemmer.stem('british')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Returns the word's representation in vector space as a 1D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word = stemmer.stem('malaysia')\n",
    "\n",
    "raw_vectors = model.wv.word_vec(word, use_norm=True)\n",
    "\n",
    "raw_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Visualizing Word2Vec\n",
    "\n",
    "Next, we'll plot the Word Vectors to see how the clusters look like:\n",
    "\n",
    "1. Use t-Distributed Stochastic Neighbor Embedding [TSNE](https://lvdmaaten.github.io/tsne/) to reduce the high-dimensional model into 2D\n",
    "2. Plot the 2D representation of the word2vec model, with the words in its vocabulary as the labels\n",
    "\n",
    "Credits: https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "# Apply t-SNE\n",
    "# this can take a while (like 1 minute or more)\n",
    "tsne = TSNE(n_components=2)\n",
    "%time X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe for the 2 dimensions,\n",
    "# indexed by the words in the vocab\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# create a zoomable interactive plot\n",
    "%matplotlib notebook\n",
    "\n",
    "# Plot the 2D representation of the word2vec model,\n",
    "# with the words in its vocabulary as the labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Create Corpus and Train Word2Vec\n",
    "\n",
    "In this exercise, we will create our own corpus and use it to train Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Create Corpus\n",
    "\n",
    "Create a corpus of text files, organized in a structure like this:\n",
    "\n",
    "```\n",
    "corpus/\n",
    "   text001.txt\n",
    "   text002.txt\n",
    "   text003.txt\n",
    "   ...\n",
    "```\n",
    "\n",
    "A sample corpus is included in the `corpus` folder, created with the first 3 chapters of Moby Dick:\n",
    "https://www.gutenberg.org/files/2701/2701-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Import corpus using NLTK\n",
    "\n",
    "We will use [`nltk.corpus.reader.plaintext`](http://www.nltk.org/howto/corpus.html) to import the corpus.\n",
    "\n",
    "Credits: https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# directory containing the corpus\n",
    "corpus_dir = 'corpus/'\n",
    "\n",
    "# PlaintextCorpusReader uses nltk.tokenize.sent_tokenize() and\n",
    "# nltk.tokenize.word_tokenize() to split texts into sentences and words\n",
    "newcorpus = PlaintextCorpusReader(corpus_dir,\n",
    "                                  '.*\\.txt',\n",
    "                                  encoding='latin1') # or 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files found by the reader\n",
    "newcorpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first file in the corpus\n",
    "f = newcorpus.open('text001.txt')\n",
    "print(f.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# sentences in the corpus:\n",
    "newcorpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# number of sentences\n",
    "len(newcorpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence_lists(sentence_lists, remove_stopwords=True):\n",
    "    \"\"\"Cleans and converts the sentence lists\n",
    "    Args:\n",
    "        text: sentence lists\n",
    "        remove_stopwords: whether to remove stopwords\n",
    "    Returns:\n",
    "        A tuple:\n",
    "            The cleaned sentence list\n",
    "            The token count\n",
    "    \"\"\"\n",
    "    # set of English stop words\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    result = []\n",
    "    wordcount = 0\n",
    "\n",
    "    for sentence in sentence_lists:\n",
    "        # Convert to lowercase\n",
    "        tokens = [t.lower() for t in sentence]\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stopwords:\n",
    "            tokens = [t for t in tokens if not t in stops]\n",
    "        \n",
    "        # Remove non-letters and numbers\n",
    "        tokens = [re.sub('[^a-zA-Z0-9]', '', t) for t in tokens]\n",
    "        \n",
    "        # Stem the words\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        result += [tokens]\n",
    "        wordcount += len(tokens)\n",
    "    \n",
    "    return (result, wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your Tasks:\n",
    "\n",
    "1. Convert newcorpus.sents() to sentence wordlists, using the `clean_sentence_lists` helper function\n",
    "2. Train a Word2Vec model, with initial hyperparameter settings (use your best guess)\n",
    "3. Try some word similarity queries\n",
    "4. Tweak your model by adjusting some hyperparameter settings\n",
    "5. Plot the completed Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert newcorpus.sents() to sentence wordlists, \n",
    "# using the clean_sentence_lists helper function\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "print('Converting %d sentences to training set...' % len(newcorpus.sents()))\n",
    "\n",
    "training_set, training_set_size = clean_sentence_lists(newcorpus.sents())\n",
    "\n",
    "print('Training set size: %d stem words, %d sentences' \\\n",
    "      % (training_set_size, len(training_set)))\n",
    "\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Train a Word2Vec model, with initial hyperparameter settings\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "# Set values for various parameters\n",
    "sg = 1                # Algorithm: 1: skip-gram, 0: CBOW\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 2       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Initialize and train the model.\n",
    "# This may take a while if your training set is large (e.g. 500,000 words)\n",
    "print('Training Word2Vec model...')\n",
    "%time model = word2vec.Word2Vec(training_set, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"corpus_{}features_{}minwords_{}context_{}downsampling.w2v\" \\\n",
    "    .format(num_features, min_word_count, context, str(downsampling))\n",
    "model.save(model_name)\n",
    "\n",
    "print('Saved model as %s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Try some word similarity queries\n",
    "# Your code here\n",
    "\n",
    "print('Vocab length:', len(model.wv.vocab))\n",
    "print('Vocab:', model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('whale')\n",
    "word2 = stemmer.stem('harpoon')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('whale')\n",
    "word2 = stemmer.stem('landlord')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word = stemmer.stem('harpoon')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "# Apply t-SNE\n",
    "# this can take a while (like 1 minute or more)\n",
    "tsne = TSNE(n_components=2)\n",
    "%time X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Create a dataframe for the 2 dimensions,\n",
    "# indexed by the words in the vocab\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the completed Word2Vec model\n",
    "\n",
    "# create a zoomable interactive plot\n",
    "%matplotlib notebook\n",
    "\n",
    "# Plot the 2D representation of the word2vec model,\n",
    "# with the words in its vocabulary as the labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "https://github.com/charlieg/A-Smattering-of-NLP-in-Python\n",
    "\n",
    "http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
