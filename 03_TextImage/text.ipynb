{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "- Overview\n",
    "- Parsing, Stemming, Lemmatization\n",
    "- Named Entity Recognition\n",
    "- Stop Words\n",
    "- Frequency Analysis\n",
    "- Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "NLTK book: http://www.nltk.org/book/\n",
    "\n",
    "spaCy 101: https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run this command from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) conda install gensim cython nltk spacy scikit-learn\n",
    "```\n",
    "\n",
    "### gensim: for training word2vec\n",
    "\n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "\n",
    "### Cython: to speed up training word2vec\n",
    "http://docs.cython.org/en/latest/src/quickstart/install.html\n",
    "\n",
    "\n",
    "### NLTK: NLP toolkit\n",
    "Installation: https://www.nltk.org/install.html\n",
    "\n",
    "Book: http://www.nltk.org/book\n",
    "\n",
    "### spaCy: another NLP toolkit\n",
    "\n",
    "Simpler to use than NLTK (but usually fewer knobs)\n",
    "\n",
    "https://spacy.io/api/\n",
    "\n",
    "https://spacy.io/usage/spacy-101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# What is Text Processing?\n",
    "\n",
    "- A sub-field of Natural Language Processing (NLP)\n",
    "- Natural Language Processing is ...\n",
    " - Teaching machines to understand and produce language (text, speech)\n",
    " - A combination of computer science and computational linguistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Text Processing Tasks\n",
    "\n",
    "- Word categorization and tagging: part of speech, type of entity\n",
    "- Semantic Analysis: finding meanings of documents\n",
    "- Topic Modeling: finding topics from documents\n",
    "- Document similarity: comparing if two documents are semantically similar\n",
    "- etc.\n",
    "\n",
    "Note: Speech is text processing + acoustic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Parsing, Stemming & Lemmatization\n",
    "\n",
    "- Tokenization: splitting text into words\n",
    "- Sentence boundary detection: splitting text into sentences\n",
    "- Stemming: finding word stems\n",
    "   - stating => state, reference => refer\n",
    "- Lemmatization: finding the base form of words\n",
    "   - was => be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tokenization\n",
    "\n",
    "- Segmenting text into words, punctuation, etc.\n",
    "- Rule-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Tokenization with spaCy\n",
    "\n",
    "![tokenization in spaCy](assets/text/tokenization.svg)\n",
    "\n",
    "(image: https://spacy.io/usage/spacy-101#annotations-token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm: started\n",
      "    Running setup.py install for en-core-web-sm: finished with status 'done'\n",
      "Successfully installed en-core-web-sm-2.0.0\n",
      "\n",
      "    Error: Couldn't link model to 'en_core_web_sm'\n",
      "    Creating a symlink in spacy/data failed. Make sure you have the required\n",
      "    permissions and try re-running the command as admin, or use a\n",
      "    virtualenv. You can still import the model as a module and call its\n",
      "    load() method, or create the symlink manually.\n",
      "\n",
      "    C:\\Users\\issohl\\AppData\\Local\\Continuum\\miniconda3\\envs\\mldds03\\lib\\site-packages\\en_core_web_sm\n",
      "    -->\n",
      "    C:\\Users\\issohl\\AppData\\Local\\Continuum\\miniconda3\\envs\\mldds03\\lib\\site-packages\\spacy\\data\\en_core_web_sm\n",
      "\n",
      "\n",
      "    Creating a shortcut link for 'en' didn't work (maybe you don't have\n",
      "    admin permissions?), but you can still load the model via its full\n",
      "    package name: nlp = spacy.load('{name}')\n",
      "    Download successful but linking failed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download the English model\n",
    "# You can find other models here: https://spacy.io/models/en\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.channelnewsasia.com/news/lifestyle/singapore-millennials-food-destinations-eating-holiday-tokyo-10458484\n",
    "text = u\"According to a new survey by Hotels.com, \" \\\n",
    "      u\"87 per cent of Singaporean millennials admitted that food is the biggest deciding \" \\\n",
    "      u\"factor when deciding where to go – and 78 per cent said they spend most of their time \" \\\n",
    "      u\"munching their way around a place rather than visiting tourist landmarks (60 per cent) or shopping (40 per cent).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According VERB prep\n",
      "to ADP prep\n",
      "a DET det\n",
      "new ADJ amod\n",
      "survey NOUN pobj\n",
      "by ADP prep\n",
      "Hotels.com PROPN pobj\n",
      ", PUNCT punct\n",
      "87 NUM nummod\n",
      "per ADP nmod\n",
      "cent NOUN appos\n",
      "of ADP prep\n",
      "Singaporean ADJ amod\n",
      "millennials NOUN pobj\n",
      "admitted VERB ROOT\n",
      "that ADP mark\n",
      "food NOUN nsubj\n",
      "is VERB ccomp\n",
      "the DET det\n",
      "biggest ADJ amod\n",
      "deciding VERB amod\n",
      "factor NOUN attr\n",
      "when ADV advmod\n",
      "deciding VERB advcl\n",
      "where ADV advmod\n",
      "to PART aux\n",
      "go VERB xcomp\n",
      "– PUNCT punct\n",
      "and CCONJ cc\n",
      "78 NUM conj\n",
      "per NOUN prep\n",
      "cent NOUN pobj\n",
      "said VERB conj\n",
      "they PRON nsubj\n",
      "spend VERB ccomp\n",
      "most ADJ dobj\n",
      "of ADP prep\n",
      "their ADJ poss\n",
      "time NOUN pobj\n",
      "munching VERB xcomp\n",
      "their ADJ poss\n",
      "way NOUN dobj\n",
      "around ADP prep\n",
      "a DET det\n",
      "place NOUN pobj\n",
      "rather ADV advmod\n",
      "than ADP prep\n",
      "visiting VERB conj\n",
      "tourist NOUN compound\n",
      "landmarks NOUN dobj\n",
      "( PUNCT punct\n",
      "60 NUM appos\n",
      "per NOUN prep\n",
      "cent NOUN pobj\n",
      ") PUNCT punct\n",
      "or CCONJ cc\n",
      "shopping NOUN conj\n",
      "( PUNCT punct\n",
      "40 NUM parataxis\n",
      "per NOUN prep\n",
      "cent NOUN pobj\n",
      ") PUNCT punct\n",
      ". PUNCT punct\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc:\n",
    "    # text, part-of-speech, syntactic dependencies\n",
    "    print(token.text, token.pos_, token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'determiner'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('DET')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://spacy.io/api/token\n",
    "\n",
    "https://spacy.io/api/token#attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"7890\" height=\"697.0\" style=\"max-width: none; height: 697.0px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">According</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"190\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"190\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"330\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"330\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"470\">new</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"470\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">survey</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">by</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">Hotels.com,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1030\">87</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1030\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">per</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">cent</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1450\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1450\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1590\">Singaporean</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1590\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">millennials</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1870\">admitted</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1870\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2010\">that</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2010\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2150\">food</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2150\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2290\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2290\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2430\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2430\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2570\">biggest</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2570\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2710\">deciding</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2710\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2850\">factor</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2850\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"2990\">when</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"2990\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3130\">deciding</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3130\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3270\">where</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3270\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3410\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3410\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3550\">go –</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3550\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3690\">and</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3690\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3830\">78</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3830\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"3970\">per</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"3970\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4110\">cent</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4110\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4250\">said</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4250\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4390\">they</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4390\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4530\">spend</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4530\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4670\">most</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4670\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4810\">of</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4810\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"4950\">their</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"4950\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5090\">time</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5090\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5230\">munching</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5230\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5370\">their</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5370\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5510\">way</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5510\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5650\">around</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5650\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5790\">a</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5790\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"5930\">place</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"5930\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6070\">rather</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6070\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6210\">than</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6210\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6350\">visiting</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6490\">tourist</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6490\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6630\">landmarks (</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6630\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6770\">60</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6770\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"6910\">per</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"6910\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7050\">cent)</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7050\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7190\">or</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7190\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7330\">shopping (</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7330\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7470\">40</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7470\">NUM</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7610\">per</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7610\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"607.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"7750\">cent).</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"7750\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,562.0 C70,72.0 1865.0,72.0 1865.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,564.0 L62,552.0 78,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M70,562.0 C70,492.0 155.0,492.0 155.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M155.0,564.0 L163.0,552.0 147.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M350,562.0 C350,422.0 580.0,422.0 580.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350,564.0 L342,552.0 358,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M490,562.0 C490,492.0 575.0,492.0 575.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M490,564.0 L482,552.0 498,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M210,562.0 C210,352.0 585.0,352.0 585.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M585.0,564.0 L593.0,552.0 577.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M630,562.0 C630,492.0 715.0,492.0 715.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M715.0,564.0 L723.0,552.0 707.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M770,562.0 C770,492.0 855.0,492.0 855.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M855.0,564.0 L863.0,552.0 847.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-7\" stroke-width=\"2px\" d=\"M1050,562.0 C1050,422.0 1280.0,422.0 1280.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-7\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1050,564.0 L1042,552.0 1058,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-8\" stroke-width=\"2px\" d=\"M1190,562.0 C1190,492.0 1275.0,492.0 1275.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-8\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1190,564.0 L1182,552.0 1198,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-9\" stroke-width=\"2px\" d=\"M910,562.0 C910,352.0 1285.0,352.0 1285.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-9\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1285.0,564.0 L1293.0,552.0 1277.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-10\" stroke-width=\"2px\" d=\"M1330,562.0 C1330,492.0 1415.0,492.0 1415.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-10\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1415.0,564.0 L1423.0,552.0 1407.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-11\" stroke-width=\"2px\" d=\"M1610,562.0 C1610,492.0 1695.0,492.0 1695.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-11\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1610,564.0 L1602,552.0 1618,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-12\" stroke-width=\"2px\" d=\"M1470,562.0 C1470,422.0 1700.0,422.0 1700.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-12\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1700.0,564.0 L1708.0,552.0 1692.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-13\" stroke-width=\"2px\" d=\"M2030,562.0 C2030,422.0 2260.0,422.0 2260.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-13\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">mark</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2030,564.0 L2022,552.0 2038,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-14\" stroke-width=\"2px\" d=\"M2170,562.0 C2170,492.0 2255.0,492.0 2255.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-14\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2170,564.0 L2162,552.0 2178,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-15\" stroke-width=\"2px\" d=\"M1890,562.0 C1890,352.0 2265.0,352.0 2265.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-15\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2265.0,564.0 L2273.0,552.0 2257.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-16\" stroke-width=\"2px\" d=\"M2450,562.0 C2450,352.0 2825.0,352.0 2825.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-16\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2450,564.0 L2442,552.0 2458,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-17\" stroke-width=\"2px\" d=\"M2590,562.0 C2590,422.0 2820.0,422.0 2820.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-17\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2590,564.0 L2582,552.0 2598,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-18\" stroke-width=\"2px\" d=\"M2730,562.0 C2730,492.0 2815.0,492.0 2815.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-18\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2730,564.0 L2722,552.0 2738,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-19\" stroke-width=\"2px\" d=\"M2310,562.0 C2310,282.0 2830.0,282.0 2830.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-19\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">attr</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M2830.0,564.0 L2838.0,552.0 2822.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-20\" stroke-width=\"2px\" d=\"M3010,562.0 C3010,492.0 3095.0,492.0 3095.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-20\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3010,564.0 L3002,552.0 3018,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-21\" stroke-width=\"2px\" d=\"M2310,562.0 C2310,142.0 3120.0,142.0 3120.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-21\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advcl</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3120.0,564.0 L3128.0,552.0 3112.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-22\" stroke-width=\"2px\" d=\"M3290,562.0 C3290,422.0 3520.0,422.0 3520.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-22\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3290,564.0 L3282,552.0 3298,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-23\" stroke-width=\"2px\" d=\"M3430,562.0 C3430,492.0 3515.0,492.0 3515.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-23\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3430,564.0 L3422,552.0 3438,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-24\" stroke-width=\"2px\" d=\"M3150,562.0 C3150,352.0 3525.0,352.0 3525.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-24\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3525.0,564.0 L3533.0,552.0 3517.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-25\" stroke-width=\"2px\" d=\"M3570,562.0 C3570,492.0 3655.0,492.0 3655.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-25\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3655.0,564.0 L3663.0,552.0 3647.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-26\" stroke-width=\"2px\" d=\"M3570,562.0 C3570,422.0 3800.0,422.0 3800.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-26\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3800.0,564.0 L3808.0,552.0 3792.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-27\" stroke-width=\"2px\" d=\"M3850,562.0 C3850,492.0 3935.0,492.0 3935.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-27\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M3935.0,564.0 L3943.0,552.0 3927.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-28\" stroke-width=\"2px\" d=\"M3990,562.0 C3990,492.0 4075.0,492.0 4075.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-28\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4075.0,564.0 L4083.0,552.0 4067.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-29\" stroke-width=\"2px\" d=\"M1890,562.0 C1890,2.0 4250.0,2.0 4250.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-29\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4250.0,564.0 L4258.0,552.0 4242.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-30\" stroke-width=\"2px\" d=\"M4410,562.0 C4410,492.0 4495.0,492.0 4495.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-30\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4410,564.0 L4402,552.0 4418,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-31\" stroke-width=\"2px\" d=\"M4270,562.0 C4270,422.0 4500.0,422.0 4500.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-31\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4500.0,564.0 L4508.0,552.0 4492.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-32\" stroke-width=\"2px\" d=\"M4550,562.0 C4550,492.0 4635.0,492.0 4635.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-32\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4635.0,564.0 L4643.0,552.0 4627.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-33\" stroke-width=\"2px\" d=\"M4690,562.0 C4690,492.0 4775.0,492.0 4775.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-33\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4775.0,564.0 L4783.0,552.0 4767.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-34\" stroke-width=\"2px\" d=\"M4970,562.0 C4970,492.0 5055.0,492.0 5055.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-34\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M4970,564.0 L4962,552.0 4978,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-35\" stroke-width=\"2px\" d=\"M4830,562.0 C4830,422.0 5060.0,422.0 5060.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-35\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5060.0,564.0 L5068.0,552.0 5052.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-36\" stroke-width=\"2px\" d=\"M4550,562.0 C4550,212.0 5215.0,212.0 5215.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-36\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5215.0,564.0 L5223.0,552.0 5207.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-37\" stroke-width=\"2px\" d=\"M5390,562.0 C5390,492.0 5475.0,492.0 5475.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-37\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">poss</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5390,564.0 L5382,552.0 5398,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-38\" stroke-width=\"2px\" d=\"M5250,562.0 C5250,422.0 5480.0,422.0 5480.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-38\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5480.0,564.0 L5488.0,552.0 5472.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-39\" stroke-width=\"2px\" d=\"M5530,562.0 C5530,492.0 5615.0,492.0 5615.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-39\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5615.0,564.0 L5623.0,552.0 5607.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-40\" stroke-width=\"2px\" d=\"M5810,562.0 C5810,492.0 5895.0,492.0 5895.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-40\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5810,564.0 L5802,552.0 5818,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-41\" stroke-width=\"2px\" d=\"M5670,562.0 C5670,422.0 5900.0,422.0 5900.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-41\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M5900.0,564.0 L5908.0,552.0 5892.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-42\" stroke-width=\"2px\" d=\"M6090,562.0 C6090,492.0 6175.0,492.0 6175.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-42\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6090,564.0 L6082,552.0 6098,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-43\" stroke-width=\"2px\" d=\"M5950,562.0 C5950,422.0 6180.0,422.0 6180.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-43\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6180.0,564.0 L6188.0,552.0 6172.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-44\" stroke-width=\"2px\" d=\"M5950,562.0 C5950,352.0 6325.0,352.0 6325.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-44\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6325.0,564.0 L6333.0,552.0 6317.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-45\" stroke-width=\"2px\" d=\"M6510,562.0 C6510,492.0 6595.0,492.0 6595.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-45\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6510,564.0 L6502,552.0 6518,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-46\" stroke-width=\"2px\" d=\"M6370,562.0 C6370,422.0 6600.0,422.0 6600.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-46\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6600.0,564.0 L6608.0,552.0 6592.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-47\" stroke-width=\"2px\" d=\"M6650,562.0 C6650,492.0 6735.0,492.0 6735.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-47\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6735.0,564.0 L6743.0,552.0 6727.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-48\" stroke-width=\"2px\" d=\"M6790,562.0 C6790,492.0 6875.0,492.0 6875.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-48\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M6875.0,564.0 L6883.0,552.0 6867.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-49\" stroke-width=\"2px\" d=\"M6930,562.0 C6930,492.0 7015.0,492.0 7015.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-49\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7015.0,564.0 L7023.0,552.0 7007.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-50\" stroke-width=\"2px\" d=\"M6790,562.0 C6790,352.0 7165.0,352.0 7165.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-50\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7165.0,564.0 L7173.0,552.0 7157.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-51\" stroke-width=\"2px\" d=\"M6790,562.0 C6790,282.0 7310.0,282.0 7310.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-51\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7310.0,564.0 L7318.0,552.0 7302.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-52\" stroke-width=\"2px\" d=\"M7350,562.0 C7350,492.0 7435.0,492.0 7435.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-52\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">parataxis</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7435.0,564.0 L7443.0,552.0 7427.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-53\" stroke-width=\"2px\" d=\"M7490,562.0 C7490,492.0 7575.0,492.0 7575.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-53\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7575.0,564.0 L7583.0,552.0 7567.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-54\" stroke-width=\"2px\" d=\"M7630,562.0 C7630,492.0 7715.0,492.0 7715.0,562.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-54\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M7715.0,564.0 L7723.0,552.0 7707.0,552.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text)\n",
    "\n",
    "displacy.render(doc, style='dep', jupyter=True, options={'distance': 140})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Tokenization with NLTK\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html\n",
    "\n",
    "nltk.tokenize\n",
    " - sent_tokenize\n",
    " - word_tokenize\n",
    " - wordpunc_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.channelnewsasia.com/news/lifestyle/singapore-millennials-food-destinations-eating-holiday-tokyo-10458484\n",
    "text2 = u\"On average, they also take around 157 photos during a weeklong holiday – a fifth of which are of food or \" \\\n",
    "        u\"F&B establishments. A third of the Singaporean respondents also admitted they preferred taking Instagram-worthy \"\\\n",
    "        u\"photos of their meals over photos with friends or selfies.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On average, they also take around 157 photos during a weeklong holiday – a fifth of which are of food or F&B establishments.',\n",
       " 'A third of the Singaporean respondents also admitted they preferred taking Instagram-worthy photos of their meals over photos with friends or selfies.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# list of sentences\n",
    "sent_tokenize(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['According',\n",
       " 'to',\n",
       " 'a',\n",
       " 'new',\n",
       " 'survey',\n",
       " 'by',\n",
       " 'Hotels.com',\n",
       " ',',\n",
       " '87',\n",
       " 'per',\n",
       " 'cent',\n",
       " 'of',\n",
       " 'Singaporean',\n",
       " 'millennials',\n",
       " 'admitted',\n",
       " 'that',\n",
       " 'food',\n",
       " 'is',\n",
       " 'the',\n",
       " 'biggest',\n",
       " 'deciding',\n",
       " 'factor',\n",
       " 'when',\n",
       " 'deciding',\n",
       " 'where',\n",
       " 'to',\n",
       " 'go',\n",
       " '–',\n",
       " 'and',\n",
       " '78',\n",
       " 'per',\n",
       " 'cent',\n",
       " 'said',\n",
       " 'they',\n",
       " 'spend',\n",
       " 'most',\n",
       " 'of',\n",
       " 'their',\n",
       " 'time',\n",
       " 'munching',\n",
       " 'their',\n",
       " 'way',\n",
       " 'around',\n",
       " 'a',\n",
       " 'place',\n",
       " 'rather',\n",
       " 'than',\n",
       " 'visiting',\n",
       " 'tourist',\n",
       " 'landmarks',\n",
       " '(',\n",
       " '60',\n",
       " 'per',\n",
       " 'cent',\n",
       " ')',\n",
       " 'or',\n",
       " 'shopping',\n",
       " '(',\n",
       " '40',\n",
       " 'per',\n",
       " 'cent',\n",
       " ')',\n",
       " '.']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# flat list of words and punctuations\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['On',\n",
       "  'average',\n",
       "  ',',\n",
       "  'they',\n",
       "  'also',\n",
       "  'take',\n",
       "  'around',\n",
       "  '157',\n",
       "  'photos',\n",
       "  'during',\n",
       "  'a',\n",
       "  'weeklong',\n",
       "  'holiday',\n",
       "  '–',\n",
       "  'a',\n",
       "  'fifth',\n",
       "  'of',\n",
       "  'which',\n",
       "  'are',\n",
       "  'of',\n",
       "  'food',\n",
       "  'or',\n",
       "  'F',\n",
       "  '&',\n",
       "  'B',\n",
       "  'establishments',\n",
       "  '.'],\n",
       " ['A',\n",
       "  'third',\n",
       "  'of',\n",
       "  'the',\n",
       "  'Singaporean',\n",
       "  'respondents',\n",
       "  'also',\n",
       "  'admitted',\n",
       "  'they',\n",
       "  'preferred',\n",
       "  'taking',\n",
       "  'Instagram-worthy',\n",
       "  'photos',\n",
       "  'of',\n",
       "  'their',\n",
       "  'meals',\n",
       "  'over',\n",
       "  'photos',\n",
       "  'with',\n",
       "  'friends',\n",
       "  'or',\n",
       "  'selfies',\n",
       "  '.']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text2)\n",
    "\n",
    "# list of lists\n",
    "[word_tokenize(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'The\", 'time', 'is', 'now', '5.30am', ',', \"'\", 'he', 'said', '.']\n",
      "[\"'\", 'The', 'time', 'is', 'now', '5', '.', '30am', \",'\", 'he', 'said', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "text = \"'The time is now 5.30am,' he said.\"\n",
    "\n",
    "print(word_tokenize(text))\n",
    "\n",
    "print(wordpunct_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[('On', 'IN'),\n",
       "  ('average', 'NN'),\n",
       "  (',', ','),\n",
       "  ('they', 'PRP'),\n",
       "  ('also', 'RB'),\n",
       "  ('take', 'VBP'),\n",
       "  ('around', 'RB'),\n",
       "  ('157', 'CD'),\n",
       "  ('photos', 'NNS'),\n",
       "  ('during', 'IN'),\n",
       "  ('a', 'DT'),\n",
       "  ('weeklong', 'JJ'),\n",
       "  ('holiday', 'NN'),\n",
       "  ('–', 'VBD'),\n",
       "  ('a', 'DT'),\n",
       "  ('fifth', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('which', 'WDT'),\n",
       "  ('are', 'VBP'),\n",
       "  ('of', 'IN'),\n",
       "  ('food', 'NN'),\n",
       "  ('or', 'CC'),\n",
       "  ('F', 'NNP'),\n",
       "  ('&', 'CC'),\n",
       "  ('B', 'NNP'),\n",
       "  ('establishments', 'NNS'),\n",
       "  ('.', '.')],\n",
       " [('A', 'DT'),\n",
       "  ('third', 'JJ'),\n",
       "  ('of', 'IN'),\n",
       "  ('the', 'DT'),\n",
       "  ('Singaporean', 'NNP'),\n",
       "  ('respondents', 'NNS'),\n",
       "  ('also', 'RB'),\n",
       "  ('admitted', 'VBD'),\n",
       "  ('they', 'PRP'),\n",
       "  ('preferred', 'VBD'),\n",
       "  ('taking', 'VBG'),\n",
       "  ('Instagram-worthy', 'JJ'),\n",
       "  ('photos', 'NNS'),\n",
       "  ('of', 'IN'),\n",
       "  ('their', 'PRP$'),\n",
       "  ('meals', 'NNS'),\n",
       "  ('over', 'IN'),\n",
       "  ('photos', 'NNS'),\n",
       "  ('with', 'IN'),\n",
       "  ('friends', 'NNS'),\n",
       "  ('or', 'CC'),\n",
       "  ('selfies', 'NNS'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech tagging\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text2)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "[nltk.pos_tag(word) for word in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pronoun, possessive'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PRP$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Twitter-aware tokenizer\n",
    "\n",
    "`nltk.tokenize.TweetTokenizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cooool',\n",
       " '#dummysmiley',\n",
       " ':',\n",
       " ':-)',\n",
       " ':-P',\n",
       " '<3',\n",
       " 'and',\n",
       " 'some',\n",
       " 'arrows',\n",
       " '<',\n",
       " '>',\n",
       " '->',\n",
       " '<--']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "tknzr = TweetTokenizer()\n",
    "tweet = \"This is a cooool #dummysmiley: :-) :-P <3 and some arrows < > -> <--\"\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'This', 'is', 'waaayyy', 'too', 'much', 'for', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = '@remy: This is waaaaayyyy too much for you!!!!!!'\n",
    "\n",
    "tknzr.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stemming vs. Lemmatization\n",
    "\n",
    "- Stemming uses rule-based heuristics\n",
    "  - ponies => poni\n",
    "  - Quicker, but less precision\n",
    "- Lemmatization uses vocabulary and morphological analysis\n",
    "  - ponies => pony\n",
    "  - For English, not much improvement over stemming because context of word use is more important\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Porter Stemmer\n",
    "\n",
    "- 5 sequential phases of word reductions\n",
    "- Applies rules such as \"sses -> ss\", \"ies => i\"\n",
    "\n",
    "![stemmers](assets/text/stemmers.png)\n",
    "\n",
    "(image: https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with spaCy\n",
    "\n",
    "`spacy.lemmatizer.Lemmatizer`\n",
    "\n",
    "https://spacy.io/api/lemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.channelnewsasia.com/news/lifestyle/singapore-millennials-food-destinations-eating-holiday-tokyo-10458484\n",
    "\n",
    "text3 = u\"The global Tasty Travels survey, which was conducted by One Poll in March, \" \\\n",
    "\"included 9,000 respondents across 29 countries, including 300 Singaporeans, aged \" \\\n",
    "\"between 18 and 35 years old. Topping the list of favourite destinations among Singaporean \"\\\n",
    "\"millennials was Tokyo, followed by Seoul, Paris, London, New York, Taipei and Bangkok.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the']\n",
      "['global']\n",
      "['tasty']\n",
      "['travels']\n",
      "['survey']\n",
      "[',']\n",
      "['which']\n",
      "['be']\n",
      "['conduct']\n",
      "['by']\n",
      "['one']\n",
      "['poll']\n",
      "['in']\n",
      "['march']\n",
      "[',']\n",
      "['include']\n",
      "['9,000']\n",
      "['respondent']\n",
      "['across']\n",
      "['29']\n",
      "['country']\n",
      "[',']\n",
      "['include']\n",
      "['300']\n",
      "['singaporeans']\n",
      "[',']\n",
      "['age']\n",
      "['between']\n",
      "['18']\n",
      "['and']\n",
      "['35']\n",
      "['year']\n",
      "['old']\n",
      "['.']\n",
      "['top']\n",
      "['the']\n",
      "['list']\n",
      "['of']\n",
      "['favourite']\n",
      "['destination']\n",
      "['among']\n",
      "['singaporean']\n",
      "['millennial']\n",
      "['be']\n",
      "['tokyo']\n",
      "[',']\n",
      "['follow']\n",
      "['by']\n",
      "['seoul']\n",
      "[',']\n",
      "['paris']\n",
      "[',']\n",
      "['london']\n",
      "[',']\n",
      "['new']\n",
      "['york']\n",
      "[',']\n",
      "['taipei']\n",
      "['and']\n",
      "['bangkok']\n",
      "['.']\n"
     ]
    }
   ],
   "source": [
    "from spacy.lemmatizer import Lemmatizer\n",
    "from spacy.lang.en import LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "lemmatizer = Lemmatizer(LEMMA_INDEX, LEMMA_EXC, LEMMA_RULES)\n",
    "\n",
    "doc = nlp(text3)\n",
    "\n",
    "for token in doc:\n",
    "    print(lemmatizer(token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stemming & Lemmatization with NLTK\n",
    "\n",
    "`nltk.stem`\n",
    "- `PorterStemmer`\n",
    "- `WordNetLemmatizer`\n",
    "\n",
    "http://www.nltk.org/api/nltk.stem.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "global\n",
      "tasti\n",
      "travel\n",
      "survey\n",
      ",\n",
      "which\n",
      "wa\n",
      "conduct\n",
      "by\n",
      "one\n",
      "poll\n",
      "in\n",
      "march\n",
      ",\n",
      "includ\n",
      "9,000\n",
      "respond\n",
      "across\n",
      "29\n",
      "countri\n",
      ",\n",
      "includ\n",
      "300\n",
      "singaporean\n",
      ",\n",
      "age\n",
      "between\n",
      "18\n",
      "and\n",
      "35\n",
      "year\n",
      "old\n",
      ".\n",
      "top\n",
      "the\n",
      "list\n",
      "of\n",
      "favourit\n",
      "destin\n",
      "among\n",
      "singaporean\n",
      "millenni\n",
      "wa\n",
      "tokyo\n",
      ",\n",
      "follow\n",
      "by\n",
      "seoul\n",
      ",\n",
      "pari\n",
      ",\n",
      "london\n",
      ",\n",
      "new\n",
      "york\n",
      ",\n",
      "taipei\n",
      "and\n",
      "bangkok\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "tokens = word_tokenize(text3)\n",
    "\n",
    "for token in tokens:\n",
    "    print(stemmer.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "The\n",
      "global\n",
      "Tasty\n",
      "Travels\n",
      "survey\n",
      ",\n",
      "which\n",
      "wa\n",
      "conducted\n",
      "by\n",
      "One\n",
      "Poll\n",
      "in\n",
      "March\n",
      ",\n",
      "included\n",
      "9,000\n",
      "respondent\n",
      "across\n",
      "29\n",
      "country\n",
      ",\n",
      "including\n",
      "300\n",
      "Singaporeans\n",
      ",\n",
      "aged\n",
      "between\n",
      "18\n",
      "and\n",
      "35\n",
      "year\n",
      "old\n",
      ".\n",
      "Topping\n",
      "the\n",
      "list\n",
      "of\n",
      "favourite\n",
      "destination\n",
      "among\n",
      "Singaporean\n",
      "millennials\n",
      "wa\n",
      "Tokyo\n",
      ",\n",
      "followed\n",
      "by\n",
      "Seoul\n",
      ",\n",
      "Paris\n",
      ",\n",
      "London\n",
      ",\n",
      "New\n",
      "York\n",
      ",\n",
      "Taipei\n",
      "and\n",
      "Bangkok\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "tokens = word_tokenize(text3)\n",
    "\n",
    "for token in tokens:\n",
    "    print(lemmatizer.lemmatize(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "- Find and classify entities within text\n",
    "  - Persons\n",
    "  - Organizations\n",
    "  - Locations\n",
    "  - Time expressions\n",
    "  - Quantities\n",
    "  - Phone numbers\n",
    "  - etc\n",
    "  \n",
    "- Grammar-based models, trained classifiers\n",
    "\n",
    "- Can be corpus-dependent, see https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with spaCy\n",
    "\n",
    "https://spacy.io/api/annotation#named-entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.channelnewsasia.com/news/lifestyle/singapore-millennials-food-destinations-eating-holiday-tokyo-10458484\n",
    "\n",
    "text3 = u\"The global Tasty Travels survey, which was conducted by One Poll in March, \" \\\n",
    "\"included 9,000 respondents across 29 countries, including 300 Singaporeans, aged \" \\\n",
    "\"between 18 and 35 years old. Topping the list of favourite destinations among Singaporean \"\\\n",
    "\"millennials was Tokyo, followed by Seoul, Paris, London, New York, Taipei and Bangkok.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasty Travels ORG 11 24\n",
      "One CARDINAL 56 59\n",
      "March DATE 68 73\n",
      "9,000 CARDINAL 84 89\n",
      "29 CARDINAL 109 111\n",
      "300 CARDINAL 133 136\n",
      "Singaporeans NORP 137 149\n",
      "between 18 and 35 years old DATE 156 183\n",
      "Singaporean NORP 234 245\n",
      "Tokyo GPE 262 267\n",
      "Seoul GPE 281 286\n",
      "Paris GPE 288 293\n",
      "London GPE 295 301\n",
      "New York GPE 303 311\n",
      "Taipei GPE 313 319\n",
      "Bangkok GPE 324 331\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "doc = nlp(text3)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_, entity.start_char, entity.end_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Nationalities or religious or political groups'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('NORP')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"entities\" style=\"line-height: 2.5\">The global \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Tasty Travels\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " survey, which was conducted by \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    One\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " Poll in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    March\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", included \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    9,000\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " respondents across \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    29\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " countries, including \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    300\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Singaporeans\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       ", aged \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    between 18 and 35 years old\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ". Topping the list of favourite destinations among \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Singaporean\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " millennials was \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Tokyo\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", followed by \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Seoul\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Paris\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    London\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    New York\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Taipei\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em; box-decoration-break: clone; -webkit-box-decoration-break: clone\">\n",
       "    Bangkok\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Named Entity Recognition with NLTK\n",
    "\n",
    "```\n",
    "nltk.ne_chunk()\n",
    "```\n",
    "\n",
    "https://www.nltk.org/book/ch07.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Tree('S', [('The', 'DT'), ('global', 'JJ'), Tree('PERSON', [('Tasty', 'NNP'), ('Travels', 'NNP')]), ('survey', 'NN'), (',', ','), ('which', 'WDT'), ('was', 'VBD'), ('conducted', 'VBN'), ('by', 'IN'), ('One', 'CD'), ('Poll', 'NN'), ('in', 'IN'), Tree('GPE', [('March', 'NNP')]), (',', ','), ('included', 'VBD'), ('9,000', 'CD'), ('respondents', 'NNS'), ('across', 'IN'), ('29', 'CD'), ('countries', 'NNS'), (',', ','), ('including', 'VBG'), ('300', 'CD'), ('Singaporeans', 'NNPS'), (',', ','), ('aged', 'VBD'), ('between', 'IN'), ('18', 'CD'), ('and', 'CC'), ('35', 'CD'), ('years', 'NNS'), ('old', 'JJ'), ('.', '.')]),\n",
       " Tree('S', [('Topping', 'VBG'), ('the', 'DT'), ('list', 'NN'), ('of', 'IN'), ('favourite', 'JJ'), ('destinations', 'NNS'), ('among', 'IN'), Tree('GPE', [('Singaporean', 'JJ')]), ('millennials', 'NNS'), ('was', 'VBD'), Tree('GPE', [('Tokyo', 'NNP')]), (',', ','), ('followed', 'VBN'), ('by', 'IN'), Tree('GPE', [('Seoul', 'NNP')]), (',', ','), Tree('GPE', [('Paris', 'NNP')]), (',', ','), Tree('GPE', [('London', 'NNP')]), (',', ','), Tree('GPE', [('New', 'NNP'), ('York', 'NNP')]), (',', ','), Tree('GPE', [('Taipei', 'NNP')]), ('and', 'CC'), Tree('GPE', [('Bangkok', 'NNP')]), ('.', '.')])]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "sentences = sent_tokenize(text3)\n",
    "sentences = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "# Input to ne_chunk needs to be a part-of-speech tagged word\n",
    "sentences_pos_tagged = [nltk.pos_tag(word) for word in sentences]\n",
    "\n",
    "[nltk.ne_chunk(word_pos) for word_pos in sentences_pos_tagged]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Stop words\n",
    "\n",
    "Stop words are high-frequency words that don't contribute much lexical content:\n",
    "\n",
    "- the\n",
    "- a\n",
    "- to\n",
    "\n",
    "NLP libraries usually include a corpus of stop words.\n",
    "\n",
    "Stop word lists:\n",
    "- http://www.nltk.org/book/ch02.html#stopwords_index_term\n",
    "- https://www.semantikoz.com/blog/free-stop-word-lists-in-23-languages/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with spaCy\n",
    "\n",
    "`spacy.lang.en.stop_words`\n",
    "\n",
    "`token.is_stop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost',\n",
       " 'alone',\n",
       " 'along',\n",
       " 'already',\n",
       " 'also',\n",
       " 'although',\n",
       " 'always',\n",
       " 'am',\n",
       " 'among',\n",
       " 'amongst',\n",
       " 'amount',\n",
       " 'an',\n",
       " 'and',\n",
       " 'another',\n",
       " 'any',\n",
       " 'anyhow',\n",
       " 'anyone',\n",
       " 'anything',\n",
       " 'anyway',\n",
       " 'anywhere',\n",
       " 'are',\n",
       " 'around',\n",
       " 'as',\n",
       " 'at',\n",
       " 'back',\n",
       " 'be',\n",
       " 'became',\n",
       " 'because',\n",
       " 'become',\n",
       " 'becomes',\n",
       " 'becoming',\n",
       " 'been',\n",
       " 'before',\n",
       " 'beforehand',\n",
       " 'behind',\n",
       " 'being',\n",
       " 'below',\n",
       " 'beside',\n",
       " 'besides',\n",
       " 'between',\n",
       " 'beyond',\n",
       " 'both',\n",
       " 'bottom',\n",
       " 'but',\n",
       " 'by',\n",
       " 'ca',\n",
       " 'call',\n",
       " 'can',\n",
       " 'cannot',\n",
       " 'could',\n",
       " 'did',\n",
       " 'do',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'done',\n",
       " 'down',\n",
       " 'due',\n",
       " 'during',\n",
       " 'each',\n",
       " 'eight',\n",
       " 'either',\n",
       " 'eleven',\n",
       " 'else',\n",
       " 'elsewhere',\n",
       " 'empty',\n",
       " 'enough',\n",
       " 'even',\n",
       " 'ever',\n",
       " 'every',\n",
       " 'everyone',\n",
       " 'everything',\n",
       " 'everywhere',\n",
       " 'except',\n",
       " 'few',\n",
       " 'fifteen',\n",
       " 'fifty',\n",
       " 'first',\n",
       " 'five',\n",
       " 'for',\n",
       " 'former',\n",
       " 'formerly',\n",
       " 'forty',\n",
       " 'four',\n",
       " 'from',\n",
       " 'front',\n",
       " 'full',\n",
       " 'further',\n",
       " 'get',\n",
       " 'give',\n",
       " 'go',\n",
       " 'had',\n",
       " 'has',\n",
       " 'have',\n",
       " 'he',\n",
       " 'hence',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hereafter',\n",
       " 'hereby',\n",
       " 'herein',\n",
       " 'hereupon',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'however',\n",
       " 'hundred',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'indeed',\n",
       " 'into',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'keep',\n",
       " 'last',\n",
       " 'latter',\n",
       " 'latterly',\n",
       " 'least',\n",
       " 'less',\n",
       " 'made',\n",
       " 'make',\n",
       " 'many',\n",
       " 'may',\n",
       " 'me',\n",
       " 'meanwhile',\n",
       " 'might',\n",
       " 'mine',\n",
       " 'more',\n",
       " 'moreover',\n",
       " 'most',\n",
       " 'mostly',\n",
       " 'move',\n",
       " 'much',\n",
       " 'must',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'name',\n",
       " 'namely',\n",
       " 'neither',\n",
       " 'never',\n",
       " 'nevertheless',\n",
       " 'next',\n",
       " 'nine',\n",
       " 'no',\n",
       " 'nobody',\n",
       " 'none',\n",
       " 'noone',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'nothing',\n",
       " 'now',\n",
       " 'nowhere',\n",
       " 'of',\n",
       " 'off',\n",
       " 'often',\n",
       " 'on',\n",
       " 'once',\n",
       " 'one',\n",
       " 'only',\n",
       " 'onto',\n",
       " 'or',\n",
       " 'other',\n",
       " 'others',\n",
       " 'otherwise',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 'part',\n",
       " 'per',\n",
       " 'perhaps',\n",
       " 'please',\n",
       " 'put',\n",
       " 'quite',\n",
       " 'rather',\n",
       " 're',\n",
       " 'really',\n",
       " 'regarding',\n",
       " 'same',\n",
       " 'say',\n",
       " 'see',\n",
       " 'seem',\n",
       " 'seemed',\n",
       " 'seeming',\n",
       " 'seems',\n",
       " 'serious',\n",
       " 'several',\n",
       " 'she',\n",
       " 'should',\n",
       " 'show',\n",
       " 'side',\n",
       " 'since',\n",
       " 'six',\n",
       " 'sixty',\n",
       " 'so',\n",
       " 'some',\n",
       " 'somehow',\n",
       " 'someone',\n",
       " 'something',\n",
       " 'sometime',\n",
       " 'sometimes',\n",
       " 'somewhere',\n",
       " 'still',\n",
       " 'such',\n",
       " 'take',\n",
       " 'ten',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'thence',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'therefore',\n",
       " 'therein',\n",
       " 'thereupon',\n",
       " 'these',\n",
       " 'they',\n",
       " 'third',\n",
       " 'this',\n",
       " 'those',\n",
       " 'though',\n",
       " 'three',\n",
       " 'through',\n",
       " 'throughout',\n",
       " 'thru',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'together',\n",
       " 'too',\n",
       " 'top',\n",
       " 'toward',\n",
       " 'towards',\n",
       " 'twelve',\n",
       " 'twenty',\n",
       " 'two',\n",
       " 'under',\n",
       " 'unless',\n",
       " 'until',\n",
       " 'up',\n",
       " 'upon',\n",
       " 'us',\n",
       " 'used',\n",
       " 'using',\n",
       " 'various',\n",
       " 'very',\n",
       " 'via',\n",
       " 'was',\n",
       " 'we',\n",
       " 'well',\n",
       " 'were',\n",
       " 'what',\n",
       " 'whatever',\n",
       " 'when',\n",
       " 'whence',\n",
       " 'whenever',\n",
       " 'where',\n",
       " 'whereafter',\n",
       " 'whereas',\n",
       " 'whereby',\n",
       " 'wherein',\n",
       " 'whereupon',\n",
       " 'wherever',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'while',\n",
       " 'whither',\n",
       " 'who',\n",
       " 'whoever',\n",
       " 'whole',\n",
       " 'whom',\n",
       " 'whose',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'within',\n",
       " 'without',\n",
       " 'would',\n",
       " 'yet',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ab',\n",
       " 'aber',\n",
       " 'ach',\n",
       " 'acht',\n",
       " 'achte',\n",
       " 'achten',\n",
       " 'achter',\n",
       " 'achtes',\n",
       " 'ag',\n",
       " 'alle',\n",
       " 'allein',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'allerdings',\n",
       " 'alles',\n",
       " 'allgemeinen',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'andere',\n",
       " 'anderen',\n",
       " 'andern',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'ausser',\n",
       " 'ausserdem',\n",
       " 'außer',\n",
       " 'außerdem',\n",
       " 'bald',\n",
       " 'bei',\n",
       " 'beide',\n",
       " 'beiden',\n",
       " 'beim',\n",
       " 'beispiel',\n",
       " 'bekannt',\n",
       " 'bereits',\n",
       " 'besonders',\n",
       " 'besser',\n",
       " 'besten',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bisher',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'dabei',\n",
       " 'dadurch',\n",
       " 'dafür',\n",
       " 'dagegen',\n",
       " 'daher',\n",
       " 'dahin',\n",
       " 'dahinter',\n",
       " 'damals',\n",
       " 'damit',\n",
       " 'danach',\n",
       " 'daneben',\n",
       " 'dank',\n",
       " 'dann',\n",
       " 'daran',\n",
       " 'darauf',\n",
       " 'daraus',\n",
       " 'darf',\n",
       " 'darfst',\n",
       " 'darin',\n",
       " 'darum',\n",
       " 'darunter',\n",
       " 'darüber',\n",
       " 'das',\n",
       " 'dasein',\n",
       " 'daselbst',\n",
       " 'dass',\n",
       " 'dasselbe',\n",
       " 'davon',\n",
       " 'davor',\n",
       " 'dazu',\n",
       " 'dazwischen',\n",
       " 'daß',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deiner',\n",
       " 'dem',\n",
       " 'dementsprechend',\n",
       " 'demgegenüber',\n",
       " 'demgemäss',\n",
       " 'demgemäß',\n",
       " 'demselben',\n",
       " 'demzufolge',\n",
       " 'den',\n",
       " 'denen',\n",
       " 'denn',\n",
       " 'denselben',\n",
       " 'der',\n",
       " 'deren',\n",
       " 'derjenige',\n",
       " 'derjenigen',\n",
       " 'dermassen',\n",
       " 'dermaßen',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'des',\n",
       " 'deshalb',\n",
       " 'desselben',\n",
       " 'dessen',\n",
       " 'deswegen',\n",
       " 'dich',\n",
       " 'die',\n",
       " 'diejenige',\n",
       " 'diejenigen',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'dir',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'drei',\n",
       " 'drin',\n",
       " 'dritte',\n",
       " 'dritten',\n",
       " 'dritter',\n",
       " 'drittes',\n",
       " 'du',\n",
       " 'durch',\n",
       " 'durchaus',\n",
       " 'durfte',\n",
       " 'durften',\n",
       " 'dürfen',\n",
       " 'dürft',\n",
       " 'eben',\n",
       " 'ebenso',\n",
       " 'ehrlich',\n",
       " 'eigen',\n",
       " 'eigene',\n",
       " 'eigenen',\n",
       " 'eigener',\n",
       " 'eigenes',\n",
       " 'ein',\n",
       " 'einander',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einigeeinigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'einmaleins',\n",
       " 'elf',\n",
       " 'en',\n",
       " 'ende',\n",
       " 'endlich',\n",
       " 'entweder',\n",
       " 'er',\n",
       " 'erst',\n",
       " 'erste',\n",
       " 'ersten',\n",
       " 'erster',\n",
       " 'erstes',\n",
       " 'es',\n",
       " 'etwa',\n",
       " 'etwas',\n",
       " 'euch',\n",
       " 'früher',\n",
       " 'fünf',\n",
       " 'fünfte',\n",
       " 'fünften',\n",
       " 'fünfter',\n",
       " 'fünftes',\n",
       " 'für',\n",
       " 'gab',\n",
       " 'ganz',\n",
       " 'ganze',\n",
       " 'ganzen',\n",
       " 'ganzer',\n",
       " 'ganzes',\n",
       " 'gar',\n",
       " 'gedurft',\n",
       " 'gegen',\n",
       " 'gegenüber',\n",
       " 'gehabt',\n",
       " 'gehen',\n",
       " 'geht',\n",
       " 'gekannt',\n",
       " 'gekonnt',\n",
       " 'gemacht',\n",
       " 'gemocht',\n",
       " 'gemusst',\n",
       " 'genug',\n",
       " 'gerade',\n",
       " 'gern',\n",
       " 'gesagt',\n",
       " 'geschweige',\n",
       " 'gewesen',\n",
       " 'gewollt',\n",
       " 'geworden',\n",
       " 'gibt',\n",
       " 'ging',\n",
       " 'gleich',\n",
       " 'gott',\n",
       " 'gross',\n",
       " 'grosse',\n",
       " 'grossen',\n",
       " 'grosser',\n",
       " 'grosses',\n",
       " 'groß',\n",
       " 'große',\n",
       " 'großen',\n",
       " 'großer',\n",
       " 'großes',\n",
       " 'gut',\n",
       " 'gute',\n",
       " 'guter',\n",
       " 'gutes',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'habt',\n",
       " 'hast',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'heisst',\n",
       " 'heißt',\n",
       " 'her',\n",
       " 'heute',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'hoch',\n",
       " 'hätte',\n",
       " 'hätten',\n",
       " 'ich',\n",
       " 'ihm',\n",
       " 'ihn',\n",
       " 'ihnen',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'im',\n",
       " 'immer',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'infolgedessen',\n",
       " 'ins',\n",
       " 'irgend',\n",
       " 'ist',\n",
       " 'ja',\n",
       " 'jahr',\n",
       " 'jahre',\n",
       " 'jahren',\n",
       " 'je',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedermann',\n",
       " 'jedermanns',\n",
       " 'jedoch',\n",
       " 'jemand',\n",
       " 'jemandem',\n",
       " 'jemanden',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kam',\n",
       " 'kann',\n",
       " 'kannst',\n",
       " 'kaum',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'kleine',\n",
       " 'kleinen',\n",
       " 'kleiner',\n",
       " 'kleines',\n",
       " 'kommen',\n",
       " 'kommt',\n",
       " 'konnte',\n",
       " 'konnten',\n",
       " 'kurz',\n",
       " 'können',\n",
       " 'könnt',\n",
       " 'könnte',\n",
       " 'lang',\n",
       " 'lange',\n",
       " 'leicht',\n",
       " 'leider',\n",
       " 'lieber',\n",
       " 'los',\n",
       " 'machen',\n",
       " 'macht',\n",
       " 'machte',\n",
       " 'mag',\n",
       " 'magst',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mehr',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mensch',\n",
       " 'menschen',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'mit',\n",
       " 'mittel',\n",
       " 'mochte',\n",
       " 'mochten',\n",
       " 'morgen',\n",
       " 'muss',\n",
       " 'musst',\n",
       " 'musste',\n",
       " 'mussten',\n",
       " 'muß',\n",
       " 'möchte',\n",
       " 'mögen',\n",
       " 'möglich',\n",
       " 'mögt',\n",
       " 'müssen',\n",
       " 'müsst',\n",
       " 'na',\n",
       " 'nach',\n",
       " 'nachdem',\n",
       " 'nahm',\n",
       " 'natürlich',\n",
       " 'neben',\n",
       " 'nein',\n",
       " 'neue',\n",
       " 'neuen',\n",
       " 'neun',\n",
       " 'neunte',\n",
       " 'neunten',\n",
       " 'neunter',\n",
       " 'neuntes',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'nie',\n",
       " 'niemand',\n",
       " 'niemandem',\n",
       " 'niemanden',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oben',\n",
       " 'oder',\n",
       " 'offen',\n",
       " 'oft',\n",
       " 'ohne',\n",
       " 'recht',\n",
       " 'rechte',\n",
       " 'rechten',\n",
       " 'rechter',\n",
       " 'rechtes',\n",
       " 'richtig',\n",
       " 'rund',\n",
       " 'sagt',\n",
       " 'sagte',\n",
       " 'sah',\n",
       " 'satt',\n",
       " 'schlecht',\n",
       " 'schon',\n",
       " 'sechs',\n",
       " 'sechste',\n",
       " 'sechsten',\n",
       " 'sechster',\n",
       " 'sechstes',\n",
       " 'sehr',\n",
       " 'sei',\n",
       " 'seid',\n",
       " 'seien',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'seit',\n",
       " 'seitdem',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'sieben',\n",
       " 'siebente',\n",
       " 'siebenten',\n",
       " 'siebenter',\n",
       " 'siebentes',\n",
       " 'siebte',\n",
       " 'siebten',\n",
       " 'siebter',\n",
       " 'siebtes',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solang',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollen',\n",
       " 'sollte',\n",
       " 'sollten',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'sowie',\n",
       " 'später',\n",
       " 'statt',\n",
       " 'tag',\n",
       " 'tage',\n",
       " 'tagen',\n",
       " 'tat',\n",
       " 'teil',\n",
       " 'tel',\n",
       " 'trotzdem',\n",
       " 'tun',\n",
       " 'uhr',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unser',\n",
       " 'unsere',\n",
       " 'unserer',\n",
       " 'unter',\n",
       " 'vergangene',\n",
       " 'vergangenen',\n",
       " 'viel',\n",
       " 'viele',\n",
       " 'vielem',\n",
       " 'vielen',\n",
       " 'vielleicht',\n",
       " 'vier',\n",
       " 'vierte',\n",
       " 'vierten',\n",
       " 'vierter',\n",
       " 'viertes',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'wahr',\n",
       " 'wann',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'wart',\n",
       " 'warum',\n",
       " 'was',\n",
       " 'wegen',\n",
       " 'weil',\n",
       " 'weit',\n",
       " 'weiter',\n",
       " 'weitere',\n",
       " 'weiteren',\n",
       " 'weiteres',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wem',\n",
       " 'wen',\n",
       " 'wenig',\n",
       " 'wenige',\n",
       " 'weniger',\n",
       " 'weniges',\n",
       " 'wenigstens',\n",
       " 'wenn',\n",
       " 'wer',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'werdet',\n",
       " 'wessen',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'willst',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirklich',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wohl',\n",
       " 'wollen',\n",
       " 'wollt',\n",
       " 'wollte',\n",
       " 'wollten',\n",
       " 'worden',\n",
       " 'wurde',\n",
       " 'wurden',\n",
       " 'während',\n",
       " 'währenddem',\n",
       " 'währenddessen',\n",
       " 'wäre',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zehn',\n",
       " 'zehnte',\n",
       " 'zehnten',\n",
       " 'zehnter',\n",
       " 'zehntes',\n",
       " 'zeit',\n",
       " 'zu',\n",
       " 'zuerst',\n",
       " 'zugleich',\n",
       " 'zum',\n",
       " 'zunächst',\n",
       " 'zur',\n",
       " 'zurück',\n",
       " 'zusammen',\n",
       " 'zwanzig',\n",
       " 'zwar',\n",
       " 'zwei',\n",
       " 'zweite',\n",
       " 'zweiten',\n",
       " 'zweiter',\n",
       " 'zweites',\n",
       " 'zwischen',\n",
       " 'á',\n",
       " 'über',\n",
       " 'überhaupt',\n",
       " 'übrigens'}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Deutsch\n",
    "from spacy.lang.de.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What False\n",
      "do True\n",
      "Singaporean False\n",
      "millennials False\n",
      "do True\n",
      "when True\n",
      "they True\n",
      "’re False\n",
      "travelling False\n",
      "overseas False\n",
      "? False\n",
      "Apparently False\n",
      ", False\n",
      "they True\n",
      "just True\n",
      "eat False\n",
      ", False\n",
      "eat False\n",
      ", False\n",
      "eat False\n",
      "– False\n",
      "and True\n",
      "take True\n",
      "a True\n",
      "lot False\n",
      "of True\n",
      "food False\n",
      "photos False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "# https://www.channelnewsasia.com/news/lifestyle/singapore-millennials-food-destinations-eating-holiday-tokyo-10458484\n",
    "text4 = u\"What do Singaporean millennials do when they’re travelling overseas? \" \\\n",
    "    \"Apparently, they just eat, eat, eat – and take a lot of food photos.\"\n",
    "\n",
    "doc = nlp(text4)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry False\n",
      "I False\n",
      "'m False\n",
      "not True\n",
      "free False\n",
      "tonite False\n",
      ", False\n",
      "I False\n",
      "have True\n",
      "MLDDS True\n",
      "( False\n",
      "lowercase False\n",
      ": False\n",
      "mldds False\n",
      ") False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "# Adding stop words\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "\n",
    "STOP_WORDS.add('MLDDS')\n",
    "\n",
    "doc = nlp(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Stop words with NLTK\n",
    "\n",
    "```\n",
    "nltk.corpus.stopwords\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download corpus\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['aber',\n",
       " 'alle',\n",
       " 'allem',\n",
       " 'allen',\n",
       " 'aller',\n",
       " 'alles',\n",
       " 'als',\n",
       " 'also',\n",
       " 'am',\n",
       " 'an',\n",
       " 'ander',\n",
       " 'andere',\n",
       " 'anderem',\n",
       " 'anderen',\n",
       " 'anderer',\n",
       " 'anderes',\n",
       " 'anderm',\n",
       " 'andern',\n",
       " 'anderr',\n",
       " 'anders',\n",
       " 'auch',\n",
       " 'auf',\n",
       " 'aus',\n",
       " 'bei',\n",
       " 'bin',\n",
       " 'bis',\n",
       " 'bist',\n",
       " 'da',\n",
       " 'damit',\n",
       " 'dann',\n",
       " 'der',\n",
       " 'den',\n",
       " 'des',\n",
       " 'dem',\n",
       " 'die',\n",
       " 'das',\n",
       " 'daß',\n",
       " 'derselbe',\n",
       " 'derselben',\n",
       " 'denselben',\n",
       " 'desselben',\n",
       " 'demselben',\n",
       " 'dieselbe',\n",
       " 'dieselben',\n",
       " 'dasselbe',\n",
       " 'dazu',\n",
       " 'dein',\n",
       " 'deine',\n",
       " 'deinem',\n",
       " 'deinen',\n",
       " 'deiner',\n",
       " 'deines',\n",
       " 'denn',\n",
       " 'derer',\n",
       " 'dessen',\n",
       " 'dich',\n",
       " 'dir',\n",
       " 'du',\n",
       " 'dies',\n",
       " 'diese',\n",
       " 'diesem',\n",
       " 'diesen',\n",
       " 'dieser',\n",
       " 'dieses',\n",
       " 'doch',\n",
       " 'dort',\n",
       " 'durch',\n",
       " 'ein',\n",
       " 'eine',\n",
       " 'einem',\n",
       " 'einen',\n",
       " 'einer',\n",
       " 'eines',\n",
       " 'einig',\n",
       " 'einige',\n",
       " 'einigem',\n",
       " 'einigen',\n",
       " 'einiger',\n",
       " 'einiges',\n",
       " 'einmal',\n",
       " 'er',\n",
       " 'ihn',\n",
       " 'ihm',\n",
       " 'es',\n",
       " 'etwas',\n",
       " 'euer',\n",
       " 'eure',\n",
       " 'eurem',\n",
       " 'euren',\n",
       " 'eurer',\n",
       " 'eures',\n",
       " 'für',\n",
       " 'gegen',\n",
       " 'gewesen',\n",
       " 'hab',\n",
       " 'habe',\n",
       " 'haben',\n",
       " 'hat',\n",
       " 'hatte',\n",
       " 'hatten',\n",
       " 'hier',\n",
       " 'hin',\n",
       " 'hinter',\n",
       " 'ich',\n",
       " 'mich',\n",
       " 'mir',\n",
       " 'ihr',\n",
       " 'ihre',\n",
       " 'ihrem',\n",
       " 'ihren',\n",
       " 'ihrer',\n",
       " 'ihres',\n",
       " 'euch',\n",
       " 'im',\n",
       " 'in',\n",
       " 'indem',\n",
       " 'ins',\n",
       " 'ist',\n",
       " 'jede',\n",
       " 'jedem',\n",
       " 'jeden',\n",
       " 'jeder',\n",
       " 'jedes',\n",
       " 'jene',\n",
       " 'jenem',\n",
       " 'jenen',\n",
       " 'jener',\n",
       " 'jenes',\n",
       " 'jetzt',\n",
       " 'kann',\n",
       " 'kein',\n",
       " 'keine',\n",
       " 'keinem',\n",
       " 'keinen',\n",
       " 'keiner',\n",
       " 'keines',\n",
       " 'können',\n",
       " 'könnte',\n",
       " 'machen',\n",
       " 'man',\n",
       " 'manche',\n",
       " 'manchem',\n",
       " 'manchen',\n",
       " 'mancher',\n",
       " 'manches',\n",
       " 'mein',\n",
       " 'meine',\n",
       " 'meinem',\n",
       " 'meinen',\n",
       " 'meiner',\n",
       " 'meines',\n",
       " 'mit',\n",
       " 'muss',\n",
       " 'musste',\n",
       " 'nach',\n",
       " 'nicht',\n",
       " 'nichts',\n",
       " 'noch',\n",
       " 'nun',\n",
       " 'nur',\n",
       " 'ob',\n",
       " 'oder',\n",
       " 'ohne',\n",
       " 'sehr',\n",
       " 'sein',\n",
       " 'seine',\n",
       " 'seinem',\n",
       " 'seinen',\n",
       " 'seiner',\n",
       " 'seines',\n",
       " 'selbst',\n",
       " 'sich',\n",
       " 'sie',\n",
       " 'ihnen',\n",
       " 'sind',\n",
       " 'so',\n",
       " 'solche',\n",
       " 'solchem',\n",
       " 'solchen',\n",
       " 'solcher',\n",
       " 'solches',\n",
       " 'soll',\n",
       " 'sollte',\n",
       " 'sondern',\n",
       " 'sonst',\n",
       " 'über',\n",
       " 'um',\n",
       " 'und',\n",
       " 'uns',\n",
       " 'unsere',\n",
       " 'unserem',\n",
       " 'unseren',\n",
       " 'unser',\n",
       " 'unseres',\n",
       " 'unter',\n",
       " 'viel',\n",
       " 'vom',\n",
       " 'von',\n",
       " 'vor',\n",
       " 'während',\n",
       " 'war',\n",
       " 'waren',\n",
       " 'warst',\n",
       " 'was',\n",
       " 'weg',\n",
       " 'weil',\n",
       " 'weiter',\n",
       " 'welche',\n",
       " 'welchem',\n",
       " 'welchen',\n",
       " 'welcher',\n",
       " 'welches',\n",
       " 'wenn',\n",
       " 'werde',\n",
       " 'werden',\n",
       " 'wie',\n",
       " 'wieder',\n",
       " 'will',\n",
       " 'wir',\n",
       " 'wird',\n",
       " 'wirst',\n",
       " 'wo',\n",
       " 'wollen',\n",
       " 'wollte',\n",
       " 'würde',\n",
       " 'würden',\n",
       " 'zu',\n",
       " 'zum',\n",
       " 'zur',\n",
       " 'zwar',\n",
       " 'zwischen']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('german')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What False\n",
      "do True\n",
      "Singaporean False\n",
      "millennials False\n",
      "do True\n",
      "when True\n",
      "they True\n",
      "’ False\n",
      "re True\n",
      "travelling False\n",
      "overseas False\n",
      "? False\n",
      "Apparently False\n",
      ", False\n",
      "they True\n",
      "just True\n",
      "eat False\n",
      ", False\n",
      "eat False\n",
      ", False\n",
      "eat False\n",
      "– False\n",
      "and True\n",
      "take False\n",
      "a True\n",
      "lot False\n",
      "of True\n",
      "food False\n",
      "photos False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "# https://www.channelnewsasia.com/news/lifestyle/singapore-millennials-food-destinations-eating-holiday-tokyo-10458484\n",
    "text4 = u\"What do Singaporean millennials do when they’re travelling overseas? \" \\\n",
    "    \"Apparently, they just eat, eat, eat – and take a lot of food photos.\"\n",
    "\n",
    "tokens = nltk.word_tokenize(text4)\n",
    "\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorry False\n",
      "I False\n",
      "'m False\n",
      "not True\n",
      "free False\n",
      "tonite False\n",
      ", False\n",
      "I False\n",
      "have True\n",
      "MLDDS True\n",
      "( False\n",
      "lowercase False\n",
      ": False\n",
      "mldds False\n",
      ") False\n",
      ". False\n"
     ]
    }
   ],
   "source": [
    "# Adding stop words\n",
    "stops = stopwords.words('english')\n",
    "stops.append(\"MLDDS\")\n",
    "stops = set(stops)\n",
    "\n",
    "tokens = nltk.word_tokenize(u\"Sorry I'm not free tonite, I have MLDDS (lowercase: mldds).\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token, token in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Frequency Analysis\n",
    "\n",
    "Answers two questions:\n",
    "\n",
    "1. How often does a word appear in a document?\n",
    "\n",
    "2. How important is a word in a document?\n",
    "\n",
    "Measure: Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Term Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$\\frac{f_{t, d}}{\\sum_{t' \\in d} \\, f_{t',d}}$$\n",
    "\n",
    "$f_{t, d}$: count of term $t$ in document $d$\n",
    "\n",
    "https://en.wikipedia.org/wiki/Tf%E2%80%93idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Inverse Document Frequency\n",
    "\n",
    "Most common formula:\n",
    "\n",
    "$$log\\frac{N}{\\mid\\{d \\in D : t \\in d \\}\\mid}$$\n",
    "\n",
    "$N$: number of documents\n",
    "\n",
    "$\\mid\\{d \\in D : t \\in d \\}\\mid$: number of documents containing term $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## TD-IDF\n",
    "\n",
    "$$tfidf(t, d, D) = tf(t, d) * idf(t, D)$$\n",
    "\n",
    "|term|tf|idf|tf-idf|\n",
    "|--|--|--|--|--|\n",
    "|to|large|very small|closer to 0|\n",
    "|coffee|small|large|not closer to 0|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Computing TF-IDF\n",
    "\n",
    "#### Scikit-learn:\n",
    "\n",
    "```\n",
    "sklearn.feature_extraction.text.TfidfVectorizer\n",
    "```\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "\n",
    "\n",
    "#### NLTK\n",
    "Supports tf-idf, but less popular\n",
    "```\n",
    "nltk.text.TextCollection\n",
    "```\n",
    "\n",
    "http://www.nltk.org/api/nltk.html#nltk.text.TextCollection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = u\"This is a test.\\n\" \\\n",
    "    u\"The quick brown fox jumps over the lazy dog.\\n\" \\\n",
    "    u\"The early bird gets the worm.\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "ename": "NotFittedError",
     "evalue": "TfidfVectorizer - Vocabulary wasn't fitted.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-3b74e9639f02>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtfidf_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mfeature_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# convert to dense matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\mldds03\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mget_feature_names\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    958\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    959\u001b[0m         \u001b[1;34m\"\"\"Array mapping from feature integer indices to feature name\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 960\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_vocabulary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    961\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    962\u001b[0m         return [t for t, i in sorted(six.iteritems(self.vocabulary_),\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\mldds03\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    301\u001b[0m         \u001b[1;34m\"\"\"Check if vocabulary is empty or missing (not fit-ed)\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"%(name)s - Vocabulary wasn't fitted.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 303\u001b[1;33m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vocabulary_'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    305\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\miniconda3\\envs\\mldds03\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_is_fitted\u001b[1;34m(estimator, attributes, msg, all_or_any)\u001b[0m\n\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mall_or_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattributes\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotFittedError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'name'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: TfidfVectorizer - Vocabulary wasn't fitted."
     ]
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/36800654/how-is-the-tfidfvectorizer-in-scikit-learn-supposed-to-work\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(text5)\n",
    "tokens = [token.text for token in doc]\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(tokens)\n",
    "\n",
    "# convert to dense matrix\n",
    "tfidf_dense = tfidf_matrix.todense()\n",
    "\n",
    "tfidf_dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Word Vectors\n",
    "\n",
    "https://spacy.io/usage/vectors-similarity\n",
    "\n",
    "https://github.com/charlieg/A-Smattering-of-NLP-in-Python\n",
    "\n",
    "Video: https://youtu.be/T8tQZChniMk\n",
    "\n",
    "http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Workshop: Creating Word2Vec Models\n",
    "\n",
    "Credits:\n",
    "- https://codesachin.wordpress.com/2015/10/09/generating-a-word2vec-model-from-a-block-of-text-using-gensim-python/\n",
    "- https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors\n",
    "\n",
    "Word2Vec\n",
    "- Semantic learning of text representations\n",
    "- Neural network \n",
    "- Cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Download text\n",
    "\n",
    "For demonstration purposes, we'll start with Wikipedia articles.\n",
    "\n",
    "We'll use a python library that wraps the Wikipedia APIs.\n",
    "\n",
    "https://pypi.org/project/wikipedia/\n",
    "\n",
    "Run this from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) pip install wikipedia\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia import search, page\n",
    "\n",
    "# Get our documents: wikipedia articles\n",
    "topic = 'singapore'\n",
    "\n",
    "titles = search(topic)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# retrieve all pages\n",
    "wikipages = [page(title) for title in titles]\n",
    "\n",
    "# inspect the first page\n",
    "wikipages[0].summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Process text\n",
    "\n",
    "- Split into sentences\n",
    "- Remove special characters\n",
    "- Convert to lowercase\n",
    "- Tokenize the text into words\n",
    "- Optionally remove stop words such as 'a', 'the'\n",
    "- Stem each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # python regular expressions library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK corpora\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "\n",
    "# 1. Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Download the Stop Words corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 3. Helper function to convert text\n",
    "def text_to_sentence_wordlists(text, remove_stopwords=True):\n",
    "    \"\"\"Cleans and converts text to a list of lists of tokens\n",
    "    Args:\n",
    "        text: input text\n",
    "        remove_stopwords: whether to remove stopwords\n",
    "    Returns: a tuple\n",
    "        A list of lists of tokens that looks like:\n",
    "           [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "        Total of words\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    # Reference: http://www.nltk.org/api/nltk.tokenize.html\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # set of stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    wordcount = 0\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        # Remove non-letters and numbers\n",
    "        sentence = re.sub('[^a-zA-Z0-9]', ' ', sentence)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Tokenize the sentence into words\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "        if remove_stopwords:\n",
    "            # Remove stop words\n",
    "            tokens = [token for token in tokens if not token in stops]\n",
    "    \n",
    "        # Stem the words\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        result += [tokens]\n",
    "        wordcount += len(tokens)\n",
    "    \n",
    "    return (result, wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Test our helper function to see what it does\n",
    "text = wikipages[0].summary\n",
    "print('===== Original text for first article =====')\n",
    "print(text)\n",
    "\n",
    "wordlist, count = text_to_sentence_wordlists(text,\n",
    "                                             remove_stopwords=False)\n",
    "print('\\n===== Stem words [%d words] =====' % count)\n",
    "print(wordlist)\n",
    "\n",
    "wordlist, count = text_to_sentence_wordlists(text)\n",
    "print('\\n===== Stem words - stopwords [%d words] =====' % count)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Convert all articles to sentence word lists\n",
    "\n",
    "Let's now convert all articles on our topic to sentence word lists.\n",
    "\n",
    "We were examining the summary for each article, let's see how we can get to the content.\n",
    "\n",
    "Looking at the wikipedia library's documentation, we can use `WikipediaPage.content` to get to the plain text content for each page: https://wikipedia.readthedocs.io/en/latest/code.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Converting %d articles to training set...' % len(titles))\n",
    "\n",
    "training_set = []\n",
    "training_set_size = 0\n",
    "\n",
    "for wikipage in wikipages:\n",
    "    wordlist, count = text_to_sentence_wordlists(wikipage.content)\n",
    "\n",
    "    training_set_size += count\n",
    "    training_set += wordlist\n",
    "    \n",
    "print('Training set size: %d stem words, %d sentences' \\\n",
    "      % (training_set_size, len(training_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Question to ponder:\n",
    "\n",
    "Should we randomize the training set?\n",
    "\n",
    "Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Train a word2vec model\n",
    "\n",
    "(Credits: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors)\n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced.\n",
    "\n",
    "For details on the algorithms below, see the [word2vec API documentation](https://radimrehurek.com/gensim/models/word2vec.html) as well as the [Google documentation](https://code.google.com/archive/p/word2vec/)(Performance section).\n",
    "\n",
    "### Domain characteristics\n",
    "\n",
    "Our training set is:\n",
    "- Small (under 25k words). Typically, word2vec training sets can go in hundreds of thousands.\n",
    "- Wikipedia articles about a common topic. We'll expect some words (e.g. singapore) to appear more frequently about that topic. Whether this is something we need to worry about is unclear.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Architecture:\n",
    "Architecture options are skip-gram (the default: slower, better for infrequent words) or continuous bag of words (fast). \n",
    "\n",
    "#### Training algorithm:\n",
    "This controls which algorithm to use.\n",
    "\n",
    "Hierarchical softmax (the default: better for infrequent words) or negative sampling (better for frequent words, better with low dimensional vectors). Start with the default first.\n",
    "\n",
    "#### Downsampling of frequent words:\n",
    "This controls the threshold for frequent words to be removed randomly. \n",
    "\n",
    "Randomly removing frequent words in large datasets can improve both accuracy and speed.\n",
    "\n",
    "$$p = \\frac{f-t}{f} - \\sqrt{\\frac{t}{f}}$$\n",
    "\n",
    "Where:\n",
    "- $p$: probabability that word is present\n",
    "- $f$: frequency of word in corpus\n",
    "- $t$: the threshold (our downsampling hyperparameter)\n",
    "\n",
    "A smaller $t$ means more words will be randomly removed.\n",
    "\n",
    "(Source: https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf)\n",
    "\n",
    "The [Google documentation](https://code.google.com/archive/p/word2vec/) recommends values between 1e-3 and 1e-5. Let's try 1e-3 and then iterate from there, since our training set is small.\n",
    "\n",
    "#### Word vector dimensionality:\n",
    "This controls how many features the word vector should have. Higher dimensionality (more features) usually result in better models, but also longer runtimes. Reasonable values can be in the tens to hundreds. We'll try 200.\n",
    "\n",
    "#### Context / window size:\n",
    "This defines the window-size to look for related words. For skip-gram usually around 10, for CBOW around 5. More is better, up to a point.\n",
    "\n",
    "### Worker threads:\n",
    "Number of parallel processes to run. This can significantly improve training speed.  \n",
    "\n",
    "The number to choose depends on how many logical CPU cores your computer has (on Windows, Start Menu -> System Information, look for Processors). \n",
    "\n",
    "Start with a number around 2-4, and then increase up if your computer is more powerful.\n",
    "\n",
    "### Minimum word count:\n",
    "This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. \n",
    "\n",
    "Reasonable values could be between 10 and 100. Higher values also help limit run time.\n",
    "\n",
    "For wikipedia articles, we'll try a minimum wordcount of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "word2vec.Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors\n",
    "\n",
    "# Set values for various parameters\n",
    "sg = 1                # Algorithm: 1: skip-gram, 0: CBOW\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 2       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Initialize and train the model.\n",
    "# This may take a while if your training set is large (e.g. 500,000 words)\n",
    "print('Training Word2Vec model...')\n",
    "%time model = word2vec.Word2Vec(training_set, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"wikipedia_{}features_{}minwords_{}context_{}downsampling.w2v\" \\\n",
    "    .format(num_features, min_word_count, context, str(downsampling))\n",
    "model.save(model_name)\n",
    "\n",
    "print('Saved model as %s' % model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Loading the saved model\n",
    "\n",
    "Here's how to load a previously saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"wikipedia_100features_50minwords_10context.w2v\"\n",
    "\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "The trained model contains a read-only `models.keyedvectors.Word2VecMeyedVectors` with methods for evaluating word relationships.\n",
    "\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "\n",
    "Here are some things to try with the word2vec model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Get the vocabulary of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in the vocab\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Check if a stem word is in the model's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('malaysia') in model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('korea') in model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Find a word that doesn't match in a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test = 'raffles indian chinese malay'\n",
    "\n",
    "# you can either use the helper function to convert to stem words\n",
    "# or call stemmer.stem() directly on each word\n",
    "wordlist, _ = text_to_sentence_wordlists(test)\n",
    "print('Input: %s' % wordlist[0])\n",
    "\n",
    "print(\"Word that doesn't match: %s\"\n",
    "      % model.wv.doesnt_match(wordlist[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top N most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = stemmer.stem('singapore')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = stemmer.stem('changi')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Measures the cosine distance and similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('changi')\n",
    "word2 = stemmer.stem('aircraft')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('changi')\n",
    "word2 = stemmer.stem('british')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Returns the word's representation in vector space as a 1D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word = stemmer.stem('malaysia')\n",
    "\n",
    "raw_vectors = model.wv.word_vec(word, use_norm=True)\n",
    "\n",
    "raw_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Visualizing Word2Vec\n",
    "\n",
    "Next, we'll plot the Word Vectors to see how the clusters look like:\n",
    "\n",
    "1. Use t-Distributed Stochastic Neighbor Embedding [TSNE](https://lvdmaaten.github.io/tsne/) to reduce the high-dimensional model into 2D\n",
    "2. Plot the 2D representation of the word2vec model, with the words in its vocabulary as the labels\n",
    "\n",
    "Credits: https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "# Apply t-SNE\n",
    "# this can take a while (like 1 minute or more)\n",
    "tsne = TSNE(n_components=2)\n",
    "%time X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe for the 2 dimensions,\n",
    "# indexed by the words in the vocab\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# create a zoomable interactive plot\n",
    "%matplotlib notebook\n",
    "\n",
    "# Plot the 2D representation of the word2vec model,\n",
    "# with the words in its vocabulary as the labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Create Corpus and Train Word2Vec\n",
    "\n",
    "In this exercise, we will create our own corpus and use it to train Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Create Corpus\n",
    "\n",
    "Create a corpus of text files, organized in a structure like this:\n",
    "\n",
    "```\n",
    "corpus/\n",
    "   text001.txt\n",
    "   text002.txt\n",
    "   text003.txt\n",
    "   ...\n",
    "```\n",
    "\n",
    "A sample corpus is included in the `corpus` folder, created with the first 3 chapters of Moby Dick:\n",
    "https://www.gutenberg.org/files/2701/2701-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Import corpus using NLTK\n",
    "\n",
    "We will use [`nltk.corpus.reader.plaintext`](http://www.nltk.org/howto/corpus.html) to import the corpus.\n",
    "\n",
    "Credits: https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# directory containing the corpus\n",
    "corpus_dir = 'corpus/'\n",
    "\n",
    "# PlaintextCorpusReader uses nltk.tokenize.sent_tokenize() and\n",
    "# nltk.tokenize.word_tokenize() to split texts into sentences and words\n",
    "newcorpus = PlaintextCorpusReader(corpus_dir,\n",
    "                                  '.*\\.txt',\n",
    "                                  encoding='latin1') # or 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files found by the reader\n",
    "newcorpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first file in the corpus\n",
    "f = newcorpus.open('text001.txt')\n",
    "print(f.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# sentences in the corpus:\n",
    "newcorpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# number of sentences\n",
    "len(newcorpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence_lists(sentence_lists, remove_stopwords=True):\n",
    "    \"\"\"Cleans and converts the sentence lists\n",
    "    Args:\n",
    "        text: sentence lists\n",
    "        remove_stopwords: whether to remove stopwords\n",
    "    Returns:\n",
    "        A tuple:\n",
    "            The cleaned sentence list\n",
    "            The token count\n",
    "    \"\"\"\n",
    "    # set of English stop words\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    result = []\n",
    "    wordcount = 0\n",
    "\n",
    "    for sentence in sentence_lists:\n",
    "        # Convert to lowercase\n",
    "        tokens = [t.lower() for t in sentence]\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stopwords:\n",
    "            tokens = [t for t in tokens if not t in stops]\n",
    "        \n",
    "        # Remove non-letters and numbers\n",
    "        tokens = [re.sub('[^a-zA-Z0-9]', '', t) for t in tokens]\n",
    "        \n",
    "        # Stem the words\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        result += [tokens]\n",
    "        wordcount += len(tokens)\n",
    "    \n",
    "    return (result, wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your Tasks:\n",
    "\n",
    "1. Convert newcorpus.sents() to sentence wordlists, using the `clean_sentence_lists` helper function\n",
    "2. Train a Word2Vec model, with initial hyperparameter settings (use your best guess)\n",
    "3. Try some word similarity queries\n",
    "4. Tweak your model by adjusting some hyperparameter settings\n",
    "5. Plot the completed Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert newcorpus.sents() to sentence wordlists, \n",
    "# using the clean_sentence_lists helper function\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "print('Converting %d sentences to training set...' % len(newcorpus.sents()))\n",
    "\n",
    "training_set, training_set_size = clean_sentence_lists(newcorpus.sents())\n",
    "\n",
    "print('Training set size: %d stem words, %d sentences' \\\n",
    "      % (training_set_size, len(training_set)))\n",
    "\n",
    "training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Train a Word2Vec model, with initial hyperparameter settings\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "# Set values for various parameters\n",
    "sg = 1                # Algorithm: 1: skip-gram, 0: CBOW\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 2       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Initialize and train the model.\n",
    "# This may take a while if your training set is large (e.g. 500,000 words)\n",
    "print('Training Word2Vec model...')\n",
    "%time model = word2vec.Word2Vec(training_set, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"corpus_{}features_{}minwords_{}context_{}downsampling.w2v\" \\\n",
    "    .format(num_features, min_word_count, context, str(downsampling))\n",
    "model.save(model_name)\n",
    "\n",
    "print('Saved model as %s' % model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Try some word similarity queries\n",
    "# Your code here\n",
    "\n",
    "print('Vocab length:', len(model.wv.vocab))\n",
    "print('Vocab:', model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('whale')\n",
    "word2 = stemmer.stem('harpoon')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('whale')\n",
    "word2 = stemmer.stem('landlord')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word = stemmer.stem('harpoon')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "# Apply t-SNE\n",
    "# this can take a while (like 1 minute or more)\n",
    "tsne = TSNE(n_components=2)\n",
    "%time X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "# Create a dataframe for the 2 dimensions,\n",
    "# indexed by the words in the vocab\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the completed Word2Vec model\n",
    "\n",
    "# create a zoomable interactive plot\n",
    "%matplotlib notebook\n",
    "\n",
    "# Plot the 2D representation of the word2vec model,\n",
    "# with the words in its vocabulary as the labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
