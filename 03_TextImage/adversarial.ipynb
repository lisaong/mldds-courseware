{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversarial Learning\n",
    "\n",
    "![street fighter](assets/adversarial/street_fighter.jpg)\n",
    "\n",
    "(image: https://www.flickr.com/photos/sbfisher/856125311, [Creative Commons License](https://creativecommons.org/licenses/by-nc-sa/2.0/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Adversarial examples\n",
    "- Generative Adversarial Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Adversarial Examples\n",
    "\n",
    "Data inputs that fool neural networks, but not people\n",
    "\n",
    "![example](assets/adversarial/adversarial_example.png)\n",
    "\n",
    "(image: [Deep Learning](http://www.deeplearningbook.org/contents/regularization.html), Ian Goodfellow and Yoshua Bengio and Aaron Courville)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough - Adversarial Examples\n",
    "\n",
    "In this walkthrough, we will see how a Neural Network handles adversarial examples.\n",
    "\n",
    "1. Get predictions for an image\n",
    "2. Convert image to an adversarial example\n",
    "3. Re-evaluate the adversarial example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Setup - Install Foolbox\n",
    "\n",
    "Foolbox is a Python toolbox to create adversarial examples that fool neural networks.\n",
    "\n",
    "https://github.com/bethgelab/foolbox\n",
    "\n",
    "```\n",
    "pip install foolbox\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_and_crop_image(image_path, width, height):\n",
    "    \"\"\"Resizes and crops an image to the desired size\n",
    "    Args:\n",
    "        image_path: path to the image\n",
    "        width: image width\n",
    "        height: image height\n",
    "    Returns:\n",
    "        the resulting image\n",
    "    \"\"\"\n",
    "    from PIL import Image, ImageOps\n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = ImageOps.fit(img, (width, height))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Get predictions for an image\n",
    "from keras.applications import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "model = ResNet50()\n",
    "width = height = 224\n",
    "\n",
    "image_path = './assets/adversarial/mrt.jpg'\n",
    "\n",
    "img = resize_and_crop_image(image_path, width, height)\n",
    "plt.imshow(img)\n",
    "\n",
    "x = img_to_array(img)\n",
    "x = preprocess_input(x)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "y = model.predict(x)\n",
    "preds = decode_predictions(y, top=1)\n",
    "plt.title('Original: %s' % preds)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.backend import set_learning_phase\n",
    "from PIL import Image\n",
    "import foolbox\n",
    "\n",
    "# labels from Keras\n",
    "# https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.json\n",
    "label = 829 # \"829\": [\"n04335435\", \"streetcar\"]\n",
    "\n",
    "# Example from: https://github.com/bethgelab/foolbox\n",
    "set_learning_phase(0) # not training\n",
    "\n",
    "# Element-wise preprocessing of input\n",
    "#   first subtracts the first element of preprocessing from the input\n",
    "#   and then divide the input by the second element.\n",
    "# https://foolbox.readthedocs.io/en/latest/modules/models.html#foolbox.models.KerasModel\n",
    "preprocessing = (np.array([104, 116, 123]), 1)\n",
    "fmodel = foolbox.models.KerasModel(ResNet50(), bounds=(0, 255),\n",
    "                                   preprocessing=preprocessing)\n",
    "\n",
    "# Apply attack on source image to target a different label\n",
    "attack = foolbox.attacks.FGSM(model=fmodel)\n",
    "\n",
    "img = resize_and_crop_image(image_path, width, height)\n",
    "x = np.asarray(img, dtype=np.float32)\n",
    "x = x[:, :, :3]\n",
    "\n",
    "# ::-1 to convert BGR to RGB\n",
    "adversarial = attack(x[:, :, ::-1], label)\n",
    "\n",
    "# ::-1 to convert BGR to RGB\n",
    "# division by 255 to convert [0, 255] to [0, 1]\n",
    "plt.imshow(adversarial[:, :, ::-1] / 255)\n",
    "\n",
    "x = preprocess_input(adversarial)\n",
    "y = model.predict(np.expand_dims(x, axis=0))\n",
    "preds = decode_predictions(y, top=1)\n",
    "\n",
    "plt.title('Adversarial: %s' % preds)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Optional Exercises\n",
    "\n",
    "The Foolbox tool kit has a few other exploits available.  These are useful if we want to create adversarial inputs to augment our training data (https://github.com/bethgelab/foolbox/issues/81)\n",
    "\n",
    "1. Try other attacks available in Foolbox, such as LBFGSAttack, which tries to fake a target class.\n",
    " ```\n",
    " criterion = foolbox.criteria.TargetClass(22)\n",
    " attack    = foolbox.attacks.LBFGSAttack(fmodel, criterion)\n",
    " ```\n",
    "   - https://foolbox.readthedocs.io/en/latest/modules/attacks.html\n",
    "\n",
    "\n",
    "2. Try other image classes as practice. For a given text label, you can find the integer label by download this file:\n",
    " - https://s3.amazonaws.com/deep-learning-models/image-models/imagenet_class_index.jso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generative Adversarial Networks (GANs)\n",
    "\n",
    "- Train two networks against each other\n",
    "- Generator: generates fake images to fool Discriminator\n",
    "  - returns samples\n",
    "- Discriminator: tries to distinguish real images from fake ones\n",
    "  - returns probability that sample is real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generator\n",
    "\n",
    "![generator](assets/adversarial/generator.png)\n",
    "\n",
    "(image: https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Discriminator\n",
    "![discriminator](assets/adversarial/discriminator.png)\n",
    "\n",
    "(image: https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Objective\n",
    "\n",
    "Training is done (converged) when:\n",
    "- Generator's fake samples are indistinguishable from real samples\n",
    "- Discriminator always returns $\\frac{1}{2}$\n",
    "\n",
    "Discard Discriminator and keep the Generator as the finished model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Applications\n",
    "\n",
    "Fake celebrities: https://www.theverge.com/2017/10/30/16569402/ai-generate-fake-faces-celebs-nvidia-gan\n",
    "\n",
    "Image synthesis: https://arxiv.org/abs/1803.04469\n",
    "\n",
    "Text to Speech: https://github.com/r9y9/gantts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementations\n",
    "\n",
    "https://github.com/eriklindernoren/Keras-GAN\n",
    "\n",
    "https://github.com/eriklindernoren/PyTorch-GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Training a DCGAN\n",
    "\n",
    "Deep Convolutional GAN: https://arxiv.org/abs/1511.06434\n",
    "\n",
    "Credits: https://medium.com/@mattiaspinelli/simple-generative-adversarial-network-gans-with-keras-1fe578e44a87"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Dataset: MNIST\n",
    "\n",
    "Training GANs is tricky, so we will try to reproduce it with a well-known dataset (MNIST).\n",
    "\n",
    "Input: 28x28 pixel, black and white images of handwritten digits\n",
    "\n",
    "Ouptut: 10 labels (0 to 9)\n",
    "\n",
    "http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "width = height = 28\n",
    "channels = 1\n",
    "shape = (width, height, channels)\n",
    "\n",
    "(X_train, _), (_, _) = mnist.load_data()\n",
    "\n",
    "# Rescale -1 to 1\n",
    "X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "X_train = np.expand_dims(X_train, axis=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Create models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def generator():\n",
    "    \"\"\"Defines a Generator model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256, input_shape=(100,)))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(512))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(1024))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization(momentum=0.8))\n",
    "    model.add(Dense(height * height * channels, activation='tanh'))\n",
    "    model.add(Reshape((width, height, channels)))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def discriminator():\n",
    "    \"\"\"Defines a Discriminator model\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=shape))\n",
    "    model.add(Dense((width * height * channels), input_shape=shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense((width * height * channels)//2))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Adversarial Model\n",
    "\n",
    "The adversarial model is created by chaining the generator with the discriminator.\n",
    "\n",
    "1. Input goes into the Generator, which tries to make it fake\n",
    "2. The output from the Generator will be fed into the Discriminator, which tries to discriminate the fake images from real ones.\n",
    "\n",
    "![adversarial model](assets/adversarial/adversarial_model.png)\n",
    "\n",
    "(image: https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Create Adversarial Model\n",
    "\n",
    "Create our stacked adversarial model as shown in the picture above.\n",
    "\n",
    "Steps\n",
    "1. Create and compile the generator with `binary_crossentropy` loss and `Adam(lr=0.0002, decay=8e-9)` optimizer\n",
    "\n",
    "2. Create and compile the discriminator with `binary_crossentropy` loss and `Adam(lr=0.0002, decay=8e-9)` optimizer\n",
    "\n",
    "3. Chain the two into a `Sequential()` adversarial model, and compile it.\n",
    "  - For the adversarial model, the discriminator's weights should be frozen.\n",
    "\n",
    "You can refer to https://medium.com/@mattiaspinelli/simple-generative-adversarial-network-gans-with-keras-1fe578e44a87 if you are stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Train the GAN\n",
    " \n",
    "![training](assets/adversarial/training.png)\n",
    "\n",
    "(image: https://towardsdatascience.com/gan-by-example-using-keras-on-tensorflow-backend-1a6d515a60d0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def plot_images(samples=16, step=0):\n",
    "    \"\"\"Plots the generated images at the given step\n",
    "    Args:\n",
    "        samples: number of images to generate\n",
    "        step: step count\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    noise = np.random.normal(0, 1, (samples,100))\n",
    "    images = gen.predict(noise)\n",
    "        \n",
    "    plt.figure(figsize=(10,10))\n",
    "    \n",
    "    for i in range(images.shape[0]):\n",
    "        plt.subplot(4, 4, i+1)\n",
    "        image = images[i, :, :, :]\n",
    "        image = np.reshape(image, [height, width])\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Training Setup: Sanity Check\n",
    "\n",
    "To make sure our training code works, we'll do a sanity check with very few epochs and a tiny batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 20 # small number for workshop purposes, typically 20000\n",
    "batch = 4 # small number for workshop purposes, typically 32\n",
    "plot_interval = 200\n",
    "\n",
    "for cnt in range(epochs):\n",
    "\n",
    "    # Get real images\n",
    "    random_index =  np.random.randint(0, len(X_train) - batch//2)\n",
    "    legit_images = X_train[random_index : random_index + batch//2].reshape(batch//2, width, height, channels)\n",
    "\n",
    "    # Have the generator predict fake images\n",
    "    print('epoch: %d, [Generating images, batch size: %d]' % (cnt, batch))\n",
    "    gen_noise = np.random.normal(0, 1, (batch//2,100))\n",
    "    synthetic_images = gen.predict(gen_noise)\n",
    "    x_combined_batch = np.concatenate((legit_images, synthetic_images))\n",
    "    y_combined_batch = np.concatenate((np.ones((batch//2, 1)), np.zeros((batch//2, 1))))\n",
    "\n",
    "    # Train the discriminator with the fake images and the real images\n",
    "    # perform 1 gradient update on this batch\n",
    "    print('epoch: %d, [Training Discriminator, batch size: %d]' % (cnt, batch))\n",
    "    dis_loss = dis.train_on_batch(x_combined_batch, y_combined_batch)\n",
    "\n",
    "    # Train the generator (which is embedded in the Adversarial network)\n",
    "    # For the Adversarial network, the discriminator weights are frozen.\n",
    "    noise = np.random.normal(0, 1, (batch,100))\n",
    "    y_mislabeled = np.ones((batch, 1))\n",
    "    \n",
    "    # perform 1 gradient update on this batch\n",
    "    print('epoch: %d, [Training Generator, batch size: %d]' % (cnt, len(x_combined_batch)))\n",
    "    gan_loss = gan.train_on_batch(noise, y_mislabeled)\n",
    "    print('epoch: %d, [Discriminator loss: %.3f], [ Generator loss: %.3f]' % (cnt, dis_loss[0], gan_loss))\n",
    "\n",
    "    # show progress\n",
    "    if cnt % plot_interval == 0 : \n",
    "        plot_images(step=cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Train model\n",
    "\n",
    "Now that we've run through a quick sanity check, try setting epochs and batch to larger values\n",
    "\n",
    "```\n",
    "epochs = 20 # small number for workshop purposes, typically 20000\n",
    "batch = 4 # small number for workshop purposes, typically 32\n",
    "\n",
    "```\n",
    "\n",
    "Warning: training will be slow on CPU-only machines. You can try gradually bumping up the epochs / batch values.\n",
    "\n",
    "Another option is to look into running this on a GPU machine, using a service such as: https://neptune.ml/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "|Material|Read it for|URL|\n",
    "|--|--|--|\n",
    "|Section 7.13 Adversarial Training (Pages 265-266)|How to improve network robustness with adversarial examples|http://www.deeplearningbook.org/contents/regularization.html|\n",
    "|Section 7.13 Generative Adversarial Networks (Pages 696-699|Introduction to GANs (motivation, challenges), written by the inventor of GANs|http://www.deeplearningbook.org/contents/generative_models.html|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
