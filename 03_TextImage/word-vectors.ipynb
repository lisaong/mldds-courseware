{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Why Word Vectors exist\n",
    "- How to use them\n",
    "- How to train them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Setup\n",
    "\n",
    "Run this command from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) conda install gensim cython nltk\n",
    "```\n",
    "\n",
    "### gensim: for training word2vec\n",
    "\n",
    "https://radimrehurek.com/gensim/\n",
    "\n",
    "\n",
    "### Cython: to speed up training word2vec\n",
    "http://docs.cython.org/en/latest/src/quickstart/install.html\n",
    "\n",
    "\n",
    "### NLTK: NLP toolkit\n",
    "Installation: https://www.nltk.org/install.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why do we need Word Vectors\n",
    "\n",
    "To represent word meanings in an efficient way\n",
    "\n",
    "To express word meaning based on context\n",
    " - Context: window of words around this word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Before Word Vectors: Synsets\n",
    "\n",
    "- Synsets: Lists of synonyms for a word\n",
    "  - Someone needs to curate the list (create, update, delete)\n",
    "    - Language-specific\n",
    "  - Does not adapt as language evolves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Uses of Synsets\n",
    "- Synsets are still useful in NLP\n",
    " - for tasks where word meaning is relatively static (e.g. dated literature)\n",
    " - as a baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Walkthrough - WordNet\n",
    "\n",
    "WordNet is a well-known database that contains sets of synonyms of English words.\n",
    "\n",
    "In this walkthrough, we will use NLTK to query WordNet.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Click on http://wordnetweb.princeton.edu/perl/webwn\n",
    "2. Enter a word to search for (has to be single word)\n",
    "\n",
    "For example, when searching for the word \"machine\":\n",
    "\n",
    "![wordnet](assets/word-vectors/wordnet-search.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Besides the browser, you can use NLTK to query WordNet.\n",
    "\n",
    "Examples:\n",
    "http://www.nltk.org/howto/wordnet.html\n",
    "\n",
    "API:\n",
    "http://www.nltk.org/_modules/nltk/corpus/reader/wordnet.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('machine.n.01'),\n",
       " Synset('machine.n.02'),\n",
       " Synset('machine.n.03'),\n",
       " Synset('machine.n.04'),\n",
       " Synset('machine.n.05'),\n",
       " Synset('car.n.01'),\n",
       " Synset('machine.v.01'),\n",
       " Synset('machine.v.02')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "# query synsets for 'machine'\n",
    "wn.synsets('machine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Synsets are a set of synonyms that share a common meaning.\n",
    "\n",
    "Synset attributes, accessible via methods with the same name:\n",
    "\n",
    "- name: The canonical name of this synset, formed using the first lemma\n",
    "  of this synset. Note that this may be different from the name\n",
    "  passed to the constructor if that string used a different lemma to\n",
    "  identify the synset.\n",
    "- pos: The synset's part of speech, matching one of the module level\n",
    "  attributes ADJ, ADJ_SAT, ADV, NOUN or VERB.\n",
    "- lemmas: A list of the Lemma objects for this synset.\n",
    "- definition: The definition for this synset.\n",
    "- examples: A list of example strings for this synset.\n",
    "- offset: The offset in the WordNet dict file of this synset.\n",
    "- lexname: The name of the lexicographer file containing this synset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('device.n.01')]\n",
      "[Synset('person.n.01')]\n",
      "[Synset('organization.n.01')]\n",
      "[Synset('mechanical_device.n.01')]\n",
      "[Synset('organization.n.01')]\n",
      "[Synset('motor_vehicle.n.01')]\n",
      "[Synset('shape.v.02')]\n",
      "[Synset('produce.v.02')]\n"
     ]
    }
   ],
   "source": [
    "# Loop to print out what each synset represents\n",
    "# Each set is a different semantic meaning of the word 'machine'\n",
    "\n",
    "for s in wn.synsets('machine'):\n",
    "    print(s.hypernyms())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "robot vs. machine similarity: 0.25\n",
      "sunset vs. machine similarity: 0.058823529411764705\n"
     ]
    }
   ],
   "source": [
    "# check if another word is similar to 'machine', using it's 'device' meaning\n",
    "# larger number means more similarity\n",
    "\n",
    "robot_synset1 = wn.synsets('robot')[0]\n",
    "print('robot vs. machine similarity:', robot_synset1.path_similarity(machine_synset1))\n",
    "      \n",
    "sunset_synset1 = wn.synsets('sunset')[0]\n",
    "print('sunset vs. machine similarity:', sunset_synset1.path_similarity(machine_synset1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Word Vectors\n",
    "\n",
    "Instead of static lists of words, word vectors are trained from examples of text.\n",
    "\n",
    "The examples of text (text corpus) should be \"large enough\" to capture the possible meanings of the word.\n",
    "\n",
    "Anyone can train a word vector. Unlike SynSets, you don't need to be a linguist or expert in word meanings. You just need enough examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Pre-trained Word Vectors\n",
    "\n",
    "These are available, but are usually huge downloads (GB)\n",
    "- https://code.google.com/archive/p/word2vec/\n",
    "- https://github.com/Hironsan/awesome-embedding-models\n",
    "\n",
    "Instead, we will train our own Word Vectors. This is most flexible because you can adapt to your particular text corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Workshop: Creating Word2Vec Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Download training text corpus\n",
    "\n",
    "For demonstration purposes, we'll start with Wikipedia articles.\n",
    "\n",
    "We'll use a python library that wraps the Wikipedia APIs.\n",
    "\n",
    "https://pypi.org/project/wikipedia/\n",
    "\n",
    "Run this from an Anaconda prompt (within the mldds03 environment):\n",
    "\n",
    "```\n",
    "(mldds03) pip install wikipedia\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from wikipedia import search, page\n",
    "\n",
    "# Get our documents: wikipedia articles\n",
    "topic = 'singapore'\n",
    "\n",
    "titles = search(topic)\n",
    "titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# retrieve all pages\n",
    "wikipages = [page(title) for title in titles]\n",
    "\n",
    "# inspect the first page\n",
    "wikipages[0].summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Process text\n",
    "\n",
    "- Split into sentences\n",
    "- Remove special characters\n",
    "- Convert to lowercase\n",
    "- Tokenize the text into words\n",
    "- Optionally remove stop words such as 'a', 'the'\n",
    "- Stem each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # python regular expressions library\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download NLTK corpora\n",
    "# List of available corpora: http://www.nltk.org/book/ch02.html#tab-corpora\n",
    "\n",
    "# 1. Download the Punkt sentence tokenizer\n",
    "# https://www.nltk.org/_modules/nltk/tokenize/punkt.html\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 2. Download the Stop Words corpus\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 3. Helper function to convert text\n",
    "def text_to_sentence_wordlists(text, remove_stopwords=True):\n",
    "    \"\"\"Cleans and converts text to a list of lists of tokens\n",
    "    Args:\n",
    "        text: input text\n",
    "        remove_stopwords: whether to remove stopwords\n",
    "    Returns: a tuple\n",
    "        A list of lists of tokens that looks like:\n",
    "           [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "        Total of words\n",
    "    \"\"\"\n",
    "    # Split into sentences\n",
    "    # Reference: http://www.nltk.org/api/nltk.tokenize.html\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # set of stopwords\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    wordcount = 0\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        # Remove non-letters and numbers\n",
    "        sentence = re.sub('[^a-zA-Z0-9]', ' ', sentence)\n",
    "\n",
    "        # Convert to lowercase\n",
    "        sentence = sentence.lower()\n",
    "        \n",
    "        # Tokenize the sentence into words\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "    \n",
    "        if remove_stopwords:\n",
    "            # Remove stop words\n",
    "            tokens = [token for token in tokens if not token in stops]\n",
    "    \n",
    "        # Stem the words\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        result += [tokens]\n",
    "        wordcount += len(tokens)\n",
    "    \n",
    "    return (result, wordcount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Test our helper function to see what it does\n",
    "text = wikipages[0].summary\n",
    "print('===== Original text for first article =====')\n",
    "print(text)\n",
    "\n",
    "wordlist, count = text_to_sentence_wordlists(text,\n",
    "                                             remove_stopwords=False)\n",
    "print('\\n===== Stem words [%d words] =====' % count)\n",
    "print(wordlist)\n",
    "\n",
    "wordlist, count = text_to_sentence_wordlists(text)\n",
    "print('\\n===== Stem words - stopwords [%d words] =====' % count)\n",
    "print(wordlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Convert all articles to sentence word lists\n",
    "\n",
    "Let's now convert all articles on our topic to sentence word lists.\n",
    "\n",
    "We were examining the summary for each article, let's see how we can get to the content.\n",
    "\n",
    "Looking at the wikipedia library's documentation, we can use `WikipediaPage.content` to get to the plain text content for each page: https://wikipedia.readthedocs.io/en/latest/code.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikipages[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print('Converting %d articles to training set...' % len(titles))\n",
    "\n",
    "training_set = []\n",
    "training_set_size = 0\n",
    "\n",
    "for wikipage in wikipages:\n",
    "    wordlist, count = text_to_sentence_wordlists(wikipage.content)\n",
    "\n",
    "    training_set_size += count\n",
    "    training_set += wordlist\n",
    "    \n",
    "print('Training set size: %d stem words, %d sentences' \\\n",
    "      % (training_set_size, len(training_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Question to ponder:\n",
    "\n",
    "Should we randomize the training set?\n",
    "\n",
    "Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Train a word2vec model\n",
    "\n",
    "(Credits: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors)\n",
    "\n",
    "With the list of nicely parsed sentences, we're ready to train the model. There are a number of parameter choices that affect the run time and the quality of the final model that is produced.\n",
    "\n",
    "For details on the algorithms below, see the [word2vec API documentation](https://radimrehurek.com/gensim/models/word2vec.html) as well as the [Google documentation](https://code.google.com/archive/p/word2vec/)(Performance section).\n",
    "\n",
    "### Domain characteristics\n",
    "\n",
    "Our training set is:\n",
    "- Small (under 25k words). Typically, word2vec training sets can go in hundreds of thousands.\n",
    "- Wikipedia articles about a common topic. We'll expect some words (e.g. singapore) to appear more frequently about that topic. Whether this is something we need to worry about is unclear.\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "#### Architecture:\n",
    "Architecture options are skip-gram (the default: slower, better for infrequent words) or continuous bag of words (fast). \n",
    "\n",
    "#### Training algorithm:\n",
    "This controls which algorithm to use.\n",
    "\n",
    "Hierarchical softmax (the default: better for infrequent words) or negative sampling (better for frequent words, better with low dimensional vectors). Start with the default first.\n",
    "\n",
    "#### Downsampling of frequent words:\n",
    "This controls the threshold for frequent words to be removed randomly. \n",
    "\n",
    "Randomly removing frequent words in large datasets can improve both accuracy and speed.\n",
    "\n",
    "$$p = \\frac{f-t}{f} - \\sqrt{\\frac{t}{f}}$$\n",
    "\n",
    "Where:\n",
    "- $p$: probabability that word is present\n",
    "- $f$: frequency of word in corpus\n",
    "- $t$: the threshold (our downsampling hyperparameter)\n",
    "\n",
    "A smaller $t$ means more words will be randomly removed.\n",
    "\n",
    "(Source: https://levyomer.files.wordpress.com/2015/03/improving-distributional-similarity-tacl-2015.pdf)\n",
    "\n",
    "The [Google documentation](https://code.google.com/archive/p/word2vec/) recommends values between 1e-3 and 1e-5. Let's try 1e-3 and then iterate from there, since our training set is small.\n",
    "\n",
    "#### Word vector dimensionality:\n",
    "This controls how many features the word vector should have. Higher dimensionality (more features) usually result in better models, but also longer runtimes. Reasonable values can be in the tens to hundreds. We'll try 200.\n",
    "\n",
    "#### Context / window size:\n",
    "This defines the window-size to look for related words. For skip-gram usually around 10, for CBOW around 5. More is better, up to a point.\n",
    "\n",
    "### Worker threads:\n",
    "Number of parallel processes to run. This can significantly improve training speed.  \n",
    "\n",
    "The number to choose depends on how many logical CPU cores your computer has (on Windows, Start Menu -> System Information, look for Processors). \n",
    "\n",
    "Start with a number around 2-4, and then increase up if your computer is more powerful.\n",
    "\n",
    "### Minimum word count:\n",
    "This helps limit the size of the vocabulary to meaningful words. Any word that does not occur at least this many times across all documents is ignored. \n",
    "\n",
    "Reasonable values could be between 10 and 100. Higher values also help limit run time.\n",
    "\n",
    "For wikipedia articles, we'll try a minimum wordcount of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "word2vec.Word2Vec?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits: https://www.kaggle.com/c/word2vec-nlp-tutorial#part-2-word-vectors\n",
    "\n",
    "# Set values for various parameters\n",
    "sg = 1                # Algorithm: 1: skip-gram, 0: CBOW\n",
    "num_features = 200    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 2       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Import the built-in logging module and configure it so that Word2Vec \n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)\n",
    "\n",
    "# Initialize and train the model.\n",
    "# This may take a while if your training set is large (e.g. 500,000 words)\n",
    "print('Training Word2Vec model...')\n",
    "%time model = word2vec.Word2Vec(training_set, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and \n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"wikipedia_{}features_{}minwords_{}context_{}downsampling.w2v\" \\\n",
    "    .format(num_features, min_word_count, context, str(downsampling))\n",
    "model.save(model_name)\n",
    "\n",
    "print('Saved model as %s' % model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Loading the saved model\n",
    "\n",
    "Here's how to load a previously saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"wikipedia_100features_50minwords_10context.w2v\"\n",
    "\n",
    "model = word2vec.Word2Vec.load(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Evaluating the model\n",
    "\n",
    "The trained model contains a read-only `models.keyedvectors.Word2VecMeyedVectors` with methods for evaluating word relationships.\n",
    "\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.Word2VecKeyedVectors\n",
    "\n",
    "Here are some things to try with the word2vec model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Get the vocabulary of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of words in the vocab\n",
    "len(model.wv.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Check if a stem word is in the model's vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmer.stem('malaysia') in model.wv.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem('korea') in model.wv.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Find a word that doesn't match in a list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test = 'raffles indian chinese malay'\n",
    "\n",
    "# you can either use the helper function to convert to stem words\n",
    "# or call stemmer.stem() directly on each word\n",
    "wordlist, _ = text_to_sentence_wordlists(test)\n",
    "print('Input: %s' % wordlist[0])\n",
    "\n",
    "print(\"Word that doesn't match: %s\"\n",
    "      % model.wv.doesnt_match(wordlist[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the top N most similar words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = stemmer.stem('singapore')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = stemmer.stem('changi')\n",
    "model.wv.most_similar(word, topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Measures the cosine distance and similarity between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('changi')\n",
    "word2 = stemmer.stem('aircraft')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word1 = stemmer.stem('changi')\n",
    "word2 = stemmer.stem('british')\n",
    "\n",
    "print('distance: %f' %\n",
    "      model.wv.distance(word1, word2))\n",
    "\n",
    "print('similarity: %f' %\n",
    "      model.wv.similarity(word1, word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Returns the word's representation in vector space as a 1D numpy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "word = stemmer.stem('malaysia')\n",
    "\n",
    "raw_vectors = model.wv.word_vec(word, use_norm=True)\n",
    "\n",
    "raw_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Visualizing Word2Vec\n",
    "\n",
    "Next, we'll plot the Word Vectors to see how the clusters look like:\n",
    "\n",
    "1. Use t-Distributed Stochastic Neighbor Embedding [TSNE](https://lvdmaaten.github.io/tsne/) to reduce the high-dimensional model into 2D\n",
    "2. Plot the 2D representation of the word2vec model, with the words in its vocabulary as the labels\n",
    "\n",
    "Credits: https://stackoverflow.com/questions/43776572/visualise-word2vec-generated-from-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "vocab = list(model.wv.vocab)\n",
    "X = model[vocab]\n",
    "\n",
    "# Apply t-SNE\n",
    "# this can take a while (like 1 minute or more)\n",
    "tsne = TSNE(n_components=2)\n",
    "%time X_tsne = tsne.fit_transform(X)\n",
    "\n",
    "X_tsne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a dataframe for the 2 dimensions,\n",
    "# indexed by the words in the vocab\n",
    "df = pd.DataFrame(X_tsne, index=vocab, columns=['x', 'y'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# create a zoomable interactive plot\n",
    "%matplotlib notebook\n",
    "\n",
    "# Plot the 2D representation of the word2vec model,\n",
    "# with the words in its vocabulary as the labels\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "ax.scatter(df['x'], df['y'])\n",
    "\n",
    "for word, pos in df.iterrows():\n",
    "    ax.annotate(word, pos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Create Corpus and Train Word2Vec\n",
    "\n",
    "In this exercise, we will create our own corpus and use it to train Word2Vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Create Corpus\n",
    "\n",
    "Create a corpus of text files, organized in a structure like this:\n",
    "\n",
    "```\n",
    "corpus/\n",
    "   text001.txt\n",
    "   text002.txt\n",
    "   text003.txt\n",
    "   ...\n",
    "```\n",
    "\n",
    "A sample corpus is included in the `corpus` folder, created with the first 3 chapters of Moby Dick:\n",
    "https://www.gutenberg.org/files/2701/2701-0.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Import corpus using NLTK\n",
    "\n",
    "We will use [`nltk.corpus.reader.plaintext`](http://www.nltk.org/howto/corpus.html) to import the corpus.\n",
    "\n",
    "Credits: https://stackoverflow.com/questions/4951751/creating-a-new-corpus-with-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.reader.plaintext import PlaintextCorpusReader\n",
    "\n",
    "# directory containing the corpus\n",
    "corpus_dir = 'corpus/'\n",
    "\n",
    "# PlaintextCorpusReader uses nltk.tokenize.sent_tokenize() and\n",
    "# nltk.tokenize.word_tokenize() to split texts into sentences and words\n",
    "newcorpus = PlaintextCorpusReader(corpus_dir,\n",
    "                                  '.*\\.txt',\n",
    "                                  encoding='latin1') # or 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files found by the reader\n",
    "newcorpus.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first file in the corpus\n",
    "f = newcorpus.open('text001.txt')\n",
    "print(f.read().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# sentences in the corpus:\n",
    "newcorpus.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# number of sentences\n",
    "len(newcorpus.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_sentence_lists(sentence_lists, remove_stopwords=True):\n",
    "    \"\"\"Cleans and converts the sentence lists\n",
    "    Args:\n",
    "        text: sentence lists\n",
    "        remove_stopwords: whether to remove stopwords\n",
    "    Returns:\n",
    "        A tuple:\n",
    "            The cleaned sentence list\n",
    "            The token count\n",
    "    \"\"\"\n",
    "    # set of English stop words\n",
    "    stops = set(stopwords.words('english'))\n",
    "\n",
    "    stemmer = PorterStemmer()\n",
    "    \n",
    "    result = []\n",
    "    wordcount = 0\n",
    "\n",
    "    for sentence in sentence_lists:\n",
    "        # Convert to lowercase\n",
    "        tokens = [t.lower() for t in sentence]\n",
    "        \n",
    "        # Remove stop words\n",
    "        if remove_stopwords:\n",
    "            tokens = [t for t in tokens if not t in stops]\n",
    "        \n",
    "        # Remove non-letters and numbers\n",
    "        tokens = [re.sub('[^a-zA-Z0-9]', '', t) for t in tokens]\n",
    "        \n",
    "        # Stem the words\n",
    "        tokens = [stemmer.stem(t) for t in tokens]\n",
    "        \n",
    "        result += [tokens]\n",
    "        wordcount += len(tokens)\n",
    "    \n",
    "    return (result, wordcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Your Tasks:\n",
    "\n",
    "1. Convert newcorpus.sents() to sentence wordlists, using the `clean_sentence_lists` helper function\n",
    "2. Train a Word2Vec model, with initial hyperparameter settings (use your best guess)\n",
    "3. Try some word similarity queries\n",
    "4. Apply t-SNE to project the high dimensional model to 2-dimensions\n",
    "5. Plot the 2-dimensional projection of your model\n",
    "6. Tweak your model by adjusting some hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert newcorpus.sents() to sentence wordlists, \n",
    "# using the clean_sentence_lists helper function\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Train a Word2Vec model, with initial hyperparameter settings\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Try some word similarity queries\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 4. Apply t-SNE to reduce the your model to 2 dimensions for plotting\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Plot the completed Word2Vec model\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# References\n",
    "\n",
    "NLTK book: http://www.nltk.org/book/\n",
    "\n",
    "\n",
    "CS224d 2015 Lecture 2: Word Vectors: https://youtu.be/T8tQZChniMk"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
