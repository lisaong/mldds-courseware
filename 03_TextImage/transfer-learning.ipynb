{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "![tranfer_inner_chi](assets/transfer/transfer_inner_chi.jpg)\n",
    "\n",
    "(image: kuailexiaorongrong.blog.163.com, via https://sg.news.yahoo.com/6-kungfu-moves-movies-wished-194241611.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Introduction & motivation\n",
    "- Adapting Neural Networks\n",
    "- Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfering the knowledge of one model to perform a new task.\n",
    "\n",
    "\"Domain Adaptation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "- Lots of data, time, resources needed to train and tune a neural network from scratch\n",
    "  - An ImageNet deep neural net can take weeks to train and fine-tune from scratch.\n",
    "  - Unless you have 256 GPUs, possible to achieve in [1 hour](https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/)\n",
    "- Cheaper, faster way of adapting a neural network by exploiting their generalization properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional vs. Transfer Learning\n",
    "\n",
    "![tradition_v_transfer](assets/transfer/traditional_v_transfer.png)\n",
    "\n",
    "(image: [Survey on Transfer Learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.9185&rep=rep1&type=pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning Types\n",
    "\n",
    "\n",
    "|Type|Description|Examples|\n",
    "|--|--|--|\n",
    "|Inductive|Adapt existing **supervised** training model on new **labeled** dataset|Classification, Regression|\n",
    "|Transductive|Adapt existing **supervised** training model on new **unlabeled** dataset|Classification, Regression|\n",
    "|Unsupervised|Adapt existing **unsupervised** training model on new **unlabeled** dataset|Clustering, Dimensionality Reduction|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning Applications\n",
    "\n",
    "- Image classification (most common): learn new image classes\n",
    "- Text sentiment classification\n",
    "- Text translation to new languages\n",
    "- Speaker adaptation in speech recognition\n",
    "- Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning Services\n",
    "\n",
    "Transfer learning is used in many \"train your own AI model\" services:\n",
    "  - just upload 5-10 images to train a new model! in minutes!\n",
    "\n",
    "![custom vision](assets/transfer/custom-vision.png)\n",
    "\n",
    "(image: https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Layers: General to Specific \n",
    "\n",
    "- Bottom/first/earlier layers: general learners\n",
    " - Low-level notions of edges, visual shapes\n",
    "\n",
    "- Top/last/later layers: specific learners\n",
    "  - High-level features such as eyes, feathers\n",
    "  \n",
    "Note: the top/bottom notation is confusing, I'd avoid it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: VGG 16 Filters\n",
    "\n",
    "![vgg filters](assets/transfer/vgg16_filters_overview.jpg)\n",
    "\n",
    "https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![overview](assets/transfer/Transfer+Learning+Overview.jpg)\n",
    "\n",
    "(image: [Aghamirzaie & Salomon](http://slideplayer.com/slide/8370683/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Process\n",
    "\n",
    "1. Start with pre-trained network\n",
    "\n",
    "2. Partition network into:\n",
    " - Featurizers: identify which layers to keep\n",
    " - Classifiers: identify which layers to replace\n",
    "\n",
    "3. Re-train classifier layers with new data\n",
    "\n",
    "4. Unfreeze weights and fine-tune whole network with smaller learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Freezing and Fine-tuning\n",
    "\n",
    "![vgg 16 modified](assets/transfer/vgg16_modified.png)\n",
    "\n",
    "(image: http://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which layers to re-train?\n",
    "\n",
    "- Depends on the domain\n",
    "- Start by re-training the last layers (last full-connected and last convolutional)\n",
    "  - work backwards if performance is not satisfactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "![transfer performance](assets/transfer/transfer_performance.png)\n",
    "\n",
    "(image: http://arxiv.org/abs/1411.1792)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When and how to fine-tune?\n",
    "\n",
    "Suppose we have model A, trained on dataset A\n",
    "Q: How do we apply transfer learning to dataset B to create model B?\n",
    "\n",
    "|Dataset size|Dataset similarity|Recommendation|\n",
    "|--|--|--|\n",
    "|Large|Very different|Train model B from scratch, initialize weights from model A|\n",
    "|Large|Similar|OK to fine-tune (less likely to overfit)|\n",
    "|Small|Very different|Train classifier using the earlier layers (later layers won't help much)|\n",
    "|Small|Similar|Don't fine-tune (overfitting). Train a linear classifier|\n",
    "\n",
    "https://cs231n.github.io/transfer-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Rates\n",
    "\n",
    "- Training linear classifier: typical learning rate\n",
    "\n",
    "- Fine-tuning: use smaller learning rate to avoid distorting the existing weights\n",
    "  - Assumes weights are close to \"good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workshop: Learning New Image Classes\n",
    "\n",
    "In this workshop, we will:\n",
    "- Create a dataset of new classes not found in ImageNet\n",
    "- Perform inductive transfer learning on a pre-trained ImageNet neural network\n",
    "- Evaluate the results\n",
    "\n",
    "Credits: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gather your dataset\n",
    "\n",
    "We will create a new dataset to perform a new multi-class classification task.\n",
    "\n",
    "1. Pick a category that is NOT found in ImageNet\n",
    "  - For reference, the 1000 imagenet classes are here: http://image-net.org/challenges/LSVRC/2014/browse-synsets\n",
    "\n",
    "2. Download the images from the web. Organize them in a directory structure like this:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "        ducks/\n",
    "            duck001.jpg\n",
    "            duck002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        dogs/\n",
    "            dog001.jpg\n",
    "            dog002.jpg\n",
    "            ...\n",
    "        cats/\n",
    "            cat001.jpg\n",
    "            cat002.jpg\n",
    "            ...\n",
    "        ducks/\n",
    "            ducks001.jpg\n",
    "            ducks002.jpg\n",
    "            ...\n",
    "```            \n",
    "\n",
    "Guidelines\n",
    "  - Provide at least 5 training images per class. The more the better.\n",
    "  - Use any standard image format, such as jpg and png \n",
    "  - Use different images for the training and validation sets, to avoid overfitting\n",
    "  - A sample dataset is available in the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Set dataset path and labels\n",
    "Update `dataset_path` with the path to your dataset\n",
    "- You can use an absolute path (e.g. 'D:/tmp/data') or a relative path\n",
    "\n",
    "Update `labels` with the labels for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 3\n"
     ]
    }
   ],
   "source": [
    "# Update to set the path of your dataset\n",
    "# You can use an absolute path (e.g. 'D:/tmp/data') or a relative path\n",
    "dataset_path='./data'\n",
    "\n",
    "# Update to set the labels for your dataset\n",
    "labels=['chapati', 'fishball_noodle', 'satay']\n",
    "\n",
    "n_classes=len(labels)\n",
    "print('Num classes:', n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_image_files(folder_name, extensions=['png', 'jpg']):\n",
    "    \"\"\"Counts 1-level nested image files in a folder\n",
    "    Arg:\n",
    "        folder_name: name of folder to search\n",
    "        extensions: array of image file extensions\n",
    "    Returns:\n",
    "        number of image files\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    import glob\n",
    "    return reduce((lambda x, y: x + y),\n",
    "        [len(glob.glob('%s/**/*.%s' % (folder_name, ext), recursive=True))\n",
    "            for ext in extensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 45\n",
      "Validation set size: 15\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_folder = os.path.join(dataset_path, 'train')\n",
    "n_train_set = count_image_files(train_folder)\n",
    "\n",
    "print('Training set size:', n_train_set)\n",
    "\n",
    "val_folder = os.path.join(dataset_path, 'validation')\n",
    "n_val_set = count_image_files(val_folder)\n",
    "print('Validation set size:', n_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Perform Data Augmentation\n",
    "\n",
    "- Data Augmentation is a technique to improve the performance of classification models\n",
    "  - This is especially helpful for small training datasets\n",
    "  - http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf\n",
    "\n",
    "- We will use `keras.preprocessing.image.ImageDataGenerator` to randomly rotate and horizontal flip our training data.\n",
    "  - This is documented in https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images belonging to 3 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# https://keras.io/preprocessing/image/#imagedatagenerator-class\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 160x160 matches one of the sizes supported by MobileNet\n",
    "# (the neural network we'll transfer learn from)\n",
    "img_height = img_width = 160\n",
    "channels = 3\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=5,\n",
    "                            zoom_range=.2,\n",
    "                            horizontal_flip=True)\n",
    "\n",
    "# generate image data from the training set\n",
    "generator = datagen.flow_from_directory(train_folder,\n",
    "                                        color_mode='rgb',\n",
    "                                        target_size=(img_height, img_width),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True)\n",
    "\n",
    "# display some images\n",
    "x, y = next(generator)\n",
    "plt.imshow(x[0])\n",
    "plt.title(y[0])\n",
    "plt.show()\n",
    "\n",
    "x, y = next(generator)\n",
    "plt.imshow(x[0])\n",
    "plt.title(y[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Baseline Model - 4-layer CNN from scratch\n",
    "\n",
    "As a baseline, let's train a simple Convolutional Neural Network to do multi-class classification.\n",
    "\n",
    "This will be trained from scratch and compared against the transfer learning model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 160, 160, 8)       224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 53, 53, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 53, 53, 16)        1168      \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 53, 53, 16)        64        \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 53, 53, 16)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 26, 26, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 13, 13, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 13, 13, 32)        128       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 6, 6, 32)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 3459      \n",
      "=================================================================\n",
      "Total params: 19,059\n",
      "Trainable params: 18,899\n",
      "Non-trainable params: 160\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Activation,\\\n",
    "    BatchNormalization, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Block 1\n",
    "# depth 8, kernel 3, stride 1, with padding\n",
    "# input shape: 160, 160, 3 \n",
    "# output shape (of the block): 53, 53, 8\n",
    "model.add(Conv2D(filters=8, kernel_size=(3,3), padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_width, img_height, channels)))\n",
    "# Note: input_shape is inferred for subsequent layers\n",
    "model.add(MaxPool2D(pool_size=(3,3)))\n",
    "\n",
    "# Convolutional Block 2\n",
    "# depth 16, kernel 3, stride 1, with padding\n",
    "# input shape: 53, 53, 8 \n",
    "# output shape (of the block): 26, 26, 16\n",
    "# Note: Batch norm is inserted before activation for 2nd conv block onwards\n",
    "#       Batch Norm removes noise in the covariates (means, variances)\n",
    "#       when we stack convolutional blocks\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional Block 3\n",
    "# depth 32, kernel 3, stride 1, with padding\n",
    "# input shape: 26, 26, 16\n",
    "# output shape (of the block): 13, 13, 32\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional Block 4\n",
    "# depth 32, kernel 3, stride 1, with padding\n",
    "# input shape: 13, 13, 32\n",
    "# output shape (of the block): 6, 6, 32\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Classifier\n",
    "# input shape: 6, 6, 32\n",
    "# output shape: 3\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TensorBoard - Monitoring Training Progress\n",
    "\n",
    "This is the first time we're training neural networks in keras.\n",
    "- We'll be using the default backend of keras, which is Tensorflow.\n",
    "- Therefore, we can use [Tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) to view training progress and metrics.\n",
    "\n",
    "TensorBoard is already included in Tensorflow, so there is no separate installation required.\n",
    "\n",
    "### Launching TensorBoard\n",
    "\n",
    "#### Windows\n",
    "Launch a new Anaconda prompt:\n",
    "\n",
    "```\n",
    "activate mldds03\n",
    "cd \\path\\to\\mldds-courseware\\03_TextImage\n",
    "mkdir logs\n",
    "tensorboard --logdir=./logs --host=0.0.0.0 --port=8008\n",
    "```\n",
    "\n",
    "#### MacOS / Ubuntu\n",
    "Launch a new terminal:\n",
    "\n",
    "```\n",
    "source activate mldds03\n",
    "cd /path/to/mldds-courseware/03_TextImage\n",
    "mkdir logs\n",
    "tensorboard --logdir=./logs --host=0.0.0.0 --port=8008\n",
    "```\n",
    "\n",
    "### Viewing TensorBoard\n",
    "\n",
    "Once you see TensorBoard launched, you can navigate to http://localhost:8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 45 images belonging to 3 classes.\n",
      "Epoch 1/15\n",
      "45/45 [==============================] - 2s 52ms/step - loss: 0.3710 - acc: 0.8444\n",
      "Epoch 2/15\n",
      "45/45 [==============================] - 2s 55ms/step - loss: 0.1027 - acc: 0.9778\n",
      "Epoch 3/15\n",
      "45/45 [==============================] - 2s 46ms/step - loss: 0.1183 - acc: 1.0000: 1s - loss:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24db5309710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import time\n",
    "\n",
    "batch_size = 1 # feel free to increase this if you have more images\n",
    "\n",
    "train_generator = datagen.flow_from_directory(train_folder,\n",
    "                                              color_mode='rgb',\n",
    "                                              target_size=(img_height, img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "# TensorBoard\n",
    "# make a unique log index so that it's easier to filter\n",
    "# by training sessions\n",
    "log_index = int(time.time())\n",
    "\n",
    "# we set histogram_freq=0 because we are using a generator\n",
    "tensorboard = TensorBoard(log_dir='./logs/baseline_cnn/%s' % log_index,\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=False)\n",
    "\n",
    "# Avoid overfitting by setting up early stopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=0,\n",
    "                               verbose=0, mode='auto')\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    n_train_set//batch_size,\n",
    "                    epochs=15,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can view training progress from Tensorboard by going to http://localhost:8008\n",
    "\n",
    "![tensorboard](assets/transfer/tensorboard-demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Classification Model Validation\n",
    " \n",
    "1. Create a test_generator using datagen.flow_from_directory, but point it to the validation folder and set batch_size to all available test samples\n",
    "```\n",
    "test_generator = datagen.flow_from_directory(val_folder,\n",
    "                                                color_mode='rgb',\n",
    "                                                target_size=(img_height, img_width),\n",
    "                                                batch_size=n_val_set,\n",
    "                                                class_mode='categorical',\n",
    "                                                shuffle=False)\n",
    "test_x, test_y = next(test_generator)\n",
    "```\n",
    "\n",
    "2. Run predictions using the CNN we just trained\n",
    "```\n",
    "pred_y = model.predict(test_x)\n",
    "```\n",
    "\n",
    "3. The predictions from the CNN are continuous values, so we need to convert them to categorical.\n",
    "\n",
    "```\n",
    "# convert numbers into one-hot rows\n",
    "bins = np.array([0.5]) # x <= 0.5 becomes 0, else 1\n",
    "pred_y_onehot = np.digitize(pred_y, bins)\n",
    "\n",
    "# convert one-hot rows to label index\n",
    "pred_y_labels = pred_y_onehot.argmax(axis=1)\n",
    "\n",
    "```\n",
    "4. Use sklearn.metrics to evaluate the classification model (using confusion_matrix, classification_report). Note that confusion_matrix takes in label indices, not one-hot arrays.\n",
    "\n",
    "### Setup\n",
    "\n",
    "You may need to install sklearn into your conda environment.\n",
    "\n",
    "```\n",
    "conda install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 images belonging to 3 classes.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.43      0.60      0.50         5\n",
      "          1       0.50      0.20      0.29         5\n",
      "          2       0.67      0.80      0.73         5\n",
      "\n",
      "avg / total       0.53      0.53      0.50        15\n",
      "\n",
      "[[3 1 1]\n",
      " [3 1 1]\n",
      " [1 0 4]]\n"
     ]
    }
   ],
   "source": [
    "# Your code here\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "test_generator = datagen.flow_from_directory(val_folder,\n",
    "                                             color_mode='rgb',\n",
    "                                             target_size=(img_height, img_width),\n",
    "                                             batch_size=n_val_set,\n",
    "                                             class_mode='categorical',\n",
    "                                             shuffle=False)\n",
    "\n",
    "test_x, test_y = next(test_generator)\n",
    "\n",
    "pred_y = model.predict(test_x)\n",
    "\n",
    "# convert numbers into one-hot rows\n",
    "bins = np.array([0.5]) # x <= 0.5 becomes 0, else 1\n",
    "pred_y_onehot = np.digitize(pred_y, bins)\n",
    "\n",
    "# convert one-hot rows to label index\n",
    "pred_y_labels = pred_y_onehot.argmax(axis=1)\n",
    "\n",
    "print(classification_report(test_y, pred_y_onehot))\n",
    "print(confusion_matrix(test_y.argmax(axis=1), pred_y_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Transfer Learning with ImageNet MobileNet\n",
    "\n",
    "Now that we have our baseline CNN, let's try transfer learning. \n",
    "\n",
    "2 Steps:\n",
    "1. Freeze and retrain a classifier on top of MobileNet\n",
    "2. Un-freeze and fine-tune the model we got from step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Load the featurizer portion of MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 160, 160, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)    (None, 162, 162, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv1 (Conv2D)               (None, 80, 80, 32)        864       \n",
      "_________________________________________________________________\n",
      "conv1_bn (BatchNormalization (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv1_relu (Activation)      (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_1 (ZeroPadding2D)   (None, 82, 82, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_1 (DepthwiseConv2D)  (None, 80, 80, 32)        288       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_bn (BatchNormaliza (None, 80, 80, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv_dw_1_relu (Activation)  (None, 80, 80, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_1 (Conv2D)           (None, 80, 80, 64)        2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_1_bn (BatchNormaliza (None, 80, 80, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_pw_1_relu (Activation)  (None, 80, 80, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_2 (ZeroPadding2D)   (None, 82, 82, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_2 (DepthwiseConv2D)  (None, 40, 40, 64)        576       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_bn (BatchNormaliza (None, 40, 40, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv_dw_2_relu (Activation)  (None, 40, 40, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_2 (Conv2D)           (None, 40, 40, 128)       8192      \n",
      "_________________________________________________________________\n",
      "conv_pw_2_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_2_relu (Activation)  (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_3 (ZeroPadding2D)   (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_3 (DepthwiseConv2D)  (None, 40, 40, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_3_relu (Activation)  (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_3 (Conv2D)           (None, 40, 40, 128)       16384     \n",
      "_________________________________________________________________\n",
      "conv_pw_3_bn (BatchNormaliza (None, 40, 40, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_pw_3_relu (Activation)  (None, 40, 40, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_4 (ZeroPadding2D)   (None, 42, 42, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_4 (DepthwiseConv2D)  (None, 20, 20, 128)       1152      \n",
      "_________________________________________________________________\n",
      "conv_dw_4_bn (BatchNormaliza (None, 20, 20, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv_dw_4_relu (Activation)  (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_4 (Conv2D)           (None, 20, 20, 256)       32768     \n",
      "_________________________________________________________________\n",
      "conv_pw_4_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_4_relu (Activation)  (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_5 (ZeroPadding2D)   (None, 22, 22, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_5 (DepthwiseConv2D)  (None, 20, 20, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_5_relu (Activation)  (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_5 (Conv2D)           (None, 20, 20, 256)       65536     \n",
      "_________________________________________________________________\n",
      "conv_pw_5_bn (BatchNormaliza (None, 20, 20, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_pw_5_relu (Activation)  (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_6 (ZeroPadding2D)   (None, 22, 22, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_6 (DepthwiseConv2D)  (None, 10, 10, 256)       2304      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_bn (BatchNormaliza (None, 10, 10, 256)       1024      \n",
      "_________________________________________________________________\n",
      "conv_dw_6_relu (Activation)  (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_6 (Conv2D)           (None, 10, 10, 512)       131072    \n",
      "_________________________________________________________________\n",
      "conv_pw_6_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_6_relu (Activation)  (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_7 (ZeroPadding2D)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_7 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_7_relu (Activation)  (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_7 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_7_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_7_relu (Activation)  (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_8 (ZeroPadding2D)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_8 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_8_relu (Activation)  (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_8 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_8_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_8_relu (Activation)  (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_9 (ZeroPadding2D)   (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_9 (DepthwiseConv2D)  (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_9_relu (Activation)  (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_9 (Conv2D)           (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_9_bn (BatchNormaliza (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_9_relu (Activation)  (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_10 (ZeroPadding2D)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_10 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_10_relu (Activation) (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_10 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_10_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_10_relu (Activation) (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_11 (ZeroPadding2D)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_11 (DepthwiseConv2D) (None, 10, 10, 512)       4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_11_relu (Activation) (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pw_11 (Conv2D)          (None, 10, 10, 512)       262144    \n",
      "_________________________________________________________________\n",
      "conv_pw_11_bn (BatchNormaliz (None, 10, 10, 512)       2048      \n",
      "_________________________________________________________________\n",
      "conv_pw_11_relu (Activation) (None, 10, 10, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)  (None, 12, 12, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D) (None, 5, 5, 512)         4608      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormaliz (None, 5, 5, 512)         2048      \n",
      "_________________________________________________________________\n",
      "conv_dw_12_relu (Activation) (None, 5, 5, 512)         0         \n",
      "_________________________________________________________________\n",
      "conv_pw_12 (Conv2D)          (None, 5, 5, 1024)        524288    \n",
      "_________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_12_relu (Activation) (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pad_13 (ZeroPadding2D)  (None, 7, 7, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_dw_13_relu (Activation) (None, 5, 5, 1024)        0         \n",
      "_________________________________________________________________\n",
      "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
      "_________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
      "_________________________________________________________________\n",
      "conv_pw_13_relu (Activation) (None, 5, 5, 1024)        0         \n",
      "=================================================================\n",
      "Total params: 3,228,864\n",
      "Trainable params: 3,206,976\n",
      "Non-trainable params: 21,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.applications import MobileNet\n",
    "\n",
    "# Exclude the Dense layer from the network by setting include_top=False\n",
    "# We are going to re-train a classifier with the remaining layers\n",
    "featurizer = MobileNet(include_top=False,\n",
    "                       weights='imagenet',\n",
    "                       input_shape=(img_width, img_height, channels))\n",
    "featurizer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Freeze the featurizer weights\n",
    "\n",
    "If the weights and labels were not previously saved:\n",
    "  1. Run the training set through the MobileNet featurizer\n",
    "  2. Save the output of the featurizer and corresponding training set labels\n",
    "\n",
    "Else load the weights and labels from files.\n",
    "\n",
    "Since this is supervised learning, we need both the weights and labels to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weights to mobilenet_features_train.npy:\n",
      "Found 45 images belonging to 3 classes.\n",
      "Saving labels to mobilenet_labels_train.npy:\n",
      "features_train.shape: (45, 5, 5, 1024)\n",
      "labels_train: [[1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# For large neural networks such as VGG, it is a good idea to save the\n",
    "# weights to a file that we load during transfer learning training.\n",
    "\n",
    "# https://wiki.python.org/moin/UsingPickle\n",
    "import pickle\n",
    "weights_file = 'mobilenet_features_train.npy'\n",
    "labels_file = 'mobilenet_labels_train.npy'\n",
    "\n",
    "batch_size = 1 # feel free to increase this if you have more images\n",
    "\n",
    "if os.path.isfile(weights_file) and os.path.isfile(labels_file):\n",
    "    print('Loading weights from %s:' % weights_file)\n",
    "    with open(weights_file, 'rb') as f:\n",
    "        features_train = pickle.load(f)\n",
    "        \n",
    "    print('Loading labels from %s:' % labels_file)\n",
    "    with open(labels_file, 'rb') as f:\n",
    "        labels_train = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print('Saving weights to %s:' % weights_file)\n",
    "    train_generator = datagen.flow_from_directory(train_folder,\n",
    "                                                  color_mode='rgb',\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=n_train_set,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "    print('Saving labels to %s:' % labels_file)\n",
    "    \n",
    "    # capture the featurizer weights and labels\n",
    "    train_x, labels_train = next(train_generator)\n",
    "\n",
    "    features_train = featurizer.predict(train_x)    \n",
    "\n",
    "    # save to file\n",
    "    pickle.dump(features_train, open(weights_file, 'wb'))\n",
    "    pickle.dump(labels_train, open(labels_file, 'wb'))\n",
    "    \n",
    "print('features_train.shape:', features_train.shape)\n",
    "print('labels_train:', labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Train a new classifier with the frozen weights\n",
    "\n",
    "Now that we have the featurizer weights, we can:\n",
    "\n",
    "1. Create a general classifier.\n",
    "2. Train the classifier with the weights (X) and labels (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Create a general classifier\n",
    "\n",
    "Note that this classifier does not have to be related to the architecture of the original network.\n",
    "\n",
    "It simply treats the inputs as opaque features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_4 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                1638464   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 1,638,659\n",
      "Trainable params: 1,638,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Flatten(input_shape=features_train.shape[1:])) # flatten (5, 5, 1024) to vector\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(3, activation='softmax'))\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: Train the classifier\n",
    "\n",
    "1. Setup a tensorboard callback\n",
    "\n",
    "```\n",
    "tensorboard = TensorBoard(log_dir='./logs/mobilenet_trf/%s' % log_index,\n",
    "                             histogram_freq=0, write_graph=True, write_images=False)\n",
    "```\n",
    "\n",
    "2. Setup an EarlyStopping callback to monitor the validation loss. \n",
    " - To understand what it does, try experimenting with these parameters or removing this callback to see what happens.\n",
    " - To disable this callback, you need to update the list of callbacks passed to `fit`\n",
    " - You should see overfitting if you disable the callback AND increase `epochs` for `fit`.\n",
    "\n",
    "```\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=0, mode='auto')\n",
    "```\n",
    "\n",
    "3. Compile the model, then fit with the features and labels\n",
    "\n",
    "```\n",
    "classifier.compile(optimizer='rmsprop',\n",
    "                   loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(x=features_train,\n",
    "               y=labels_train,\n",
    "               validation_split=0.2,\n",
    "               batch_size=batch_size,\n",
    "               epochs=10,\n",
    "               callbacks=[tensorboard, early_stopping])\n",
    "```\n",
    "\n",
    "If you run `fit` multiple times, you may notice that the model continues training from the previous call to `fit`.\n",
    " - To reset the model state, you may find it useful to just re-declare the model before calling `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 36 samples, validate on 9 samples\n",
      "Epoch 1/10\n",
      "36/36 [==============================] - 2s 43ms/step - loss: 7.0611 - acc: 0.3889 - val_loss: 4.8602 - val_acc: 0.3333\n",
      "Epoch 2/10\n",
      "36/36 [==============================] - 2s 42ms/step - loss: 5.0864 - acc: 0.6111 - val_loss: 0.2943 - val_acc: 0.7778\n",
      "Epoch 3/10\n",
      "36/36 [==============================] - 1s 41ms/step - loss: 4.2207 - acc: 0.6944 - val_loss: 2.4206 - val_acc: 0.6667\n",
      "Epoch 4/10\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 1.0093 - acc: 0.8889 - val_loss: 0.1564 - val_acc: 0.8889\n",
      "Epoch 5/10\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 1.1762 - acc: 0.8056 - val_loss: 1.4532 - val_acc: 0.7778\n",
      "Epoch 6/10\n",
      "36/36 [==============================] - 1s 38ms/step - loss: 1.2441 - acc: 0.8611 - val_loss: 0.2414 - val_acc: 0.8889\n",
      "Epoch 7/10\n",
      "36/36 [==============================] - 1s 40ms/step - loss: 0.6030 - acc: 0.9444 - val_loss: 8.7196e-05 - val_acc: 1.0000\n",
      "Epoch 8/10\n",
      "36/36 [==============================] - 1s 33ms/step - loss: 0.9644 - acc: 0.8889 - val_loss: 0.6522 - val_acc: 0.8889\n",
      "Epoch 9/10\n",
      "36/36 [==============================] - 1s 35ms/step - loss: 0.2561 - acc: 0.9722 - val_loss: 1.1921e-07 - val_acc: 1.0000\n",
      "Epoch 10/10\n",
      "36/36 [==============================] - 1s 34ms/step - loss: 0.0199 - acc: 0.9722 - val_loss: 1.1921e-07 - val_acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x24dbbf36128>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your code here\n",
    "\n",
    "# If iteratively running this, easier to keep in the same cell\n",
    "classifier = Sequential()\n",
    "classifier.add(Flatten(input_shape=features_train.shape[1:])) # flatten (5, 5, 1024) to vector\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(3, activation='softmax'))\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4,\n",
    "                               verbose=0, mode='auto')\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs/mobilenet_trf/%s' % log_index,\n",
    "                           histogram_freq=0, write_graph=True,\n",
    "                           write_images=False)\n",
    "\n",
    "classifier.compile(optimizer='rmsprop',\n",
    "                   loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(x=features_train,\n",
    "               y=labels_train,\n",
    "               validation_split=0.2,\n",
    "               batch_size=batch_size,\n",
    "               epochs=10,\n",
    "               callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Evaluating the transfer learnt model is not as straightforward as giving it the test inputs.\n",
    "\n",
    "We'll need to:\n",
    "1. Get the validation set features (and labels) from the MobileNet featurizer\n",
    "2. Pass the features into the classifier to get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 images belonging to 3 classes.\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00         5\n",
      "          1       0.83      1.00      0.91         5\n",
      "          2       1.00      0.80      0.89         5\n",
      "\n",
      "avg / total       0.94      0.93      0.93        15\n",
      "\n",
      "[[5 0 0]\n",
      " [0 5 0]\n",
      " [0 1 4]]\n"
     ]
    }
   ],
   "source": [
    "test_generator = datagen.flow_from_directory(val_folder,\n",
    "                                             color_mode='rgb',\n",
    "                                             target_size=(img_height, img_width),\n",
    "                                             batch_size=n_val_set,\n",
    "                                             class_mode='categorical',\n",
    "                                             shuffle=False)\n",
    "\n",
    "# Get the validation set features (and labels) from the MobileNet featurizer\n",
    "test_x, test_y = next(test_generator)\n",
    "test_features = featurizer.predict(test_x)\n",
    "\n",
    "# Pass the features into the classifier to get the predictions.\n",
    "pred_y = classifier.predict(test_features)\n",
    "\n",
    "# convert numbers into one-hot rows\n",
    "bins = np.array([0.5]) # x <= 0.5 becomes 0, else 1\n",
    "pred_y_onehot = np.digitize(pred_y, bins)\n",
    "\n",
    "# convert one-hot rows to label index\n",
    "pred_y_labels = pred_y_onehot.argmax(axis=1)\n",
    "\n",
    "# Evaluate metrics\n",
    "print(classification_report(test_y, pred_y_onehot))\n",
    "print(confusion_matrix(test_y.argmax(axis=1), pred_y_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Fine-tuning\n",
    "\n",
    "We'll cover the steps of fine-tuning a network for the last section of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "mobilenet = MobileNet(include_top=False,\n",
    "                      weights='imagenet',\n",
    "                      input_shape=(img_width, img_height, channels))\n",
    "combinedModel = Model(inputs=mobilenet.input,\n",
    "                      outputs=model(mobilenet.output))\n",
    "\n",
    "for layer in combinedModel.layers[:-3]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "|Material|Read it for|URL|\n",
    "|--|--|--|\n",
    "|A Survey on Transfer Learning (IEEE)|Overview of transfer learning|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.9185&rep=rep1&type=pdf|\n",
    "|Transductive Learning: Motivation, Model, Algorithms|Explanation of Induction vs. Transduction Transfer Learning|http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2527.pdf|\n",
    "|Supervised and Unsupervised Transfer Learning for Question Answering (Paper)|More unique application of transfer learning|https://arxiv.org/abs/1711.05345|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
