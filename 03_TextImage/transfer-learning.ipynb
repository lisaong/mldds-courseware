{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "![tranfer_inner_chi](assets/transfer/transfer_inner_chi.jpg)\n",
    "\n",
    "(image: kuailexiaorongrong.blog.163.com, via https://sg.news.yahoo.com/6-kungfu-moves-movies-wished-194241611.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Introduction & motivation\n",
    "- Adapting Neural Networks\n",
    "- Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Transfering the knowledge of one model to perform a new task.\n",
    "\n",
    "\"Domain Adaptation\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "- Lots of data, time, resources needed to train and tune a neural network from scratch\n",
    "  - An ImageNet deep neural net can take weeks to train and fine-tune from scratch.\n",
    "  - Unless you have 256 GPUs, possible to achieve in [1 hour](https://research.fb.com/publications/accurate-large-minibatch-sgd-training-imagenet-in-1-hour/)\n",
    "- Cheaper, faster way of adapting a neural network by exploiting their generalization properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Traditional vs. Transfer Learning\n",
    "\n",
    "![tradition_v_transfer](assets/transfer/traditional_v_transfer.png)\n",
    "\n",
    "(image: [Survey on Transfer Learning](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.9185&rep=rep1&type=pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning Types\n",
    "\n",
    "\n",
    "|Type|Description|Examples|\n",
    "|--|--|--|\n",
    "|Inductive|Adapt existing **supervised** training model on new **labeled** dataset|Classification, Regression|\n",
    "|Transductive|Adapt existing **supervised** training model on new **unlabeled** dataset|Classification, Regression|\n",
    "|Unsupervised|Adapt existing **unsupervised** training model on new **unlabeled** dataset|Clustering, Dimensionality Reduction|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning Applications\n",
    "\n",
    "- Image classification (most common): learn new image classes\n",
    "- Text sentiment classification\n",
    "- Text translation to new languages\n",
    "- Speaker adaptation in speech recognition\n",
    "- Question answering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Transfer Learning Services\n",
    "\n",
    "Transfer learning is used in many \"train your own AI model\" services:\n",
    "  - just upload 5-10 images to train a new model! in minutes!\n",
    "\n",
    "![custom vision](assets/transfer/custom-vision.png)\n",
    "\n",
    "(image: https://azure.microsoft.com/en-us/services/cognitive-services/custom-vision-service/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Transfer Learning in Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network Layers: General to Specific \n",
    "\n",
    "- Bottom/first/earlier layers: general learners\n",
    " - Low-level notions of edges, visual shapes\n",
    "\n",
    "- Top/last/later layers: specific learners\n",
    "  - High-level features such as eyes, feathers\n",
    "  \n",
    "Note: the top/bottom notation is confusing, I'd avoid it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example: VGG 16 Filters\n",
    "\n",
    "![vgg filters](assets/transfer/vgg16_filters_overview.jpg)\n",
    "\n",
    "https://blog.keras.io/how-convolutional-neural-networks-see-the-world.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![overview](assets/transfer/Transfer+Learning+Overview.jpg)\n",
    "\n",
    "(image: [Aghamirzaie & Salomon](http://slideplayer.com/slide/8370683/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Process\n",
    "\n",
    "1. Start with pre-trained network\n",
    "\n",
    "2. Partition network into:\n",
    " - Featurizers: identify which layers to keep\n",
    " - Classifiers: identify which layers to replace\n",
    "\n",
    "3. Re-train classifier layers with new data\n",
    "\n",
    "4. Unfreeze weights and fine-tune whole network with smaller learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Freezing and Fine-tuning\n",
    "\n",
    "![vgg 16 modified](assets/transfer/vgg16_modified.png)\n",
    "\n",
    "(image: http://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Which layers to re-train?\n",
    "\n",
    "- Depends on the domain\n",
    "- Start by re-training the last layers (last full-connected and last convolutional)\n",
    "  - work backwards if performance is not satisfactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Example\n",
    "\n",
    "![transfer performance](assets/transfer/transfer_performance.png)\n",
    "\n",
    "(image: http://arxiv.org/abs/1411.1792)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## When and how to fine-tune?\n",
    "\n",
    "Suppose we have model A, trained on dataset A\n",
    "Q: How do we apply transfer learning to dataset B to create model B?\n",
    "\n",
    "|Dataset size|Dataset similarity|Recommendation|\n",
    "|--|--|--|\n",
    "|Large|Very different|Train model B from scratch, initialize weights from model A|\n",
    "|Large|Similar|OK to fine-tune (less likely to overfit)|\n",
    "|Small|Very different|Train classifier using the earlier layers (later layers won't help much)|\n",
    "|Small|Similar|Don't fine-tune (overfitting). Train a linear classifier|\n",
    "\n",
    "https://cs231n.github.io/transfer-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Rates\n",
    "\n",
    "- Training linear classifier: typical learning rate\n",
    "\n",
    "- Fine-tuning: use smaller learning rate to avoid distorting the existing weights\n",
    "  - Assumes weights are close to \"good\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workshop: Learning New Image Classes\n",
    "\n",
    "In this workshop, we will:\n",
    "- Create a dataset of new classes not found in ImageNet\n",
    "- Perform inductive transfer learning on a pre-trained ImageNet neural network\n",
    "- Evaluate the results\n",
    "\n",
    "Credits: https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Choose your own dataset\n",
    "\n",
    "We will create a new dataset to perform a new multi-class classification task.\n",
    "\n",
    "1. Pick a category that is NOT found in ImageNet\n",
    "  - For reference, the 1000 imagenet classes are here: http://image-net.org/challenges/LSVRC/2014/browse-synsets\n",
    "\n",
    "2. Download your images from the web. Organize them in a directory structure like this:\n",
    "```\n",
    "data/\n",
    "    train/\n",
    "        chapati/\n",
    "            chapati001.jpg\n",
    "            chapati002.jpg\n",
    "            ...\n",
    "        fishball_noodle/\n",
    "            fishball_noodle001.jpg\n",
    "            fishball_noodle002.jpg\n",
    "            ...\n",
    "        satay/\n",
    "            satay001.jpg\n",
    "            satay002.jpg\n",
    "            ...\n",
    "    validation/\n",
    "        chapati/\n",
    "            chapati001.jpg\n",
    "            chapati002.jpg\n",
    "            ...\n",
    "        fishball_noodle/\n",
    "            fishball_noodle001.jpg\n",
    "            fishball_noodle002.jpg\n",
    "            ...\n",
    "        satay/\n",
    "            satay001.jpg\n",
    "            satay002.jpg\n",
    "            ...\n",
    "```            \n",
    "\n",
    "### Guidelines\n",
    "  - Pick classes that have distinctive image characteristics\n",
    "     - For example, circles, lines, patterns.\n",
    "  - Provide at least 20 training images per class. The more the better, to avoid overfitting.\n",
    "  - Use different images for the training and validation sets.\n",
    "  - Use any standard image format, such as jpg and png \n",
    "  - A sample dataset is available in the data folder. Warning: it may make you hungry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Set dataset path and labels\n",
    "Update `dataset_path` with the path to your dataset\n",
    "- You can use an absolute path (e.g. 'D:/tmp/data') or a relative path\n",
    "\n",
    "Update `labels` with the labels for your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Update to set the path of your dataset\n",
    "# You can use an absolute path (e.g. 'D:/tmp/data') or a relative path\n",
    "dataset_path='./data'\n",
    "\n",
    "# Update to set the labels for your dataset\n",
    "labels=['chapati', 'fishball_noodle', 'satay']\n",
    "\n",
    "n_classes=len(labels)\n",
    "print('Num classes:', n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_image_files(folder_name, extensions=['png', 'jpg']):\n",
    "    \"\"\"Counts 1-level nested image files in a folder\n",
    "    Arg:\n",
    "        folder_name: name of folder to search\n",
    "        extensions: array of image file extensions\n",
    "    Returns:\n",
    "        number of image files\n",
    "    \"\"\"\n",
    "    from functools import reduce\n",
    "    import glob\n",
    "    return reduce((lambda x, y: x + y),\n",
    "        [len(glob.glob('%s/**/*.%s' % (folder_name, ext), recursive=True))\n",
    "            for ext in extensions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "train_folder = os.path.join(dataset_path, 'train')\n",
    "n_train_set = count_image_files(train_folder)\n",
    "\n",
    "print('Training set size:', n_train_set)\n",
    "\n",
    "val_folder = os.path.join(dataset_path, 'validation')\n",
    "n_val_set = count_image_files(val_folder)\n",
    "print('Validation set size:', n_val_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Perform Data Augmentation\n",
    "\n",
    "- Data Augmentation is a technique to improve the performance of classification models\n",
    "  - This is especially helpful for small training datasets\n",
    "  - http://cs231n.stanford.edu/reports/2017/pdfs/300.pdf\n",
    "\n",
    "- We will use `keras.preprocessing.image.ImageDataGenerator` to randomly rotate and horizontal flip our training data.\n",
    "  - This is documented in https://keras.io/preprocessing/image/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# https://keras.io/preprocessing/image/#imagedatagenerator-class\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 160x160 matches one of the sizes supported by MobileNet\n",
    "# (the neural network we'll transfer learn from)\n",
    "img_height = img_width = 160\n",
    "channels = 3\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                            rotation_range=5,\n",
    "                            zoom_range=.2,\n",
    "                            horizontal_flip=True)\n",
    "\n",
    "# generate image data from the training set\n",
    "generator = datagen.flow_from_directory(train_folder,\n",
    "                                        color_mode='rgb',\n",
    "                                        target_size=(img_height, img_width),\n",
    "                                        batch_size=1,\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True)\n",
    "\n",
    "# display some images\n",
    "x, y = next(generator)\n",
    "plt.imshow(x[0])\n",
    "plt.title(y[0])\n",
    "plt.show()\n",
    "\n",
    "x, y = next(generator)\n",
    "plt.imshow(x[0])\n",
    "plt.title(y[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Baseline Model - 4-layer CNN from scratch\n",
    "\n",
    "As a baseline, let's train a simple Convolutional Neural Network to do multi-class classification.\n",
    "\n",
    "This will be trained from scratch and compared against the transfer learning model(s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPool2D, Activation,\\\n",
    "    BatchNormalization, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Block 1\n",
    "# depth 8, kernel 3, stride 1, with padding\n",
    "# input shape: 160, 160, 3 \n",
    "# output shape (of the block): 53, 53, 8\n",
    "model.add(Conv2D(filters=8, kernel_size=(3,3), padding='same',\n",
    "                 activation='relu',\n",
    "                 input_shape=(img_width, img_height, channels)))\n",
    "# Note: input_shape is inferred for subsequent layers\n",
    "model.add(MaxPool2D(pool_size=(3,3)))\n",
    "\n",
    "# Convolutional Block 2\n",
    "# depth 16, kernel 3, stride 1, with padding\n",
    "# input shape: 53, 53, 8 \n",
    "# output shape (of the block): 26, 26, 16\n",
    "# Note: Batch norm is inserted before activation for 2nd conv block onwards\n",
    "#       Batch Norm removes noise in the covariates (means, variances)\n",
    "#       when we stack convolutional blocks\n",
    "model.add(Conv2D(filters=16, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional Block 3\n",
    "# depth 32, kernel 3, stride 1, with padding\n",
    "# input shape: 26, 26, 16\n",
    "# output shape (of the block): 13, 13, 32\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Convolutional Block 4\n",
    "# depth 32, kernel 3, stride 1, with padding\n",
    "# input shape: 13, 13, 32\n",
    "# output shape (of the block): 6, 6, 32\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "# Classifier\n",
    "# input shape: 6, 6, 32\n",
    "# output shape: 3\n",
    "model.add(Flatten())\n",
    "model.add(Dense(n_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### TensorBoard - Monitoring Training Progress\n",
    "\n",
    "This is the first time we're training neural networks in keras.\n",
    "- We'll be using the default backend of keras, which is Tensorflow.\n",
    "- Therefore, we can use [Tensorboard](https://www.tensorflow.org/programmers_guide/summaries_and_tensorboard) to view training progress and metrics.\n",
    "\n",
    "TensorBoard is already included in Tensorflow, so there is no separate installation required.\n",
    "\n",
    "### Launching TensorBoard\n",
    "\n",
    "#### Windows\n",
    "Launch a new Anaconda prompt:\n",
    "\n",
    "```\n",
    "activate mldds03\n",
    "cd \\path\\to\\mldds-courseware\\03_TextImage\n",
    "mkdir logs\n",
    "tensorboard --logdir=./logs --host=0.0.0.0 --port=8008\n",
    "```\n",
    "\n",
    "#### MacOS / Ubuntu\n",
    "Launch a new terminal:\n",
    "\n",
    "```\n",
    "source activate mldds03\n",
    "cd /path/to/mldds-courseware/03_TextImage\n",
    "mkdir logs\n",
    "tensorboard --logdir=./logs --host=0.0.0.0 --port=8008\n",
    "```\n",
    "\n",
    "### Viewing TensorBoard\n",
    "\n",
    "Once you see TensorBoard launched, you can navigate to http://localhost:8008"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import time\n",
    "\n",
    "batch_size = 1 # feel free to increase this if you have more images\n",
    "\n",
    "train_generator = datagen.flow_from_directory(train_folder,\n",
    "                                              color_mode='rgb',\n",
    "                                              target_size=(img_height, img_width),\n",
    "                                              batch_size=batch_size,\n",
    "                                              class_mode='categorical',\n",
    "                                              shuffle=True)\n",
    "# TensorBoard\n",
    "# make a unique log index so that it's easier to filter\n",
    "# by training sessions\n",
    "log_index = int(time.time())\n",
    "\n",
    "# we set histogram_freq=0 because we are using a generator\n",
    "tensorboard = TensorBoard(log_dir='./logs/baseline_cnn/%s' % log_index,\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=False)\n",
    "\n",
    "# Avoid overfitting by setting up early stopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=0,\n",
    "                               verbose=0, mode='auto')\n",
    "\n",
    "model.compile(optimizer='adadelta',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(train_generator,\n",
    "                    n_train_set//batch_size,\n",
    "                    epochs=15,\n",
    "                    callbacks=[tensorboard, early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "You can view training progress from Tensorboard by going to http://localhost:8008\n",
    "\n",
    "![tensorboard](assets/transfer/tensorboard-demo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Classification Model Validation\n",
    " \n",
    "1. Create a test_generator using datagen.flow_from_directory, but point it to the validation folder and set batch_size to all available test samples\n",
    "\n",
    "```\n",
    "# no data augmentation for the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(val_folder,\n",
    "                                                  color_mode='rgb',\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=n_val_set,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)\n",
    "test_x, test_y = next(test_generator)\n",
    "```\n",
    "\n",
    "2. Run predictions using the CNN we just trained\n",
    "```\n",
    "pred_y = model.predict(test_x)\n",
    "```\n",
    "\n",
    "3. The predictions from the CNN are continuous values, so we need to convert them to categorical.\n",
    "\n",
    "```\n",
    "# convert numbers into one-hot rows\n",
    "bins = np.array([0.5]) # x <= 0.5 becomes 0, else 1\n",
    "pred_y_onehot = np.digitize(pred_y, bins)\n",
    "\n",
    "# convert one-hot rows to label index\n",
    "pred_y_labels = pred_y_onehot.argmax(axis=1)\n",
    "\n",
    "```\n",
    "4. Use sklearn.metrics to evaluate the classification model (using confusion_matrix, classification_report). Note that confusion_matrix takes in label indices, not one-hot arrays.\n",
    "\n",
    "### Setup\n",
    "\n",
    "You may need to install sklearn into your conda environment.\n",
    "\n",
    "```\n",
    "conda install scikit-learn\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If you used the sample dataset, you should get classification metrics similar to this:\n",
    "```\n",
    "Found 15 images belonging to 3 classes.\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.38      0.60      0.46         5\n",
    "          1       1.00      0.20      0.33         5\n",
    "          2       0.50      0.60      0.55         5\n",
    "\n",
    "avg / total       0.62      0.47      0.45        15\n",
    "\n",
    "[[3 0 2]\n",
    " [3 1 1]\n",
    " [2 0 3]]\n",
    "```\n",
    "\n",
    "This baseline model doesn't perform too well.  Let's see what we get with Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Transfer Learning with ImageNet MobileNet\n",
    "\n",
    "Now that we have our baseline CNN, let's try transfer learning. \n",
    "\n",
    "2 Steps:\n",
    "1. Freeze and retrain a classifier on top of MobileNet\n",
    "2. Un-freeze and fine-tune the model we got from step 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Load the featurizer portion of MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import MobileNet\n",
    "\n",
    "# Exclude the Dense layer from the network by setting include_top=False\n",
    "# We are going to re-train a classifier with the remaining layers\n",
    "featurizer = MobileNet(include_top=False,\n",
    "                       weights='imagenet',\n",
    "                       input_shape=(img_width, img_height, channels))\n",
    "featurizer.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Freeze the featurizer weights\n",
    "\n",
    "If the weights and labels were not previously saved:\n",
    "  1. Run the training set through the MobileNet featurizer\n",
    "  2. Save the output of the featurizer and corresponding training set labels\n",
    "\n",
    "Else load the weights and labels from files.\n",
    "\n",
    "Since this is supervised learning, we need both the weights and labels to train the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# For large neural networks such as VGG, it is a good idea to save the\n",
    "# weights to a file that we load during transfer learning training.\n",
    "\n",
    "# https://wiki.python.org/moin/UsingPickle\n",
    "import pickle\n",
    "weights_file = 'mobilenet_features_train.npy'\n",
    "labels_file = 'mobilenet_labels_train.npy'\n",
    "\n",
    "batch_size = 1 # feel free to increase this if you have more images\n",
    "\n",
    "if os.path.isfile(weights_file) and os.path.isfile(labels_file):\n",
    "    print('Loading weights from %s:' % weights_file)\n",
    "    with open(weights_file, 'rb') as f:\n",
    "        features_train = pickle.load(f)\n",
    "        \n",
    "    print('Loading labels from %s:' % labels_file)\n",
    "    with open(labels_file, 'rb') as f:\n",
    "        labels_train = pickle.load(f)\n",
    "        \n",
    "else:\n",
    "    print('Saving weights to %s:' % weights_file)\n",
    "    train_generator = datagen.flow_from_directory(train_folder,\n",
    "                                                  color_mode='rgb',\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=n_train_set,\n",
    "                                                  class_mode='categorical')\n",
    "\n",
    "    print('Saving labels to %s:' % labels_file)\n",
    "    \n",
    "    # capture the featurizer weights and labels\n",
    "    train_x, labels_train = next(train_generator)\n",
    "\n",
    "    features_train = featurizer.predict(train_x)    \n",
    "\n",
    "    # save to file\n",
    "    pickle.dump(features_train, open(weights_file, 'wb'))\n",
    "    pickle.dump(labels_train, open(labels_file, 'wb'))\n",
    "    \n",
    "print('features_train.shape:', features_train.shape)\n",
    "print('labels_train:', labels_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Train a new classifier with the frozen weights\n",
    "\n",
    "Now that we have the featurizer weights, we can:\n",
    "\n",
    "1. Create a general classifier.\n",
    "2. Train the classifier with the weights (X) and labels (y)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Create a general classifier\n",
    "\n",
    "Note that this classifier does not have to be related to the architecture of the original network.\n",
    "\n",
    "It simply treats the inputs as opaque features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Flatten, Dense, Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "classifier.add(Flatten(input_shape=features_train.shape[1:])) # flatten (5, 5, 1024) to vector\n",
    "classifier.add(Dense(64, activation='relu'))\n",
    "classifier.add(Dropout(0.5))\n",
    "classifier.add(Dense(3, activation='softmax'))\n",
    "\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: Train the classifier\n",
    "\n",
    "1. Setup a tensorboard callback\n",
    "\n",
    "```\n",
    "tensorboard = TensorBoard(log_dir='./logs/mobilenet_trf/%s' % log_index,\n",
    "                             histogram_freq=0, write_graph=True, write_images=False)\n",
    "```\n",
    "\n",
    "2. Setup an EarlyStopping callback to monitor the validation loss. \n",
    " - To understand what it does, try experimenting with these parameters or removing this callback to see what happens.\n",
    " - To disable this callback, you need to update the list of callbacks passed to `fit`\n",
    " - You should see overfitting if you disable the callback AND increase `epochs` for `fit`.\n",
    "\n",
    "```\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4, verbose=0, mode='auto')\n",
    "```\n",
    "\n",
    "3. Compile the model, then fit with the features and labels\n",
    "\n",
    "```\n",
    "classifier.compile(optimizer='rmsprop',\n",
    "                   loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "classifier.fit(x=features_train,\n",
    "               y=labels_train,\n",
    "               validation_split=0.2,\n",
    "               batch_size=batch_size,\n",
    "               epochs=10,\n",
    "               callbacks=[tensorboard, early_stopping])\n",
    "```\n",
    "\n",
    "If you run `fit` multiple times, you may notice that the model continues training from the previous call to `fit`.\n",
    " - To reset the model state, you may find it useful to just re-declare the model before calling `fit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Evaluating the transfer learnt model is not as straightforward as giving it the test inputs.\n",
    "\n",
    "We'll need to:\n",
    "1. Get the validation set features (and labels) from the MobileNet featurizer\n",
    "2. Pass the features into the classifier to get the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(val_folder,\n",
    "                                                  color_mode='rgb',\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=n_val_set,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)\n",
    "\n",
    "# Get the validation set features (and labels) from the MobileNet featurizer\n",
    "test_x, test_y = next(test_generator)\n",
    "test_features = featurizer.predict(test_x)\n",
    "\n",
    "# Pass the features into the classifier to get the predictions.\n",
    "pred_y = classifier.predict(test_features)\n",
    "\n",
    "# convert numbers into one-hot rows\n",
    "bins = np.array([0.5]) # x <= 0.5 becomes 0, else 1\n",
    "pred_y_onehot = np.digitize(pred_y, bins)\n",
    "\n",
    "# convert one-hot rows to label index\n",
    "pred_y_labels = pred_y_onehot.argmax(axis=1)\n",
    "\n",
    "# Evaluate metrics\n",
    "print(classification_report(test_y, pred_y_onehot))\n",
    "print(confusion_matrix(test_y.argmax(axis=1), pred_y_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If you are using the sample dataset, you should get classification metrics similar to this:\n",
    "\n",
    "```\n",
    "Found 15 images belonging to 3 classes.\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       0.71      1.00      0.83         5\n",
    "          1       1.00      1.00      1.00         5\n",
    "          2       1.00      0.60      0.75         5\n",
    "\n",
    "avg / total       0.90      0.87      0.86        15\n",
    "\n",
    "[[5 0 0]\n",
    " [0 5 0]\n",
    " [2 0 3]]\n",
    "```\n",
    "\n",
    "A much better improvement from the baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Fine-tuning the Featurizer\n",
    "\n",
    "To further improve the result, we can try to \"fine-tune\" the last few layers of the featurizer.\n",
    "\n",
    "Let's examine the architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# We can plot the featurizer architecture\n",
    "SVG(model_to_dot(featurizer, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We can also plot the classifier architecture \n",
    "SVG(model_to_dot(classifier, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### MobileNet Convolutional Block\n",
    "MobileNet is unique in that it's convolutional block is actually a Depthwise-separable block.\n",
    " - This speeds up the network and reduces the number of parameters it needs to store\n",
    " - This means that we should fine-tune the last `Depthwise Conv2D` and `Conv2D` blocks **as a unit**, because together they perform the standard convolution operation.\n",
    "\n",
    "![Depthwise Separable Conv Block](assets/transfer/depthwise_conv_block.png)\n",
    "\n",
    "(image: https://arxiv.org/abs/1704.04861)\n",
    "\n",
    "If you are interested, slides and demos on a talk on optimzed neural nets for mobile devices: https://github.com/lisaong/stackup-workshops/blob/master/ai-edge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "This picture shows the candidate layers to be fine-tuned (in yellow).\n",
    "\n",
    "![fine tune candidates](assets/transfer/mobilenet-finetune.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Construct Model for Fine-tuning\n",
    "Now that we've identified the candidate layers, the next steps are to:\n",
    "\n",
    "1. Create a new MobileNet featurizer with the ImageNet weights\n",
    "2. Append the classifier we trained earlier to the featurizer\n",
    "3. Freeze the featurizer layers that we don't want to fine-tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "\n",
    "# 1. Create a new MobileNet featurizer with the ImageNet weights\n",
    "mobilenet = MobileNet(include_top=False,\n",
    "                      weights='imagenet',\n",
    "                      input_shape=(img_width, img_height, channels))\n",
    "\n",
    "# 2. Append the classifier we trained earlier to the featurizer\n",
    "combined_model = Model(inputs=mobilenet.input,\n",
    "                       outputs=classifier(mobilenet.output))\n",
    "\n",
    "# 3. Freeze the featurizer layers that we don't want to fine-tune\n",
    "\n",
    "# first, we confirm these are the layers we want to keep unfrozen\n",
    "combined_model.layers[-7:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "To freeze the other layers, we set `layer[index].trainable = False`\n",
    "\n",
    "Once this is set, the model summary should show significantly fewer trainable parameters.\n",
    "\n",
    "Weights (if any) for these last 7 layers will be trainable:\n",
    "\n",
    "```\n",
    "conv_dw_13 (DepthwiseConv2D) (None, 5, 5, 1024)        9216      \n",
    "_________________________________________________________________\n",
    "conv_dw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
    "_________________________________________________________________\n",
    "conv_dw_13_relu (Activation) (None, 5, 5, 1024)        0         \n",
    "_________________________________________________________________\n",
    "conv_pw_13 (Conv2D)          (None, 5, 5, 1024)        1048576   \n",
    "_________________________________________________________________\n",
    "conv_pw_13_bn (BatchNormaliz (None, 5, 5, 1024)        4096      \n",
    "_________________________________________________________________\n",
    "conv_pw_13_relu (Activation) (None, 5, 5, 1024)        0         \n",
    "_________________________________________________________________\n",
    "sequential_10 (Sequential)   (None, 3)                 1638659   \n",
    "=================================================================\n",
    "Total params: 4,867,523\n",
    "Trainable params: 2,700,547\n",
    "Non-trainable params: 2,166,976\n",
    "_________________________________________________________________\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Freeze the other layers\n",
    "for layer in combined_model.layers[:-7]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "combined_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Setup Optimizer and Learning Rate for Fine-Tuning\n",
    "\n",
    "For fine-tuning, we want to keep the learning rate slow, and avoid aggressive optimizers:\n",
    "- Try RMSProp or SGD: https://keras.io/optimizers/\n",
    "- Slow learning rate that decays exponentially"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: Train the combined model\n",
    "\n",
    "Using the hints below, write the code to train the combined model.\n",
    "\n",
    "1. Setup data augmentation for the training set. The code should be similar to what was done for saving the featurizer weights. \n",
    "\n",
    "```\n",
    "# setup data augmentation in the same way as before\n",
    "train_datagen = ImageDataGenerator(...)\n",
    "\n",
    "# batch_size should be n_train_set\n",
    "# the other setting should be similar to before\n",
    "train_generator = train_datagen.flow_from_directory(...)\n",
    "\n",
    "# get the training set\n",
    "train_x, labels_train = next(train_generator)\n",
    "```\n",
    "\n",
    "2. Train the featurizer using SGD, with an exponentially decaying learning rate. Make sure that early stopping is more strict, because we don't want to overfit.\n",
    "\n",
    "```\n",
    "# compile the model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "combined_model.compile(loss='categorical_crossentropy',\n",
    "                       optimizer=SGD(lr=1e-4, decay=0.9),\n",
    "                       metrics=['accuracy'])\n",
    "\n",
    "# setup tensorboard to track our logs in a separate folder for easy filtering\n",
    "# the other settings should be similar to fitting the classifier\n",
    "tensorboard = TensorBoard(log_dir='./logs/mobilenet_finetune/%s' % log_index, ...)\n",
    "\n",
    "# let's be even more strict with early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=0,\n",
    "                               verbose=0, mode='auto')\n",
    "\n",
    "# finally, fine-tune the model.\n",
    "# the other settings should be similar to fitting the classifier\n",
    "# Note: since the learning rate is slow, we will need to increase the number of epochs\n",
    "combined_model.fit(x=train_x,\n",
    "                   y=labels_train,\n",
    "                   epochs=30.\n",
    "                   ...)\n",
    "```\n",
    "\n",
    "Note: you do not need to call `next(test_generator)` or `next(train_generator)` when using `fit_generator` for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Evaluate the Combined Model\n",
    "\n",
    "Last, but not the least, evalute the classification metrics for the combined model.\n",
    "\n",
    "- Unlike the first Transfer Learning case, we do not need to pre-process the test samples through the featurizer and then the classifier.\n",
    "- We can pass the test samples directly into the combined model.\n",
    "\n",
    "If you were using the sample data, you should see a remarkable improvement in the F1 score:\n",
    "\n",
    "```\n",
    "Found 15 images belonging to 3 classes.\n",
    "             precision    recall  f1-score   support\n",
    "\n",
    "          0       1.00      1.00      1.00         5\n",
    "          1       0.83      1.00      0.91         5\n",
    "          2       1.00      0.80      0.89         5\n",
    "\n",
    "avg / total       0.94      0.93      0.93        15\n",
    "\n",
    "[[5 0 0]\n",
    " [0 5 0]\n",
    " [0 1 4]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "test_generator = test_datagen.flow_from_directory(val_folder,\n",
    "                                                  color_mode='rgb',\n",
    "                                                  target_size=(img_height, img_width),\n",
    "                                                  batch_size=n_val_set,\n",
    "                                                  class_mode='categorical',\n",
    "                                                  shuffle=False)\n",
    "\n",
    "# Get the validation set features (and labels) from the MobileNet featurizer\n",
    "test_x, test_y = next(test_generator)\n",
    "\n",
    "# Pass the features into the combined model to get the predictions.\n",
    "pred_y = combined_model.predict(test_x)\n",
    "\n",
    "# convert numbers into one-hot rows\n",
    "bins = np.array([0.5]) # x <= 0.5 becomes 0, else 1\n",
    "pred_y_onehot = np.digitize(pred_y, bins)\n",
    "\n",
    "# convert one-hot rows to label index\n",
    "pred_y_labels = pred_y_onehot.argmax(axis=1)\n",
    "\n",
    "# Evaluate metrics\n",
    "print(classification_report(test_y, pred_y_onehot))\n",
    "print(confusion_matrix(test_y.argmax(axis=1), pred_y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "This has been another looong workshop. It's easy to get lost in what we're trying to do.\n",
    "\n",
    "#### Start\n",
    "We started with a pre-trained MobileNet neural net.\n",
    "\n",
    "#### Objective\n",
    "Adapt it to a new dataset and a new task (new classes).\n",
    "\n",
    "#### Process\n",
    "1. Train a baseline classifier using a 4-layer Convolutional Neural Network\n",
    "\n",
    "2. Transfer learning stage 1\n",
    " - Partition the MobileNet CNN into featurizer and classifier.\n",
    " - Freeze the featurizer weights.\n",
    " - Train a new classifier (transfer-learning classifier) using the featurizer weights as inputs.\n",
    "\n",
    "3. Transfer learning stage 2: fine-tuning\n",
    " - Unfreeze the last convolutional block of the featurizer\n",
    " - Add the transfer-learning classifier to the featurizer to make a combined model\n",
    " - Re-train the combined model with a very slow learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading List\n",
    "\n",
    "|Material|Read it for|URL|\n",
    "|--|--|--|\n",
    "|A Survey on Transfer Learning (IEEE)|Overview of transfer learning|http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.147.9185&rep=rep1&type=pdf|\n",
    "|Transductive Learning: Motivation, Model, Algorithms|Explanation of Induction vs. Transduction Transfer Learning|http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/pdfs/pdf2527.pdf|\n",
    "|Supervised and Unsupervised Transfer Learning for Question Answering (Paper)|More unique application of transfer learning|https://arxiv.org/abs/1711.05345|\n",
    "|Building powerful image classification models using very little data|More walkthroughs and explanations of this workshop (we used this as the reference tutorial)|https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
