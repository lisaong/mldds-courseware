{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Neural Networks: General-purpose learning algorithm for modeling non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... if you train it with \"enough\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear inputs\n",
    "\n",
    "- Images\n",
    "- Text\n",
    "- Speech\n",
    "- XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of linear models\n",
    "\n",
    "Not \"linearly separable\"\n",
    "\n",
    "![xor](assets/neural/xor.png)\n",
    "\n",
    "Can't draw boundary to separate x's and o's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling non-linearity\n",
    "\n",
    "Transform $x$ into $\\phi(x)$ to become linearly separable\n",
    "\n",
    "![xor](assets/neural/xor_phi.png)\n",
    "\n",
    "$\\phi(x)$ is the basis for a \"neuron\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuron\n",
    "\n",
    "$$y = W\\phi(x) + b$$\n",
    "\n",
    "$$\\phi(x) = g(W'x + b')$$\n",
    "\n",
    "Trainable: $W', b', W, b$\n",
    "\n",
    "$g(x)$ is a non-linear function, e.g. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuron (Perceptron)\n",
    "\n",
    "![neuron](assets/neural/neuron.png)\n",
    "\n",
    "(image: Neural Network Methods in Natural Language Processing, Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "Multiple neurons in 1 layer make up an \"Artificial Neural Network\"\n",
    "\n",
    "![neural network](assets/neural/300px-Colored_neural_network.svg.png)\n",
    "\n",
    "(image: [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network (Deep)\n",
    "\n",
    "Multiple \"hidden\" layers of neurons make up a \"Deep Neural Network\"\n",
    "\n",
    "![multi-layer perceptron](assets/neural/deep_nn.png)\n",
    "\n",
    "(image: Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of a Neural Network\n",
    "\n",
    "|Term|Description|Examples|\n",
    "|--|--|--|\n",
    "|Input dimension|How many inputs|4|\n",
    "|Output dimension|How many outputs|3|\n",
    "|Number of hidden layers|Number of layers, excluding input and output|2|\n",
    "|Activation type|Type of non-linear function|sigmoid, ReLU, tanh|\n",
    "|Hidden layer type|How the neurons are connected together|Fully-connected, Convolutional|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation types\n",
    "\n",
    "What non-linearity is applied\n",
    "\n",
    "![dnn](assets/neural/activations.png)\n",
    "\n",
    "(image: Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer types\n",
    "\n",
    "How the neurons are connected together, and what operations are performed with x, W, and b:\n",
    "\n",
    "- Dense\n",
    "- Convolutional\n",
    "- Recurrent\n",
    "- Residual\n",
    "\n",
    "More detail to come..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: Neural Network Architectures in keras\n",
    "\n",
    "In this walkthrough, we will use Keras to examine the architecture of some well-known neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup - Graphviz\n",
    "\n",
    "We'll be installing Graphviz for visualizing the architectures.\n",
    "\n",
    "1. Download and install graphviz binaries from: https://graphviz.gitlab.io/download/\n",
    "2. Add the path to graphviz to your PATH environment variable, e.g. `C:/Program Files (x86)/Graphviz2.38/bin`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup - Conda environment\n",
    "\n",
    "1. Create a new conda environment called `mldds03`\n",
    "  a. Launch an `Anaconda Python` command window\n",
    "  b. `conda create -n mldds03 python=3`\n",
    "2. Activate the conda environment: `conda activate mldds03`\n",
    "3. Install: `conda install jupyter keras pydot`\n",
    "4. Navigate to the courseware folder: `cd mldds-courseware`\n",
    "5. Launch Jupyter: `jupyter notebook` and open this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained Neural Networks in Keras\n",
    "\n",
    "\"Pre-trained\" neural networks are available under `keras.applications`\n",
    "\n",
    "https://keras.io/applications/\n",
    "\n",
    "These are trained on the ImageNet dataset (http://www.image-net.org/), which contains millions of images.\n",
    "\n",
    "The neural network architectures from keras are previous years submissions to the ImageNet annual challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MobileNet\n",
    "\n",
    "MobileNet is a pre-trained ImageNet DNN optimized to run on smaller devices.\n",
    "\n",
    "Keras documentation: https://keras.io/applications/#mobilenet\n",
    "\n",
    "You can find the URL to the original research paper that proposed this network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import mobilenet\n",
    "\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet')\n",
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### ResNet50\n",
    "\n",
    "ResNet50 is another pre-trained ImageNet DNN. This is a larger network than MobileNet (almost 26 million parameters). It improves accuracy by introducing residual connections, which are connections that skip layers.\n",
    "\n",
    "https://keras.io/applications/#resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Creating Neural Networks using Keras\n",
    "Finally, let's try something simpler.\n",
    "\n",
    "Let's create a 1-layer network that can do linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Reference: https://gist.github.com/fchollet/b7507f373a3446097f26840330c1c378\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "simple_model = Sequential()\n",
    "simple_model.add(Dense(1, input_dim=4, activation='sigmoid')) # 4 inputs, 1 output\n",
    "simple_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.Model.compile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How about a 2-layer network to make it a deep neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "\n",
    "deeper_model = Sequential()\n",
    "\n",
    "# imagine we tuned hyperparamters and derived this magic architecture to solve all our problems\n",
    "deeper_model.add(Dense(12, input_dim=42, activation='relu')) # 42 inputs, 12 outputs\n",
    "deeper_model.add(Dense(4, activation='relu')) # 12 inputs, 4 outputs\n",
    "deeper_model.add(Activation(\"softmax\"))\n",
    "deeper_model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "\n",
    "deeper_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Visualizing Neural Net Architectures in Keras\n",
    "\n",
    "https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "model_to_dot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(simple_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(deeper_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(mobilenet_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(resnet_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "A neural network is trained using:\n",
    "- Stochastic Gradient Descent (we covered this in Training Basics)\n",
    "- Back Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Back Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "|Material|Read it for|URL\n",
    "|--|--|--|\n",
    "|Lecture 1: Deep Learning Challenge. Is There Theory?|Intro to Deep Learning|https://stats385.github.io/lecture_slides (lecture 1)|\n",
    "|Lecture 2: Overview of Deep Learning from a Practical Point of View|More background on Neural Nets|https://stats385.github.io/lecture_slides (lecture 2)|\n",
    "|Guide to the Sequential Model|Basic usage of Keras for neural net training|https://keras.io/getting-started/sequential-model-guide/|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
