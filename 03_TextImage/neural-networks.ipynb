{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivation\n",
    "\n",
    "Neural Networks: General-purpose learning algorithm for modeling non-linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... if you train it with \"enough\" data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Non-linear inputs\n",
    "\n",
    "- Images\n",
    "- Text\n",
    "- Speech\n",
    "- XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Limitations of linear models\n",
    "\n",
    "Not \"linearly separable\"\n",
    "\n",
    "![xor](assets/neural/xor.png)\n",
    "\n",
    "Can't draw boundary to separate x's and o's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modeling non-linearity\n",
    "\n",
    "Transform $x$ into $\\phi(x)$ to become linearly separable\n",
    "\n",
    "![xor](assets/neural/xor_phi.png)\n",
    "\n",
    "$\\phi(x)$ is the basis for a \"neuron\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuron\n",
    "\n",
    "$$y = W\\phi(x) + b$$\n",
    "\n",
    "$$\\phi(x) = g(W'x + b')$$\n",
    "\n",
    "Trainable: $W', b', W, b$\n",
    "\n",
    "$g(x)$ is a non-linear function, e.g. Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuron (Perceptron)\n",
    "\n",
    "![neuron](assets/neural/neuron.png)\n",
    "\n",
    "(image: Neural Network Methods in Natural Language Processing, Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network\n",
    "\n",
    "Multiple neurons in 1 layer make up an \"Artificial Neural Network\"\n",
    "\n",
    "![neural network](assets/neural/300px-Colored_neural_network.svg.png)\n",
    "\n",
    "(image: [Wikipedia](https://en.wikipedia.org/wiki/Artificial_neural_network))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neural Network (Deep)\n",
    "\n",
    "Multiple \"hidden\" layers of neurons make up a \"Deep Neural Network\"\n",
    "\n",
    "![multi-layer perceptron](assets/neural/deep_nn.png)\n",
    "\n",
    "(image: Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Properties of a Neural Network\n",
    "\n",
    "|Term|Description|Examples|\n",
    "|--|--|--|\n",
    "|Input dimension|How many inputs|4|\n",
    "|Output dimension|How many outputs|3|\n",
    "|Number of hidden layers|Number of layers, excluding input and output|2|\n",
    "|Activation type|Type of non-linear function|sigmoid, ReLU, tanh|\n",
    "|Hidden layer type|How the neurons are connected together|Fully-connected, Convolutional|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation types\n",
    "\n",
    "What non-linearity is applied\n",
    "\n",
    "![dnn](assets/neural/activations.png)\n",
    "\n",
    "(image: Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer types\n",
    "\n",
    "How the neurons are connected together, and what operations are performed with x, W, and b:\n",
    "\n",
    "- Dense\n",
    "- Convolutional\n",
    "- Recurrent\n",
    "- Residual\n",
    "\n",
    "More detail to come..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough: Neural Network Architectures in keras\n",
    "\n",
    "In this walkthrough, we will use Keras to examine the architecture of some well-known neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Setup - Conda environment\n",
    "\n",
    "1. Create a new conda environment called `mldds03`\n",
    "  a. Launch an `Anaconda Python` command window\n",
    "  b. `conda create -n mldds03 python=3`\n",
    "2. Activate the conda environment: `conda activate mldds03`\n",
    "3. Install: `conda install jupyter numpy pandas matplotlib keras pydot python-graphviz`\n",
    "4. Navigate to the courseware folder: `cd mldds-courseware`\n",
    "5. Launch Jupyter: `jupyter notebook` and open this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Pre-trained Neural Networks in Keras\n",
    "\n",
    "\"Pre-trained\" neural networks are available under `keras.applications`\n",
    "\n",
    "https://keras.io/applications/\n",
    "\n",
    "These are trained on the ImageNet dataset (http://www.image-net.org/), which contains millions of images.\n",
    "\n",
    "The neural network architectures from keras are previous years submissions to the ImageNet annual challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MobileNet\n",
    "\n",
    "MobileNet is a pre-trained ImageNet DNN optimized to run on smaller devices.\n",
    "\n",
    "Documentation: https://keras.io/applications/#mobilenet\n",
    "\n",
    "Implementation: https://github.com/keras-team/keras-applications/blob/master/keras_applications/mobilenet.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import mobilenet\n",
    "\n",
    "mobilenet_model = mobilenet.MobileNet(weights='imagenet')\n",
    "mobilenet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### ResNet50\n",
    "\n",
    "ResNet50 is another pre-trained ImageNet DNN. This is a larger network than MobileNet (almost 26 million parameters). It improves accuracy by introducing residual connections, which are connections that skip layers.\n",
    "\n",
    "Documentation: https://keras.io/applications/#resnet50\n",
    "\n",
    "Implementation: https://github.com/keras-team/keras-applications/blob/master/keras_applications/resnet50.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications import resnet50\n",
    "\n",
    "resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Creating Neural Networks using Keras\n",
    "Finally, let's try something simpler.\n",
    "\n",
    "Let's create a 1-layer network that can do linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Reference: https://gist.github.com/fchollet/b7507f373a3446097f26840330c1c378\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "simple_model = Sequential()\n",
    "simple_model.add(Dense(1, input_dim=4, activation='sigmoid')) # 4 inputs, 1 output\n",
    "simple_model.compile(optimizer='rmsprop', loss='mse')\n",
    "\n",
    "simple_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.Sequential?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.layers.Dense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.Model.compile?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "How about a 2-layer network to make it a deep neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from keras.layers import Activation\n",
    "\n",
    "deeper_model = Sequential()\n",
    "\n",
    "# imagine we tuned hyperparamters and derived this magic architecture to solve all our problems\n",
    "deeper_model.add(Dense(12, input_dim=42, activation='relu')) # 42 inputs, 12 outputs\n",
    "deeper_model.add(Dense(4, activation='relu')) # 12 inputs, 4 outputs\n",
    "deeper_model.add(Activation(\"softmax\"))\n",
    "deeper_model.compile(optimizer='rmsprop', loss='binary_crossentropy')\n",
    "\n",
    "deeper_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Visualizing Neural Net Architectures in Keras\n",
    "\n",
    "https://keras.io/visualization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "model_to_dot?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVG(model_to_dot(simple_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(deeper_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(mobilenet_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "SVG(model_to_dot(resnet_model, show_shapes=True).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Troubleshooting: Graphviz Installation\n",
    "\n",
    "If pydot is not able to find `graphviz`, you can try installing graphviz manually.\n",
    "\n",
    "1. Download and install graphviz binaries from: https://graphviz.gitlab.io/download/\n",
    "2. Add the path to graphviz to your PATH environment variable, e.g. `C:/Program Files (x86)/Graphviz2.38/bin`\n",
    "3. Launch a new `Anaconda Prompt` and re-run the Jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training a neural network\n",
    "\n",
    "A neural network is trained using Stochastic Gradient Descent\n",
    "  - Forward Propagation to compute the output at each layer\n",
    "  - Back Propagation to compute gradients\n",
    "  - Update weights and biases using gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Propagation\n",
    "\n",
    "For 1 neuron:\n",
    "\n",
    "$$y = W'g(Wx + b) + b'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Propagation\n",
    "\n",
    "2 layers of neurons:\n",
    "\n",
    "$$x_1 = W_1'g(W_1x + b_1) + b_1'$$\n",
    "\n",
    "$$y = x_2 = W_2'g(W_2x_1 + b_2) + b_2'$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Forward Propagation\n",
    "\n",
    "For layer $l$, single layer operation:\n",
    "\n",
    "$$x_l = \\sigma_l(W_lx_{l-1} + b_1)$$\n",
    "\n",
    "where $\\sigma_l(z) = W_l'g(z) + b_l'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feedforward through Layers\n",
    "\n",
    "for $l = 1$ to $\\,L$:\n",
    "\n",
    "$\\,\\,\\,\\,x_l = \\sigma_l(W_lx_{l-1} + b_l)$\n",
    "\n",
    "Where:\n",
    "- Number of layers: $L$\n",
    "- Input: $x_0$, Output: $x_L$\n",
    "- Note: $x_l$ are tensors with the input & output dimensions of that layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "\n",
    "Objective\n",
    "- Compute the gradients of the cost function $J$ w.r.t. to $W^j_l$ and $b^j_l$ (layer $l$, neuron $j$)\n",
    "  - Partial derivatives $\\frac{\\partial J}{\\partial W^j_l}$, $\\frac{\\partial J}{\\partial b^j_l}$ \n",
    "- E.g. quadratic cost function, $n$ training samples, output $x_L$:\n",
    "$$J({W_l},{b_l}) = \\frac{1}{2n}\\sum_{i=1}^n {\\|y^i - x_{L}^i\\|}^2$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Backward Propagation\n",
    "\n",
    "1. Feedforward from layer 1 to L\n",
    "2. Compute the output error vector at layer L ($\\delta_L$)\n",
    "3. Backward propagate the error (backwards from layer L-1, .. 1) to compute per-layer error vectors ($\\delta_l$)\n",
    "4. Compute gradient of cost function for layer $l$, neuron $j$:\n",
    "$$\\frac{\\partial J}{\\partial W_l^j} = x_{l-1}^j\\delta_l^j$$\n",
    "$$\\frac{\\partial J}{\\partial b_l^j} = \\delta_l^j$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent Update Rule\n",
    "\n",
    "$$W_l^j := W_l^j + \\epsilon \\frac{\\partial J}{W_l^j}$$\n",
    "\n",
    "$$b_l^j := b_l^j + \\epsilon \\frac{\\partial J}{b_l^j}$$\n",
    "\n",
    "$\\epsilon$ = learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Neural Network for Logistic Regression\n",
    "\n",
    "In this workshop, you'll train a neural network to perform logistic regression on the MNIST dataset.\n",
    "\n",
    "Credits: https://medium.com/@the1ju/simple-logistic-regression-using-keras-249e0cc9a970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Training settings\n",
    "BATCH_SIZE = 128\n",
    "NUM_CLASSES = 10\n",
    "EPOCHS = 30\n",
    "\n",
    "# Input size settings\n",
    "IMG_ROWS = 28 # 28 pixels wide\n",
    "IMG_COLS = 28 # 28 pixels high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Import the dataset, split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Input processing\n",
    "input_dim = IMG_ROWS * IMG_COLS\n",
    "X_train = X_train.reshape(60000, input_dim) \n",
    "X_test = X_test.reshape(10000, input_dim) \n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# We are doing multi-class classification\n",
    "# Convert class vectors to binary class matrices\n",
    "y_train_cat = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_test_cat = keras.utils.to_categorical(y_test, NUM_CLASSES)\n",
    "\n",
    "# Show how the classes look like\n",
    "print('y_train shape:', y_train_cat.shape)\n",
    "print('First y_train sample:', y_train_cat[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Create the model\n",
    "model = Sequential() \n",
    "model.add(Dense(NUM_CLASSES, input_dim=input_dim, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise: Training the Neural Network\n",
    "\n",
    "1. Compile the model\n",
    "2. Train the model using `sgd`, minibatch size 128 \n",
    "  - Training set: `X_train`, `y_train_cat`\n",
    "  - Test set: `X_test`, `y_test_cat`\n",
    "3. Plot the learning curve, using the accuracy metrics\n",
    "4. Analyze the learning curve to determine if overfitting or underfitting occurred. \n",
    "  - If overfitting occurred, which epoch can training stop\n",
    "  - If underfitting occurred, train more epochs to determine what the optimum number of epochs should be\n",
    "\n",
    "How to get the accuracy metrics:\n",
    "```\n",
    "history = model.fit(..., metrics=['accuracy'])\n",
    "...\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "```\n",
    "\n",
    "You may reference this example for steps 1 and 2: https://medium.com/@the1ju/simple-logistic-regression-using-keras-249e0cc9a970"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Compile and train model\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot learning curve\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Reading List\n",
    "\n",
    "|Material|Read it for|URL\n",
    "|--|--|--|\n",
    "|Lecture 1: Deep Learning Challenge. Is There Theory?|Intro to Deep Learning|https://stats385.github.io/lecture_slides (lecture 1)|\n",
    "|Lecture 2: Overview of Deep Learning from a Practical Point of View|More background on Neural Nets|https://stats385.github.io/lecture_slides (lecture 2)|\n",
    "|Neural Networks and Deep Learning, Chapter 2|Understanding Back Propagation|http://neuralnetworksanddeeplearning.com/chap2.html|\n",
    "|Guide to the Sequential Model|Basic usage of Keras for neural net training|https://keras.io/getting-started/sequential-model-guide/|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
