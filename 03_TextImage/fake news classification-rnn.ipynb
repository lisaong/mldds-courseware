{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Nets - Fake News\n",
    "\n",
    "The RNN (LSTM) architechture that we are using is shown below, a many to one RNN.\n",
    "\n",
    "![RNNs](./images/manytoone.jpg)\n",
    "\n",
    "<img src='https://media.giphy.com/media/l0Iyau7QcKtKUYIda/giphy.gif'>\n",
    "\n",
    "We achieve 87% accuracy in a test set. However, the article in Second reference claims to have 93% accuracy. The main difference is that they seem to use a Bag of Words Model, which loses the order of words when sending into the ML algorithm. Also\n",
    "\n",
    "## References:\n",
    "1. Data: https://github.com/GeorgeMcIntire/fake_real_news_dataset\n",
    "2. Classification using Scikit Learn: https://blog.kjamistan.com/comparing-scikit-learn-text-classifiers-on-a-fake-news-dataset/\n",
    "3. Glove vectors: https://nlp.stanford.edu/projects/glove/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /srv/venv/lib/python3.6/site-packages\n",
      "Collecting Pillow\n",
      "  Downloading Pillow-5.0.0-cp36-cp36m-manylinux1_x86_64.whl (5.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 5.9MB 221kB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: Pillow\n",
      "Successfully installed Pillow-5.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm\n",
    "# !conda install -y Pillow\n",
    "!pip install Pillow\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization, LSTM, Embedding, Reshape\n",
    "from keras.models import load_model, model_from_json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from os import mkdir, makedirs, remove, listdir\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# from utilties import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utilties import *\n",
    "\n",
    "from os.path import isfile, isdir, getsize\n",
    "from tqdm import tqdm\n",
    "import zipfile\n",
    "from urllib.request import urlretrieve\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "\n",
    "def downloadData(file, url):        \n",
    "    if not isfile(file):\n",
    "        with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Fake News Dataset') as pbar:\n",
    "            urlretrieve(url, file, pbar.hook)\n",
    "\n",
    "    with zipfile.ZipFile(file) as f:\n",
    "        f.extractall('./data/')\n",
    "        \n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'data'\n",
    "\n",
    "file = './data/fakenews.zip'\n",
    "url = 'https://github.com/GeorgeMcIntire/fake_real_news_dataset/raw/master/fake_or_real_news.csv.zip'\n",
    "downloadData(file, url)\n",
    "\n",
    "#################################    \n",
    "# Download GLOVE vector dataset\n",
    "#################################\n",
    "file = './data/glove.6B.zip'\n",
    "url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "downloadData(file, url)\n",
    "\n",
    "with open('./data/glove.6B.50d.txt','rb') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "glove_weights = np.zeros((len(lines), 50))\n",
    "words = []\n",
    "for i, line in enumerate(lines):\n",
    "    word_weights = line.split()\n",
    "    words.append(word_weights[0])\n",
    "    weight = word_weights[1:]\n",
    "    glove_weights[i] = np.array([float(w) for w in weight])\n",
    "word_vocab = [w.decode(\"utf-8\") for w in words]\n",
    "\n",
    "word2glove = dict(zip(word_vocab, glove_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing steps: lower case, remove urls, some punctuations etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.engine.topology import Layer\n",
    "import keras.backend as K\n",
    "from keras import initializers\n",
    "import numpy as np\n",
    "\n",
    "class Embedding2(Layer):\n",
    "\n",
    "    def __init__(self, input_dim, output_dim, fixed_weights, embeddings_initializer='uniform', \n",
    "                 input_length=None, **kwargs):\n",
    "        kwargs['dtype'] = 'int32'\n",
    "        if 'input_shape' not in kwargs:\n",
    "            if input_length:\n",
    "                kwargs['input_shape'] = (input_length,)\n",
    "            else:\n",
    "                kwargs['input_shape'] = (None,)\n",
    "        super(Embedding2, self).__init__(**kwargs)\n",
    "    \n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embeddings_initializer = embeddings_initializer\n",
    "        self.fixed_weights = fixed_weights\n",
    "        self.num_trainable = input_dim - len(fixed_weights)\n",
    "        self.input_length = input_length\n",
    "        \n",
    "        w_mean = fixed_weights.mean(axis=0)\n",
    "        w_std = fixed_weights.std(axis=0)\n",
    "        self.variable_weights = w_mean + w_std*np.random.randn(self.num_trainable, output_dim)\n",
    "\n",
    "    def build(self, input_shape, name='embeddings'):        \n",
    "        fixed_weight = K.variable(self.fixed_weights, name=name+'_fixed')\n",
    "        variable_weight = K.variable(self.variable_weights, name=name+'_var')\n",
    "        \n",
    "        self._trainable_weights.append(variable_weight)\n",
    "        self._non_trainable_weights.append(fixed_weight)\n",
    "        \n",
    "        self.embeddings = K.concatenate([fixed_weight, variable_weight], axis=0)\n",
    "        \n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if K.dtype(inputs) != 'int32':\n",
    "            inputs = K.cast(inputs, 'int32')\n",
    "        out = K.gather(self.embeddings, inputs)\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if not self.input_length:\n",
    "            input_length = input_shape[1]\n",
    "        else:\n",
    "            input_length = self.input_length\n",
    "        return (input_shape[0], input_length, self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6335, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you can smell hillarys fear</td>\n",
       "      <td>daniel greenfield a shillman journalism fellow...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>watch the exact moment paul ryan committed pol...</td>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>kerry to go to paris in gesture of sympathy</td>\n",
       "      <td>u . s . secretary of state john f . kerry said...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bernie supporters on twitter erupt in anger ag...</td>\n",
       "      <td>kaydee king kaydeeking november 9 2016 the les...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the battle of new york why this primary matters</td>\n",
       "      <td>its primary day in new york and frontrunners h...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0                        you can smell hillarys fear   \n",
       "1  watch the exact moment paul ryan committed pol...   \n",
       "2        kerry to go to paris in gesture of sympathy   \n",
       "3  bernie supporters on twitter erupt in anger ag...   \n",
       "4    the battle of new york why this primary matters   \n",
       "\n",
       "                                                text label  \n",
       "0  daniel greenfield a shillman journalism fellow...  FAKE  \n",
       "1  google pinterest digg linkedin reddit stumbleu...  FAKE  \n",
       "2  u . s . secretary of state john f . kerry said...  REAL  \n",
       "3  kaydee king kaydeeking november 9 2016 the les...  FAKE  \n",
       "4  its primary day in new york and frontrunners h...  REAL  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/fake_or_real_news.csv')\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "df.title = df.title.str.lower()\n",
    "df.text = df.text.str.lower()\n",
    "\n",
    "df.title = df.title.str.replace(r'http[\\w:/\\.]+','<URL>') # remove urls\n",
    "df.text = df.text.str.replace(r'http[\\w:/\\.]+','<URL>') # remove urls\n",
    "df.title = df.title.str.replace(r'[^\\.\\w\\s]','') #remove everything but characters and punctuation\n",
    "df.text = df.text.str.replace(r'[^\\.\\w\\s]','') #remove everything but characters and punctuation\n",
    "df.title = df.title.str.replace(r'\\.\\.+','.') #replace multple periods with a single one\n",
    "df.text = df.text.str.replace(r'\\.\\.+','.') #replace multple periods with a single one\n",
    "df.title = df.title.str.replace(r'\\.',' . ') #replace periods with a single one\n",
    "df.text = df.text.str.replace(r'\\.',' . ') #replace multple periods with a single one\n",
    "df.title = df.title.str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
    "df.text = df.text.str.replace(r'\\s\\s+',' ') #replace multple white space with a single one\n",
    "df.title = df.title.str.strip() \n",
    "df.text = df.text.str.strip() \n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the unique words. We will only consider words that have been used more than 5 times. Finally from this we create a dictionary mapping words to integers.\n",
    "\n",
    "Once this is done we will create a list of reviews where the words are converted to ints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of unique words in glove vectors:  0.668536737092\n",
      "The number of unique words are:  86806\n",
      "The first review looks like this: \n",
      "[4447, 12884, 5, 59768, 2855, 1229, 24, 0, 573, 483, 8, 5, 56, 243, 2518, 2856, 10, 1455, 1642, 1]\n",
      "And once this is converted back to words, it looks like: \n",
      "daniel greenfield a <Other> journalism fellow at the freedom center is a new york writer focusing on radical islam .\n"
     ]
    }
   ],
   "source": [
    "all_text = ' '.join(df.text.values)\n",
    "words = all_text.split()\n",
    "u_words = Counter(words).most_common()\n",
    "u_words_counter = u_words\n",
    "u_words_frequent = [word[0] for word in u_words if word[1]>5] # we will only consider words that have been used more than 5 times\n",
    "\n",
    "u_words_total = [k for k,v in u_words_counter]\n",
    "word_vocab = dict(zip(word_vocab, range(len(word_vocab))))\n",
    "word_in_glove = np.array([w in word_vocab for w in u_words_total])\n",
    "\n",
    "words_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if is_true]\n",
    "words_not_in_glove = [w for w,is_true in zip(u_words_total,word_in_glove) if not is_true]\n",
    "\n",
    "print('Fraction of unique words in glove vectors: ', sum(word_in_glove)/len(word_in_glove))\n",
    "\n",
    "# # create the dictionary\n",
    "word2num = dict(zip(words_in_glove,range(len(words_in_glove))))\n",
    "len_glove_words = len(word2num)\n",
    "freq_words_not_glove = [w for w in words_not_in_glove if w in u_words_frequent]\n",
    "b = dict(zip(freq_words_not_glove,range(len(word2num), len(word2num)+len(freq_words_not_glove))))\n",
    "word2num = dict(**word2num, **b)\n",
    "word2num['<Other>'] = len(word2num)\n",
    "num2word = dict(zip(word2num.values(), word2num.keys()))\n",
    "\n",
    "int_text = [[word2num[word] if word in word2num else word2num['<Other>'] \n",
    "             for word in content.split()] for content in df.text.values]\n",
    "\n",
    "print('The number of unique words are: ', len(u_words))\n",
    "print('The first review looks like this: ')\n",
    "print(int_text[0][:20])\n",
    "print('And once this is converted back to words, it looks like: ')\n",
    "print(' '.join([num2word[i] for i in int_text[0][:20]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f402f7c7860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([len(t) for t in int_text],50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of articles greater than 500 in length is:  3714\n",
      "The number of articles less than 50 in length is:  440\n"
     ]
    }
   ],
   "source": [
    "print('The number of articles greater than 500 in length is: ', np.sum(np.array([len(t)>500 for t in int_text])))\n",
    "print('The number of articles less than 50 in length is: ', np.sum(np.array([len(t)<50 for t in int_text])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot pass differing lengths of sentences to the algorithm. Hence we shall prepad the sentence with `<PAD>`. Sequences less than 500 in length will be prepadded and sequences that are longer than 500 will be truncated. It is assumed that the sentiment of the review can be asserted from the first 500 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2word[len(word2num)] = '<PAD>'\n",
    "word2num['<PAD>'] = len(word2num)\n",
    "\n",
    "for i, t in enumerate(int_text):\n",
    "    if len(t)<500:\n",
    "        int_text[i] = [word2num['<PAD>']]*(500-len(t)) + t\n",
    "    elif len(t)>500:\n",
    "        int_text[i] = t[:500]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "x = np.array(int_text)\n",
    "y = (df.label.values=='REAL').astype('int')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A real news article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'u . s . secretary of state john f . kerry said monday that he will stop in paris later this week amid criticism that no top american officials attended sundays unity march against terrorism . kerry said he expects to arrive in paris thursday evening as he heads home after a week abroad . he said he will fly to france at the conclusion of a series of meetings scheduled for thursday in sofia bulgaria . he plans to meet the next day with foreign minister laurent fabius and president francois hollande then return to washington . the visit by kerry who has family and childhood ties to the country and speaks fluent french could address some of the criticism that the united states snubbed france in its darkest hour in many years . the french press on monday was filled with questions about why neither president obama nor kerry attended sundays march as about 40 leaders of other nations did . obama was said to have stayed away because his own security needs can be taxing on a country and kerry had prior commitments . among roughly 40 leaders who did attend was israeli prime minister benjamin netanyahu no stranger to intense security who marched beside hollande through the city streets . the highest ranking u . s . officials attending the march were jane hartley the ambassador to france and victoria nuland the assistant secretary of state for european affairs . attorney general eric h . holder jr . was in paris for meetings with law enforcement officials but did not participate in the march . kerry spent sunday at a business summit hosted by indias prime minister narendra modi . the united states is eager for india to relax stringent laws that function as barriers to foreign investment and hopes modis government will act to open the huge indian market for more american businesses . in a news conference kerry brushed aside criticism that the united states had not sent a more senior official to paris as quibbling a little bit . he noted that many staffers of the american embassy in paris attended the march including the ambassador . he said he had wanted to be present at the march himself but could not because of his prior commitments in india . but that is why i am going there on the way home to make it crystal clear how passionately we feel about the events that have taken place there he said . and i dont think the people of france have any doubts about americas understanding of what happened of our personal sense of loss and our deep commitment to the people of france in this moment of trauma .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label=='REAL'].text.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fake news article:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'daniel greenfield a shillman journalism fellow at the freedom center is a new york writer focusing on radical islam . in the final stretch of the election hillary rodham clinton has gone to war with the fbi . the word unprecedented has been thrown around so often this election that it ought to be retired . but its still unprecedented for the nominee of a major political party to go war with the fbi . but thats exactly what hillary and her people have done . coma patients just waking up now and watching an hour of cnn from their hospital beds would assume that fbi director james comey is hillarys opponent in this election . the fbi is under attack by everyone from obama to cnn . hillarys people have circulated a letter attacking comey . there are currently more media hit pieces lambasting him than targeting trump . it wouldnt be too surprising if the clintons or their allies were to start running attack ads against the fbi . the fbis leadership is being warned that the entire leftwing establishment will form a lynch mob if they continue going after hillary . and the fbis credibility is being attacked by the media and the democrats to preemptively head off the results of the investigation of the clinton foundation and hillary clinton . the covert struggle between fbi agents and obamas doj people has gone explosively public . the new york times has compared comey to j . edgar hoover . its bizarre headline james comey role recalls hoovers fbi fairly or not practically admits up front that its spouting nonsense . the boston globe has published a column calling for comeys resignation . not to be outdone time has an editorial claiming that the scandal is really an attack on all women . james carville appeared on msnbc to remind everyone that he was still alive and insane . he accused comey of coordinating with house republicans and the kgb . and you thought the vast right wing conspiracy was a stretch . countless media stories charge comey with violating procedure . do you know whats a procedural violation emailing classified information stored on your bathroom server . senator harry reid has sent comey a letter accusing him of violating the hatch act . the hatch act is a nice idea that has as much relevance in the age of obama as the tenth amendment . but the cable news spectrum quickly filled with media hacks glancing at the wikipedia article on the hatch act under the table while accusing the fbi director of one of the most awkward conspiracies against hillary ever . if james comey is really out to hurt hillary he picked one hell of a strange way to do it . not too long ago democrats were breathing a sigh of relief when he gave hillary clinton a pass in a prominent public statement . if he really were out to elect trump by keeping the email scandal going why did he trash the investigation was he on the payroll of house republicans and the kgb back then and playing it coy or was it a sudden development where vladimir putin and paul ryan talked him into taking a look at anthony weiners computer either comey is the most cunning fbi director that ever lived or hes just awkwardly trying to navigate a political mess that has trapped him between a doj leadership whose political futures are tied to hillarys victory and his own bureau whose apolitical agents just want to be allowed to do their jobs . the only truly mysterious thing is why hillary and her associates decided to go to war with a respected federal agency . most americans like the fbi while hillary clinton enjoys a 60 unfavorable rating . and its an interesting question . hillarys old strategy was to lie and deny that the fbi even had a criminal investigation underway . instead her associates insisted that it was a security review . the fbi corrected her and she shrugged it off . but the old breezy denial approach has given way to a savage assault on the fbi . pretending that nothing was wrong was a bad strategy but it was a better one that picking a fight with the fbi while lunatic clinton associates try to claim that the fbi is really the kgb . there are two possible explanations . hillary clinton might be arrogant enough to lash out at the fbi now that she believes that victory is near . the same kind of hubris that led her to plan her victory fireworks display could lead her to declare a war on the fbi for irritating her during the final miles of her campaign . but the other explanation is that her people panicked . going to war with the fbi is not the behavior of a smart and focused presidential campaign . its an act of desperation . when a presidential candidate decides that her only option is to try and destroy the credibility of the fbi thats not hubris its fear of what the fbi might be about to reveal about her . during the original fbi investigation hillary clinton was confident that she could ride it out . and she had good reason for believing that . but that hillary clinton is gone . in her place is a paranoid wreck . within a short space of time the positive clinton campaign promising to unite the country has been replaced by a desperate and flailing operation that has focused all its energy on fighting the fbi . theres only one reason for such bizarre behavior . the clinton campaign has decided that an fbi investigation of the latest batch of emails poses a threat to its survival . and so its gone all in on fighting the fbi . its an unprecedented step born of fear . its hard to know whether that fear is justified . but the existence of that fear already tells us a whole lot . clinton loyalists rigged the old investigation . they knew the outcome ahead of time as well as they knew the debate questions . now suddenly they are no longer in control . and they are afraid . you can smell the fear . the fbi has wiretaps from the investigation of the clinton foundation . its finding new emails all the time . and clintonworld panicked . the spinmeisters of clintonworld have claimed that the email scandal is just so much smoke without fire . all thats here is the appearance of impropriety without any of the substance . but this isnt how you react to smoke . its how you respond to a fire . the misguided assault on the fbi tells us that hillary clinton and her allies are afraid of a revelation bigger than the fundamental illegality of her email setup . the email setup was a preemptive cover up . the clinton campaign has panicked badly out of the belief right or wrong that whatever crime the illegal setup was meant to cover up is at risk of being exposed . the clintons have weathered countless scandals over the years . whatever they are protecting this time around is bigger than the usual corruption bribery sexual assaults and abuses of power that have followed them around throughout the years . this is bigger and more damaging than any of the allegations that have already come out . and they dont want fbi investigators anywhere near it . the campaign against comey is pure intimidation . its also a warning . any senior fbi people who value their careers are being warned to stay away . the democrats are closing ranks around their nominee against the fbi . its an ugly and unprecedented scene . it may also be their last stand . hillary clinton has awkwardly wound her way through numerous scandals in just this election cycle . but shes never shown fear or desperation before . now that has changed . whatever she is afraid of it lies buried in her emails with huma abedin . and it can bring her down like nothing else has .'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.label=='FAKE'].text.values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Many to One LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Method:\n",
    "This method is no different to the method utilised in the sentiment analysis lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          2988500   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,018,005\n",
      "Trainable params: 3,018,005\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(len(word2num), 50)) # , batch_size=batch_size\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5701 samples, validate on 634 samples\n",
      "Epoch 1/1\n",
      "5701/5701 [==============================] - 41s 7ms/step - loss: 0.6293 - acc: 0.6648 - val_loss: 0.3867 - val_acc: 0.8549\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ff58de588>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 5\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=1, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2: Fixed Embeddings\n",
    "This is where we use the `Embedding2` class to which we give a set of weights which remain the same through training. Note especially the number of trainable parameters in the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding2_1 (Embedding2)    (None, None, 50)          2988500   \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                29440     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 3,018,005\n",
      "Trainable params: 116,355\n",
      "Non-trainable params: 2,901,650\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding2(len(word2num), 50,\n",
    "                    fixed_weights=np.array([word2glove[w] for w in words_in_glove]))) # , batch_size=batch_size\n",
    "model.add(LSTM(64))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# rmsprop = keras.optimizers.RMSprop(lr=1e-4)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I may heave cheated and run the following block 3 times. Good thing about Keras is that it remembers the last learning rate and goes from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5701 samples, validate on 634 samples\n",
      "Epoch 1/15\n",
      "5701/5701 [==============================] - 38s 7ms/step - loss: 0.6409 - acc: 0.6369 - val_loss: 0.5756 - val_acc: 0.7066\n",
      "Epoch 2/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.5710 - acc: 0.7044 - val_loss: 0.5717 - val_acc: 0.7224\n",
      "Epoch 3/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.5660 - acc: 0.7088 - val_loss: 0.5524 - val_acc: 0.7287\n",
      "Epoch 4/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.5456 - acc: 0.7232 - val_loss: 0.4942 - val_acc: 0.7603\n",
      "Epoch 5/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.5289 - acc: 0.7606 - val_loss: 0.4950 - val_acc: 0.7666\n",
      "Epoch 6/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4831 - acc: 0.7783 - val_loss: 0.4679 - val_acc: 0.7997\n",
      "Epoch 7/15\n",
      "5701/5701 [==============================] - 38s 7ms/step - loss: 0.4731 - acc: 0.7783 - val_loss: 0.4165 - val_acc: 0.8281\n",
      "Epoch 8/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.4544 - acc: 0.7978 - val_loss: 0.5005 - val_acc: 0.7934\n",
      "Epoch 9/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4507 - acc: 0.8000 - val_loss: 0.4523 - val_acc: 0.8028\n",
      "Epoch 10/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4278 - acc: 0.8028 - val_loss: 0.4864 - val_acc: 0.7886\n",
      "Epoch 11/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.4342 - acc: 0.7953 - val_loss: 0.4414 - val_acc: 0.7871\n",
      "Epoch 12/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.4254 - acc: 0.8000 - val_loss: 0.3939 - val_acc: 0.8249\n",
      "Epoch 13/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4228 - acc: 0.8181 - val_loss: 0.4455 - val_acc: 0.8044\n",
      "Epoch 14/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4432 - acc: 0.7995 - val_loss: 0.4025 - val_acc: 0.8297\n",
      "Epoch 15/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.4281 - acc: 0.8206 - val_loss: 0.4324 - val_acc: 0.8123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ff476ea90>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5701 samples, validate on 634 samples\n",
      "Epoch 1/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4452 - acc: 0.8078 - val_loss: 0.4205 - val_acc: 0.8249\n",
      "Epoch 2/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4023 - acc: 0.8325 - val_loss: 0.4309 - val_acc: 0.8155\n",
      "Epoch 3/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.3959 - acc: 0.8209 - val_loss: 0.4972 - val_acc: 0.7634\n",
      "Epoch 4/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.3869 - acc: 0.8309 - val_loss: 0.3719 - val_acc: 0.8596\n",
      "Epoch 5/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4195 - acc: 0.8114 - val_loss: 0.3889 - val_acc: 0.8486\n",
      "Epoch 6/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.4538 - acc: 0.7857 - val_loss: 0.4385 - val_acc: 0.8139\n",
      "Epoch 7/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.4061 - acc: 0.8149 - val_loss: 0.3738 - val_acc: 0.8565\n",
      "Epoch 8/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.3881 - acc: 0.8327 - val_loss: 0.3535 - val_acc: 0.8565\n",
      "Epoch 9/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.4021 - acc: 0.8195 - val_loss: 0.4217 - val_acc: 0.8060\n",
      "Epoch 10/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.3750 - acc: 0.8332 - val_loss: 0.4140 - val_acc: 0.8297\n",
      "Epoch 11/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.3696 - acc: 0.8434 - val_loss: 0.5335 - val_acc: 0.7603\n",
      "Epoch 12/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.3625 - acc: 0.8456 - val_loss: 0.3610 - val_acc: 0.8423\n",
      "Epoch 13/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.3326 - acc: 0.8595 - val_loss: 0.3584 - val_acc: 0.8549\n",
      "Epoch 14/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.3408 - acc: 0.8520 - val_loss: 0.3539 - val_acc: 0.8438\n",
      "Epoch 15/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.3312 - acc: 0.8577 - val_loss: 0.3394 - val_acc: 0.8580\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3ff4715cf8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5701 samples, validate on 634 samples\n",
      "Epoch 1/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.3340 - acc: 0.8591 - val_loss: 0.4075 - val_acc: 0.8170\n",
      "Epoch 2/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.3414 - acc: 0.8581 - val_loss: 0.3367 - val_acc: 0.8486\n",
      "Epoch 3/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.3175 - acc: 0.8665 - val_loss: 0.3124 - val_acc: 0.8675\n",
      "Epoch 4/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.3227 - acc: 0.8577 - val_loss: 0.2967 - val_acc: 0.8896\n",
      "Epoch 5/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.3087 - acc: 0.8739 - val_loss: 0.4130 - val_acc: 0.8170\n",
      "Epoch 6/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.3211 - acc: 0.8614 - val_loss: 0.3526 - val_acc: 0.8281\n",
      "Epoch 7/15\n",
      "5701/5701 [==============================] - 37s 7ms/step - loss: 0.3136 - acc: 0.8705 - val_loss: 0.2904 - val_acc: 0.8864\n",
      "Epoch 8/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.2978 - acc: 0.8779 - val_loss: 0.3251 - val_acc: 0.8722\n",
      "Epoch 9/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.2968 - acc: 0.8783 - val_loss: 0.2892 - val_acc: 0.8754\n",
      "Epoch 10/15\n",
      "5701/5701 [==============================] - 37s 6ms/step - loss: 0.2769 - acc: 0.8832 - val_loss: 0.2867 - val_acc: 0.8817\n",
      "Epoch 11/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.2742 - acc: 0.8867 - val_loss: 0.2961 - val_acc: 0.8896\n",
      "Epoch 12/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.2745 - acc: 0.8891 - val_loss: 0.2778 - val_acc: 0.8833\n",
      "Epoch 13/15\n",
      "5701/5701 [==============================] - 35s 6ms/step - loss: 0.2774 - acc: 0.8844 - val_loss: 0.2712 - val_acc: 0.8912\n",
      "Epoch 14/15\n",
      "5701/5701 [==============================] - 36s 6ms/step - loss: 0.2549 - acc: 0.8918 - val_loss: 0.2864 - val_acc: 0.8880\n",
      "Epoch 15/15\n",
      "5701/5701 [==============================] - 33s 6ms/step - loss: 0.2514 - acc: 0.8974 - val_loss: 0.2641 - val_acc: 0.9006\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f3f086b4780>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 128\n",
    "model.fit(X_train, y_train, batch_size=batch_size, epochs=15, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL NEWS! 0.94919\n"
     ]
    }
   ],
   "source": [
    "sentence = \"President Trump is the greatest president of all time period .\".lower()\n",
    "sentence_num = [word2num[w] if w in word2num else word2num['<Other>'] for w in sentence.split()]\n",
    "sentence_num = [word2num['<PAD>']]*(500-len(sentence_num)) + sentence_num\n",
    "sentence_num = np.array(sentence_num)\n",
    "sentence_prediction = model.predict(sentence_num[None,:])\n",
    "if sentence_prediction < 0.5: \n",
    "    print('FAKE NEWS! %0.5f' % sentence_prediction) \n",
    "else:\n",
    "    print('REAL NEWS! %0.5f' % sentence_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> president trump is the greatest president of all time period .'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join([num2word[w] for w in sentence_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL NEWS! 0.86151\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The chemicals in the water is turning the freaking frogs gay says cnn . \".lower()\n",
    "sentence_num = [word2num[w] if w in word2num else word2num['<Other>'] for w in sentence.split()]\n",
    "sentence_num = [word2num['<PAD>']]*(500-len(sentence_num)) + sentence_num\n",
    "sentence_num = np.array(sentence_num)\n",
    "sentence_prediction = model.predict(sentence_num[None,:])\n",
    "if sentence_prediction < 0.5: \n",
    "    print('FAKE NEWS! %0.5f' % sentence_prediction) \n",
    "else:\n",
    "    print('REAL NEWS! %0.5f' % sentence_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL NEWS! 0.57424\n"
     ]
    }
   ],
   "source": [
    "sentence = \"North korea is testing out missiles on americans living overseas .\".lower()\n",
    "sentence_num = [word2num[w] if w in word2num else word2num['<Other>'] for w in sentence.split()]\n",
    "sentence_num = [word2num['<PAD>']]*(500-len(sentence_num)) + sentence_num\n",
    "sentence_num = np.array(sentence_num)\n",
    "sentence_prediction = model.predict(sentence_num[None,:])\n",
    "if sentence_prediction < 0.5: \n",
    "    print('FAKE NEWS! %0.5f' % sentence_prediction) \n",
    "else:\n",
    "    print('REAL NEWS! %0.5f' % sentence_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAKE NEWS! 0.16170\n"
     ]
    }
   ],
   "source": [
    "sentence = \"ISS lecturer Sam GU Zhan earns the biggest salary among all says National University of Singapore .\".lower()\n",
    "sentence_num = [word2num[w] if w in word2num else word2num['<Other>'] for w in sentence.split()]\n",
    "sentence_num = [word2num['<PAD>']]*(500-len(sentence_num)) + sentence_num\n",
    "sentence_num = np.array(sentence_num)\n",
    "sentence_prediction = model.predict(sentence_num[None,:])\n",
    "if sentence_prediction < 0.5: \n",
    "    print('FAKE NEWS! %0.5f' % sentence_prediction) \n",
    "else:\n",
    "    print('REAL NEWS! %0.5f' % sentence_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634/634 [==============================] - 2s 3ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.26412493300738771, 0.90063091520255301]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
