{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MLP model trained from day 15 - 16\n",
    "\n",
    "import keras\n",
    "from keras.models import load_model\n",
    "\n",
    "model = load_model('mnist_mlp.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_3_1/kernel:0' shape=(784, 512) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_3_1/bias:0' shape=(512,) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4_1/kernel:0' shape=(512, 10) dtype=float32_ref>,\n",
       " <tf.Variable 'dense_4_1/bias:0' shape=(10,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "data = X_test[0]\n",
    "label = y_test[0]\n",
    "plt.imshow(data)\n",
    "plt.show()\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28, 28)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1eb8ada4a90>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data2 = data.reshape(1, 784)\n",
    "data2 = data2.astype('float32')\n",
    "data2 /= 255\n",
    "\n",
    "output = model.predict(data2)\n",
    "\n",
    "output = output.reshape(1, -1) # make it look like image\n",
    "plt.imshow(output) # show image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 86us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.23022360926419497, 0.9371]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# pre-process test data\n",
    "# MLP only receives vectors, so we flatten 28x28 to 784\n",
    "X_test = X_test.reshape(-1, 784) # from (-1, 28, 28) to (-1, 784)\n",
    "\n",
    "# Convert element datatype to float32 because Tensorflow prefers it\n",
    "# https://stackoverflow.com/questions/43406876/cant-use-float64-in-keras-with-tensorflow-backend\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "# scale to between 0 and 1 because gradient descent is faster\n",
    "# image pixel values range from 0 to 255\n",
    "X_test /= 255\n",
    "\n",
    "num_classes = 10 # 10 digits\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# model.evaluate()\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine tune existing model\n",
    "#\n",
    "# 1. continue training: change all layers\n",
    "# 2. fine-tune: only change some layers\n",
    "#\n",
    "\n",
    "# pre-process train data\n",
    "\n",
    "X_train = X_train.reshape(-1, 784) # from (-1, 28, 28) to (-1, 784)\n",
    "\n",
    "# Convert element datatype to float32 because Tensorflow prefers it\n",
    "# https://stackoverflow.com/questions/43406876/cant-use-float64-in-keras-with-tensorflow-backend\n",
    "X_train = X_train.astype('float32')\n",
    "\n",
    "# scale to between 0 and 1 because gradient descent is faster\n",
    "# image pixel values range from 0 to 255\n",
    "X_train_scaled = X_train / 255\n",
    "\n",
    "y_train_cat = keras.utils.to_categorical(y_train, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 45000 samples, validate on 15000 samples\n",
      "Epoch 1/3\n",
      "45000/45000 [==============================] - 6s 142us/step - loss: 0.1288 - acc: 0.9652 - val_loss: 0.1507 - val_acc: 0.9573\n",
      "Epoch 2/3\n",
      "45000/45000 [==============================] - 7s 147us/step - loss: 0.1274 - acc: 0.9658 - val_loss: 0.1497 - val_acc: 0.9575\n",
      "Epoch 3/3\n",
      "45000/45000 [==============================] - 6s 134us/step - loss: 0.1260 - acc: 0.9663 - val_loss: 0.1483 - val_acc: 0.9577\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "import time\n",
    "\n",
    "filepath = 'mnist_mlp.chkpt.h5' # save_best_only\n",
    "#filepath = 'mnist_mlp.{epoch:02d}-{val_loss:.4f}.h5' # keep everything\n",
    "modelcheckpoint = ModelCheckpoint(filepath, save_best_only=True)\n",
    "\n",
    "earlystop = EarlyStopping(patience=2)\n",
    "tensorboard = TensorBoard(log_dir='./logs/mnist_mlp/%d' % time.time())\n",
    "\n",
    "history = model.fit(X_train_scaled, y_train_cat, batch_size=128, epochs=3,\n",
    "                    callbacks=[tensorboard, earlystop, modelcheckpoint],\n",
    "                    validation_split=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
