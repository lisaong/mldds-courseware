{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify movie reviews into positive or negative\n",
    "\n",
    "Dataset: https://www.kaggle.com/utathya/imdb-review-dataset/version/1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    25000\n",
       "neg    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/imdb-review-dataset/imdb_master.csv',\n",
    "                 encoding='latin_1', index_col=0)\n",
    "\n",
    "df1 = df.loc[df.label != 'unsup']\n",
    "\n",
    "df1.label.value_counts() # count each label value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df1.loc[df1.type == 'train']\n",
    "df_test = df1.loc[df1.type == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(n=2000, random_state=42)\n",
    "df_test = df_test.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Requires: conda install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', 'this', 'is', 'a', 'test', '.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text = 'Hello this is a test.'\n",
    "\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'liked', 'cat', 'and', 'dog', ',', 'and', 'teaching', 'machine', 'to', 'learn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "text = 'he liked cats and dogs, and teaching machines to learn'\n",
    "\n",
    "lm = WordNetLemmatizer()\n",
    "\n",
    "print([lm.lemmatize(token) for token in word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['he', 'like', 'cat', 'and', 'dog', ',', 'and', 'teach', 'machin', 'to', 'learn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "text = 'he liked cats and dogs, and teaching machines to learn'\n",
    "\n",
    "stem = SnowballStemmer(language='english')\n",
    "\n",
    "print([stem.stem(token) for token in word_tokenize(text)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['liked', 'cats', 'dogs', ',', 'teaching', 'machines', 'learn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "text = 'he liked cats and dogs, and teaching machines to learn'\n",
    "\n",
    "print([token for token in word_tokenize(text) if token not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'very', 'itself', 'does', 'nor', 'as', 'had', 'not', 'ours', \"shan't\", 'out', 'yourself', \"hadn't\", \"hasn't\", 'him', 'ma', 'over', 'each', 'is', \"that'll\", 'she', 'to', \"she's\", 'but', 'should', 'shouldn', 'needn', 'when', 'those', \"weren't\", 'don', 'didn', 's', 'if', 'did', 'into', 'more', 'no', 'it', 'doing', \"didn't\", 'these', 'just', 'then', 'what', 'a', 'ain', 'now', 've', \"mightn't\", 'his', 'them', 'up', 'he', 'was', 'won', \"won't\", 'such', 'wasn', 'were', 'theirs', 'or', 'from', 'yours', \"needn't\", 'few', 'once', 'd', 'can', 'during', 'they', 'own', 'will', \"haven't\", \"isn't\", 'there', 'some', 'y', 'at', 'on', \"don't\", 'we', \"you'd\", 'against', 'both', 'aren', 'shan', 're', 'himself', 'be', 'have', 'being', 'hadn', 'any', \"wouldn't\", 'of', 'under', 'why', 'which', 'after', 'has', 'between', 'again', 'further', 'me', 'do', 'all', 'you', 'and', 'same', 'so', 'than', \"you've\", 'down', 'weren', 'an', 'most', 'couldn', 'o', 'are', \"wasn't\", 'who', 'because', 'her', 'before', 'wouldn', 'mightn', 'its', 'this', 'for', \"you're\", 'i', 'with', 'here', 'above', \"should've\", \"couldn't\", 'ourselves', 'where', 'm', 'other', 'in', 'by', 'yourselves', 'themselves', 'hasn', \"mustn't\", \"it's\", 'off', \"aren't\", 'the', 'doesn', 'through', 't', 'your', \"you'll\", 'herself', 'whom', 'mustn', 'that', 'am', 'until', 'isn', 'having', 'how', 'about', 'll', 'haven', 'myself', 'hers', 'my', 'while', \"doesn't\", 'our', 'only', \"shouldn't\", 'their', 'below', 'too', 'been'}\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>This is the first document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>This document is the second document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>And this is the third one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Is this the first document?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  document  first  is  one  second  the  third  this  \\\n",
       "0    0         1      1   1    0       0    1      0     1   \n",
       "1    0         2      0   1    0       1    1      0     1   \n",
       "2    1         0      0   1    1       0    1      1     1   \n",
       "3    0         1      1   1    0       0    1      0     1   \n",
       "\n",
       "                                    text  \n",
       "0            This is the first document.  \n",
       "1  This document is the second document.  \n",
       "2             And this is the third one.  \n",
       "3            Is this the first document?  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "   'This is the first document.',\n",
    "   'This document is the second document.',\n",
    "   'And this is the third one.',\n",
    "   'Is this the first document?',\n",
    "]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "result = cv.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(result.todense(), columns=cv.get_feature_names())\n",
    "pd.concat([df, pd.DataFrame(corpus, index=df.index, columns=['text'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>document</th>\n",
       "      <th>first</th>\n",
       "      <th>is</th>\n",
       "      <th>one</th>\n",
       "      <th>second</th>\n",
       "      <th>the</th>\n",
       "      <th>third</th>\n",
       "      <th>this</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>This is the first document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.687624</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.538648</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.281089</td>\n",
       "      <td>This document is the second document.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>0.511849</td>\n",
       "      <td>0.267104</td>\n",
       "      <td>And this is the third one.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.469791</td>\n",
       "      <td>0.580286</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.384085</td>\n",
       "      <td>Is this the first document?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        and  document     first        is       one    second       the  \\\n",
       "0  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "1  0.000000  0.687624  0.000000  0.281089  0.000000  0.538648  0.281089   \n",
       "2  0.511849  0.000000  0.000000  0.267104  0.511849  0.000000  0.267104   \n",
       "3  0.000000  0.469791  0.580286  0.384085  0.000000  0.000000  0.384085   \n",
       "\n",
       "      third      this                                   text  \n",
       "0  0.000000  0.384085            This is the first document.  \n",
       "1  0.000000  0.281089  This document is the second document.  \n",
       "2  0.511849  0.267104             And this is the third one.  \n",
       "3  0.000000  0.384085            Is this the first document?  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "corpus = [\n",
    "   'This is the first document.',\n",
    "   'This document is the second document.',\n",
    "   'And this is the third one.',\n",
    "   'Is this the first document?',\n",
    "]\n",
    "\n",
    "tv = TfidfVectorizer()\n",
    "result = tv.fit_transform(corpus)\n",
    "\n",
    "df = pd.DataFrame(result.todense(), columns=cv.get_feature_names())\n",
    "pd.concat([df, pd.DataFrame(corpus, index=df.index, columns=['text'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x21645 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 184219 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize # tokenization\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # computes tfidf\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        self.stop = set(stopwords.words('english'))\n",
    "\n",
    "    def __call__(self, doc):\n",
    "\n",
    "        # tokenize text into tokens\n",
    "        tokens = word_tokenize(doc)\n",
    "\n",
    "        # strip out punctuation\n",
    "        words = [t for t in tokens if t.isalpha()]\n",
    "        \n",
    "        # strip out stopwords\n",
    "        words = [t for t in words if t not in self.stop]\n",
    "        \n",
    "        # lemmatize each token\n",
    "        return [self.wnl.lemmatize(t) for t in words]\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(analyzer='word', tokenizer=LemmaTokenizer())\n",
    "\n",
    "V_train = tfidf.fit_transform(df_train.review)\n",
    "V_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize only the first n rows\n",
    "n = 300\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "Z_train_2d = tsne.fit_transform(V_train[:n].todense())\n",
    "y_train = df_train.label[:n]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "labels = ['pos', 'neg']\n",
    "\n",
    "for l in labels:\n",
    "    ax.scatter(Z_train_2d[y_train == l, 0],\n",
    "               Z_train_2d[y_train == l, 1],\n",
    "               label=l, alpha=.2) # alpha = transparency\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Stage 1: Setup\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # setup scaler with X_train's mean, std\n",
    "\n",
    "Stage 2: Apply\n",
    "scaler.transform(X_train) # scaler is unchanged, just performs transformation\n",
    "scaler.transform(X_test) # scaler is unchanged, just performs transformation\n",
    "scaler.transform(new_data) # scaler is unchanged, just performs transformation\n",
    "...\n",
    "\n",
    "Stage 1: Setup\n",
    "tfidf = TfidfVectorizer(..)\n",
    "tfidf.fit(X_train) # setup tfidf with X_train's terms, documents\n",
    "\n",
    "Stage 2: Apply\n",
    "tfidf.transform(X_train) # tfidf is unchanged, just performs transformation\n",
    "tfidf.transform(X_test) # tfidf is unchanged, just performs transformation\n",
    "tfidf.transform(new_text) # tfidf is unchanged, just performs transformation\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform V_test from text to vectors\n",
    "\n",
    "V_test = tfidf.transform(df_test.review) # not fit_transform()\n",
    "y_test = df_test.label\n",
    "y_train = df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\issohl\\AppData\\Local\\Continuum\\miniconda3\\envs\\mldds01\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6495\n",
      "{'C': 10, 'gamma': 0.0001}\n",
      "0.6325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "V_train_sc = scaler.fit_transform(V_train) # fit(V_train), transform(V_train)\n",
    "V_test_sc = scaler.transform(V_test)\n",
    "\n",
    "# fit SGD\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "sgd.fit(V_train_sc, y_train)\n",
    "print(sgd.score(V_test_sc, y_test))\n",
    "\n",
    "# fit SVC (GridSearch because this doesn't use gradient descent)\n",
    "Cs = [.0001, .001, .01, .1, 1, 10]\n",
    "gammas = [.0001, .001, .01, .1, 1, 10]\n",
    "gs_svc = GridSearchCV(SVC(random_state=42), param_grid={'C':Cs, 'gamma':gammas})\n",
    "gs_svc.fit(V_train_sc, y_train)\n",
    "print(gs_svc.best_params_)\n",
    "print(gs_svc.score(V_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.67      0.65      0.66      1040\n",
      "        pos       0.63      0.65      0.64       960\n",
      "\n",
      "avg / total       0.65      0.65      0.65      2000\n",
      "\n",
      "[[677 363]\n",
      " [338 622]]\n",
      "SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.59      0.97      0.73      1040\n",
      "        pos       0.88      0.27      0.42       960\n",
      "\n",
      "avg / total       0.73      0.63      0.58      2000\n",
      "\n",
      "[[1004   36]\n",
      " [ 699  261]]\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('SGD')\n",
    "pred_sgd = sgd.predict(V_test_sc)\n",
    "print(classification_report(y_test, pred_sgd))\n",
    "print(confusion_matrix(y_test, pred_sgd))\n",
    "\n",
    "print('SVC')\n",
    "pred_svc = gs_svc.predict(V_test_sc)\n",
    "print(classification_report(y_test, pred_svc))\n",
    "print(confusion_matrix(y_test, pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50001    Take a low budget, inexperienced actors doubli...\n",
      "50002    Everybody has seen 'Back To The Future,' right...\n",
      "50003    Doris Day was an icon of beauty in singing and...\n",
      "Name: review, dtype: object\n",
      "SGD ['neg' 'neg' 'neg']\n",
      "SVC ['neg' 'neg' 'pos']\n"
     ]
    }
   ],
   "source": [
    "df_unsup = df.loc[df.label == 'unsup'] # unlabeled reviews\n",
    "\n",
    "test = df_unsup.iloc[1:4].review # take 3 reviews\n",
    "print(test)\n",
    "\n",
    "# Tokenize .... Tfidf\n",
    "V_test_doc = tfidf.transform(test)\n",
    "\n",
    "# Scale\n",
    "V_test_doc_sc = scaler.transform(V_test_doc)\n",
    "\n",
    "# predict using SVC and SGD\n",
    "print('SGD', sgd.predict(V_test_doc_sc))\n",
    "print('SVC', gs_svc.predict(V_test_doc_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doris Day was an icon of beauty in singing and acting by her warm voice and genius acting in different movies obtained this film by her legend songs as (Iwill never stop loving you) with soft melody and warm lyrics by magic voice of Day.<br /><br />James Cagney was a villain of Hollywood by shark eyes and voice to send for audience the core of badness and evil characters as his profile in cinema.The producer choose previously Ava Gardener to be the hero of this film in-front Cagney but Cagney refused this choose because he said that Gardner not familiar with his role and he cheesed Day in it because of her fantastic abilities between singing and acting and she succeeded in it with Cagney.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# doc = df_train.iloc[0].review\n",
    "\n",
    "# tokens = word_tokenize(doc) # needs nltk.download('punkt')\n",
    "# print(tokens[:20])\n",
    "\n",
    "# lem = WordNetLemmatizer() # needs nltk.download('wordnet')\n",
    "\n",
    "# lemmatized = [lem.lemmatize(t) for t in tokens]\n",
    "# print(lemmatized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras embedding layer\n",
    "# https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks\n",
    "\n",
    "# Without GPU: conda install keras\n",
    "# With GPU: conda install keras-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 21460)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense, Conv1D, MaxPool1D\n",
    "\n",
    "n_vocab = 5000\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', analyzer='word',\n",
    "                        tokenizer=LemmaTokenizer(),\n",
    "                        max_features=n_vocab)\n",
    "\n",
    "tfidf.fit(df_train.review)\n",
    "V_train_keras = tfidf.transform(df_train.review)\n",
    "V_test_keras = tfidf.transform(df_test.review)\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(V_train_keras)\n",
    "V_train_keras_sc = scaler.transform(V_train_keras)\n",
    "V_test_keras_sc = scaler.transform(V_test_keras)\n",
    "\n",
    "y_train_keras = y_train.map({'pos': 1, 'neg': 0})\n",
    "y_test_keras = y_test.map({'pos': 1, 'neg': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 5000, 50)          250000    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 4998, 100)         15100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2499, 100)         0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 35)                14280     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 36        \n",
      "=================================================================\n",
      "Total params: 279,416\n",
      "Trainable params: 279,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, embedding_vector_length, input_length=n_vocab))\n",
    "model.add(Conv1D(100, kernel_size=3, activation='relu', padding='valid'))\n",
    "model.add(MaxPool1D())\n",
    "model.add(GRU(35))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 119s 119ms/step - loss: 0.6928 - acc: 0.5110 - val_loss: 0.6930 - val_acc: 0.5120\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.6927 - acc: 0.5280 - val_loss: 0.6958 - val_acc: 0.4930\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 0.6871 - acc: 0.5270 - val_loss: 0.6928 - val_acc: 0.5200\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.6804 - acc: 0.5240 - val_loss: 0.6986 - val_acc: 0.5260\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.6737 - acc: 0.5520 - val_loss: 0.7054 - val_acc: 0.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23714f7fdd8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import time\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "earlystopping = EarlyStopping(patience=2)\n",
    "tensorboard = TensorBoard(log_dir='./logs/text_gru/%d' % time.time(),\n",
    "                          write_graph=False)\n",
    "\n",
    "n = 1000\n",
    "batch_size = 16\n",
    "model.fit(V_train_keras_sc[:n], y_train_keras[:n],\n",
    "          validation_data=(V_test_keras_sc[:n], y_test_keras[:n]),\n",
    "          epochs=20, batch_size=batch_size,\n",
    "          callbacks=[earlystopping, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 83s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69238569688797, 0.52]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(V_test_keras_sc, y_test_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50001    Take a low budget, inexperienced actors doubli...\n",
      "50002    Everybody has seen 'Back To The Future,' right...\n",
      "50003    Doris Day was an icon of beauty in singing and...\n",
      "50004    After a series of silly, fun-loving movies, 19...\n",
      "50005    This isn't exactly a musical, but it almost se...\n",
      "50006    After seven years and seventeen pictures at Wa...\n",
      "50007    In the 1950's there were many film boigraphies...\n",
      "50008    MY RATING- 7.3<br /><br />This one is a curiou...\n",
      "50009    Doris Day and James Cagney are excellent in th...\n",
      "Name: review, dtype: object\n",
      "Keras Embedding layer [[0.48327655]\n",
      " [0.48329026]\n",
      " [0.48327655]\n",
      " [0.4832878 ]\n",
      " [0.4832897 ]\n",
      " [0.48354024]\n",
      " [0.48327655]\n",
      " [0.48327655]\n",
      " [0.48327655]]\n"
     ]
    }
   ],
   "source": [
    "test = df_unsup.iloc[1:10].review # take 3 reviews\n",
    "print(test)\n",
    "\n",
    "# Tokenize .... Tfidf\n",
    "V_test_doc = tfidf.transform(test)\n",
    "\n",
    "# Scale\n",
    "V_test_doc_sc = scaler.transform(V_test_doc)\n",
    "\n",
    "# predict\n",
    "print('Keras Embedding layer', model.predict(V_test_doc_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
