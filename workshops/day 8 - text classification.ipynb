{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify movie reviews into positive or negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    25000\n",
       "neg    25000\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/imdb-review-dataset/imdb_master.csv',\n",
    "                 encoding='latin_1', index_col=0)\n",
    "\n",
    "df1 = df.loc[df.label != 'unsup']\n",
    "\n",
    "df1.label.value_counts() # count each label value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df1.loc[df1.type == 'train']\n",
    "df_test = df1.loc[df1.type == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(n=2000, random_state=42)\n",
    "df_test = df_test.sample(n=2000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\issohl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2000x21460 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 165017 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize # tokenization\n",
    "from nltk.stem import WordNetLemmatizer # lemmatizes\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # computes tfidf\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "\n",
    "        # tokenize text into tokens\n",
    "        tokens = word_tokenize(doc)        \n",
    "\n",
    "        # strip out punctuation\n",
    "        words = [t for t in tokens if t.isalpha()]\n",
    "        \n",
    "        # lemmatize each token\n",
    "        return [self.wnl.lemmatize(t) for t in words]\n",
    "\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', analyzer='word',\n",
    "                        tokenizer=LemmaTokenizer())\n",
    "\n",
    "V_train = tfidf.fit_transform(df_train.review)\n",
    "V_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# visualize only the first n rows\n",
    "n = 300\n",
    "\n",
    "tsne = TSNE(n_components=2)\n",
    "Z_train_2d = tsne.fit_transform(V_train[:n].todense())\n",
    "y_train = df_train.label[:n]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "labels = ['pos', 'neg']\n",
    "\n",
    "for l in labels:\n",
    "    ax.scatter(Z_train_2d[y_train == l, 0],\n",
    "               Z_train_2d[y_train == l, 1],\n",
    "               label=l, alpha=.2) # alpha = transparency\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "Stage 1: Setup\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # setup scaler with X_train's mean, std\n",
    "\n",
    "Stage 2: Apply\n",
    "scaler.transform(X_train) # scaler is unchanged, just performs transformation\n",
    "scaler.transform(X_test) # scaler is unchanged, just performs transformation\n",
    "scaler.transform(new_data) # scaler is unchanged, just performs transformation\n",
    "...\n",
    "\n",
    "Stage 1: Setup\n",
    "tfidf = TfidfVectorizer(..)\n",
    "tfidf.fit(X_train) # setup tfidf with X_train's terms, documents\n",
    "\n",
    "Stage 2: Apply\n",
    "tfidf.transform(X_train) # tfidf is unchanged, just performs transformation\n",
    "tfidf.transform(X_test) # tfidf is unchanged, just performs transformation\n",
    "tfidf.transform(new_text) # tfidf is unchanged, just performs transformation\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform V_test from text to vectors\n",
    "\n",
    "V_test = tfidf.transform(df_test.review) # not fit_transform()\n",
    "y_test = df_test.label\n",
    "y_train = df_train.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\issohl\\AppData\\Local\\Continuum\\miniconda3\\envs\\mldds01\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6495\n",
      "{'C': 10, 'gamma': 0.0001}\n",
      "0.6325\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scale\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "V_train_sc = scaler.fit_transform(V_train) # fit(V_train), transform(V_train)\n",
    "V_test_sc = scaler.transform(V_test)\n",
    "\n",
    "# fit SGD\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "sgd.fit(V_train_sc, y_train)\n",
    "print(sgd.score(V_test_sc, y_test))\n",
    "\n",
    "# fit SVC (GridSearch because this doesn't use gradient descent)\n",
    "Cs = [.0001, .001, .01, .1, 1, 10]\n",
    "gammas = [.0001, .001, .01, .1, 1, 10]\n",
    "gs_svc = GridSearchCV(SVC(random_state=42), param_grid={'C':Cs, 'gamma':gammas})\n",
    "gs_svc.fit(V_train_sc, y_train)\n",
    "print(gs_svc.best_params_)\n",
    "print(gs_svc.score(V_test_sc, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.67      0.65      0.66      1040\n",
      "        pos       0.63      0.65      0.64       960\n",
      "\n",
      "avg / total       0.65      0.65      0.65      2000\n",
      "\n",
      "[[677 363]\n",
      " [338 622]]\n",
      "SVC\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        neg       0.59      0.97      0.73      1040\n",
      "        pos       0.88      0.27      0.42       960\n",
      "\n",
      "avg / total       0.73      0.63      0.58      2000\n",
      "\n",
      "[[1004   36]\n",
      " [ 699  261]]\n"
     ]
    }
   ],
   "source": [
    "# metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('SGD')\n",
    "pred_sgd = sgd.predict(V_test_sc)\n",
    "print(classification_report(y_test, pred_sgd))\n",
    "print(confusion_matrix(y_test, pred_sgd))\n",
    "\n",
    "print('SVC')\n",
    "pred_svc = gs_svc.predict(V_test_sc)\n",
    "print(classification_report(y_test, pred_svc))\n",
    "print(confusion_matrix(y_test, pred_svc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50001    Take a low budget, inexperienced actors doubli...\n",
      "50002    Everybody has seen 'Back To The Future,' right...\n",
      "50003    Doris Day was an icon of beauty in singing and...\n",
      "Name: review, dtype: object\n",
      "SGD ['neg' 'neg' 'neg']\n",
      "SVC ['neg' 'neg' 'pos']\n"
     ]
    }
   ],
   "source": [
    "df_unsup = df.loc[df.label == 'unsup'] # unlabeled reviews\n",
    "\n",
    "test = df_unsup.iloc[1:4].review # take 3 reviews\n",
    "print(test)\n",
    "\n",
    "# Tokenize .... Tfidf\n",
    "V_test_doc = tfidf.transform(test)\n",
    "\n",
    "# Scale\n",
    "V_test_doc_sc = scaler.transform(V_test_doc)\n",
    "\n",
    "# predict using SVC and SGD\n",
    "print('SGD', sgd.predict(V_test_doc_sc))\n",
    "print('SVC', gs_svc.predict(V_test_doc_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Doris Day was an icon of beauty in singing and acting by her warm voice and genius acting in different movies obtained this film by her legend songs as (Iwill never stop loving you) with soft melody and warm lyrics by magic voice of Day.<br /><br />James Cagney was a villain of Hollywood by shark eyes and voice to send for audience the core of badness and evil characters as his profile in cinema.The producer choose previously Ava Gardener to be the hero of this film in-front Cagney but Cagney refused this choose because he said that Gardner not familiar with his role and he cheesed Day in it because of her fantastic abilities between singing and acting and she succeeded in it with Cagney.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.values[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "# doc = df_train.iloc[0].review\n",
    "\n",
    "# tokens = word_tokenize(doc) # needs nltk.download('punkt')\n",
    "# print(tokens[:20])\n",
    "\n",
    "# lem = WordNetLemmatizer() # needs nltk.download('wordnet')\n",
    "\n",
    "# lemmatized = [lem.lemmatize(t) for t in tokens]\n",
    "# print(lemmatized[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras embedding layer\n",
    "# https://www.liip.ch/en/blog/sentiment-detection-with-keras-word-embeddings-and-lstm-deep-learning-networks\n",
    "\n",
    "# Without GPU: conda install keras\n",
    "# With GPU: conda install keras-gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 21460)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V_train_sc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, GRU, Dense, Conv1D, MaxPool1D\n",
    "\n",
    "n_vocab = 5000\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', analyzer='word',\n",
    "                        tokenizer=LemmaTokenizer(),\n",
    "                        max_features=n_vocab)\n",
    "\n",
    "tfidf.fit(df_train.review)\n",
    "V_train_keras = tfidf.transform(df_train.review)\n",
    "V_test_keras = tfidf.transform(df_test.review)\n",
    "\n",
    "scaler = StandardScaler(with_mean=False)\n",
    "scaler.fit(V_train_keras)\n",
    "V_train_keras_sc = scaler.transform(V_train_keras)\n",
    "V_test_keras_sc = scaler.transform(V_test_keras)\n",
    "\n",
    "y_train_keras = y_train.map({'pos': 1, 'neg': 0})\n",
    "y_test_keras = y_test.map({'pos': 1, 'neg': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_15 (Embedding)     (None, 5000, 50)          250000    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 4998, 100)         15100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 2499, 100)         0         \n",
      "_________________________________________________________________\n",
      "gru_3 (GRU)                  (None, 35)                14280     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 36        \n",
      "=================================================================\n",
      "Total params: 279,416\n",
      "Trainable params: 279,416\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "embedding_vector_length = 50\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, embedding_vector_length, input_length=n_vocab))\n",
    "model.add(Conv1D(100, kernel_size=3, activation='relu', padding='valid'))\n",
    "model.add(MaxPool1D())\n",
    "model.add(GRU(35))\n",
    "model.add(Dense(1, activation='sigmoid')) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 119s 119ms/step - loss: 0.6928 - acc: 0.5110 - val_loss: 0.6930 - val_acc: 0.5120\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.6927 - acc: 0.5280 - val_loss: 0.6958 - val_acc: 0.4930\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 117s 117ms/step - loss: 0.6871 - acc: 0.5270 - val_loss: 0.6928 - val_acc: 0.5200\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.6804 - acc: 0.5240 - val_loss: 0.6986 - val_acc: 0.5260\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 116s 116ms/step - loss: 0.6737 - acc: 0.5520 - val_loss: 0.7054 - val_acc: 0.5240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23714f7fdd8>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "import time\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy']) \n",
    "\n",
    "earlystopping = EarlyStopping(patience=2)\n",
    "tensorboard = TensorBoard(log_dir='./logs/text_gru/%d' % time.time(),\n",
    "                          write_graph=False)\n",
    "\n",
    "n = 1000\n",
    "batch_size = 16\n",
    "model.fit(V_train_keras_sc[:n], y_train_keras[:n],\n",
    "          validation_data=(V_test_keras_sc[:n], y_test_keras[:n]),\n",
    "          epochs=20, batch_size=batch_size,\n",
    "          callbacks=[earlystopping, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000/2000 [==============================] - 83s 42ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.69238569688797, 0.52]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(V_test_keras_sc, y_test_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50001    Take a low budget, inexperienced actors doubli...\n",
      "50002    Everybody has seen 'Back To The Future,' right...\n",
      "50003    Doris Day was an icon of beauty in singing and...\n",
      "50004    After a series of silly, fun-loving movies, 19...\n",
      "50005    This isn't exactly a musical, but it almost se...\n",
      "50006    After seven years and seventeen pictures at Wa...\n",
      "50007    In the 1950's there were many film boigraphies...\n",
      "50008    MY RATING- 7.3<br /><br />This one is a curiou...\n",
      "50009    Doris Day and James Cagney are excellent in th...\n",
      "Name: review, dtype: object\n",
      "Keras Embedding layer [[0.48327655]\n",
      " [0.48329026]\n",
      " [0.48327655]\n",
      " [0.4832878 ]\n",
      " [0.4832897 ]\n",
      " [0.48354024]\n",
      " [0.48327655]\n",
      " [0.48327655]\n",
      " [0.48327655]]\n"
     ]
    }
   ],
   "source": [
    "test = df_unsup.iloc[1:10].review # take 3 reviews\n",
    "print(test)\n",
    "\n",
    "# Tokenize .... Tfidf\n",
    "V_test_doc = tfidf.transform(test)\n",
    "\n",
    "# Scale\n",
    "V_test_doc_sc = scaler.transform(V_test_doc)\n",
    "\n",
    "# predict\n",
    "print('Keras Embedding layer', model.predict(V_test_doc_sc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
