{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN to classify IMDB movie reviews\n",
    "#   predict: positive (1), negative (0)\n",
    "#\n",
    "# 1. Load data and inspect\n",
    "# 2. Embeddings\n",
    "# 3. Train RNN (LSTM) to classify the reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import imdb\n",
    "\n",
    "# load the reviews with the top 3000 words only\n",
    "# skip top 5 most frequent words (like 'a', 'the'): stop words\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=3000,\n",
    "                                                      skip_top=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 1\n",
      "review (numbers): [2, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 2, 66, 2, 2, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 2, 172, 112, 167, 2, 336, 385, 39, 2, 172, 2, 1111, 17, 546, 38, 13, 447, 2, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 2, 1920, 2, 469, 2, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 2, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 2, 2223, 2, 16, 480, 66, 2, 33, 2, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 2, 107, 117, 2, 15, 256, 2, 2, 7, 2, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 2, 2, 1029, 13, 104, 88, 2, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 2, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 2, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 2, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
      "review (words): and as you with out themselves powerful lets loves their becomes and had and and lot from anyone to have after out atmosphere never more room and it so heart shows to years and every never going and help moments or and every and visual movie except her was several and enough more with is now current film as you and mine and unfortunately and you than him that with out themselves her get for was camp and you movie sometimes movie that with scary but and to story wonderful that in seeing in character to and 70s and with heart had and they and here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in and seen over and for anyone and and br and to whether from than out themselves history he name half some br and and odd was two most and mean for 1 any an boat she he should is thought and but and script you not while history he heart to real at and but when from one bit then have two and script their with her nobody most that with wasn't to with and acting watch an for with and film want an\n"
     ]
    }
   ],
   "source": [
    "# gets the word index json\n",
    "text_to_index = imdb.get_word_index()\n",
    "\n",
    "# maps word index json from term -> index to index -> term\n",
    "index_to_text = dict((text_to_index[k], k) for k in text_to_index)\n",
    "\n",
    "# converts first review from index to words \n",
    "print('label', y_train[0])\n",
    "print('review (numbers):', X_train[0])\n",
    "print('review (words):', \" \".join([index_to_text[x] for x in X_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "lengths = np.array([len(X_train[i]) for i in range(len(X_train))])\n",
    "\n",
    "# to find out what window size to use\n",
    "# we'll have to tune this for speeding up training or improving accuracy\n",
    "window_size = int(np.mean(lengths))\n",
    "window_size\n",
    "\n",
    "# or you can try the max (but very slow)\n",
    "#lengths[np.argmax(lengths)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, None, 50)          150000    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, None, 64)          29440     \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 32)                12416     \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 192,945\n",
      "Trainable params: 192,945\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create our RNN model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "vocab_size = 3000 # this can also be tuned (speed vs. accuracy)\n",
    "embedding_size = 50 # another parameter to tune (speed vs. accuracy)\n",
    "lstm_output_size = 32 # tune to how many output features\n",
    "\n",
    "model = Sequential()\n",
    "# featurizer\n",
    "model.add(Embedding(vocab_size, embedding_size))\n",
    "# return_sequences: output will be a sequence that you can\n",
    "# feed into another LSTM\n",
    "model.add(LSTM(64, return_sequences=True))\n",
    "model.add(LSTM(32))\n",
    "# classifier\n",
    "model.add(Dense(lstm_output_size, activation='relu')) # binary classifier\n",
    "model.add(Dense(1, activation='sigmoid')) # binary classifier\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 238)\n"
     ]
    }
   ],
   "source": [
    "# Feature Engineering: make sure sequences are same length (window_size)\n",
    "# before feeding into Keras\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# pads or truncates the sequences\n",
    "# Note: maybe truncating='post' because earlier words may be\n",
    "# useful for semantic meaning\n",
    "# Note: if you are doing text prediction, then later words are\n",
    "# more useful for prediction\n",
    "X_train = sequence.pad_sequences(X_train, maxlen=window_size)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=window_size)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "model.compile('rmsprop', loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 87s 4ms/step - loss: 0.4380 - acc: 0.7977 - val_loss: 0.3395 - val_acc: 0.8618\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 91s 5ms/step - loss: 0.3060 - acc: 0.8730 - val_loss: 0.4219 - val_acc: 0.8456\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard, EarlyStopping\n",
    "import time\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='logs/imdb_lstm%d' % time.time())\n",
    "earlystop = EarlyStopping(patience=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=10,\n",
    "                    callbacks=[tensorboard, earlystop],\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 29s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4200911285352707, 0.8476]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Score\n",
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "pred = model.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.50      0.38      0.43     12500\n",
      "          1       0.50      0.63      0.56     12500\n",
      "\n",
      "avg / total       0.50      0.50      0.50     25000\n",
      "\n",
      "[[4692 7808]\n",
      " [4607 7893]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(classification_report(y_test, pred))\n",
    "print(confusion_matrix(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To improve accuracy try increasing these:\n",
    "#\n",
    "# vocab_size = 3000 # this can also be tuned (speed vs. accuracy)\n",
    "# embedding_size = 50 # another parameter to tune (speed vs. accuracy)\n",
    "# lstm_output_size = 32 # tune to how many output features\n",
    "# \n",
    "# Try recurrent_dropout (reduce overfit)\n",
    "# Add another LSTM layer (learn more)\n",
    "\n",
    "# To speed things up:\n",
    "# \n",
    "# Increase batch size to 64, 128 (number of times per epoch)\n",
    "# Reduce the hyperparameters above\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I was so excited when I first learned that Kevin Kwan's \"Crazy Rich Asians\" was going to become a film! The book was way more appealing than I had first imagined it would be, and I'm happy to report that Jon Chu's screen version has surpassed my hopeful-but-wary expectations. Not to make it sound too simplistic, the movie was beautiful and very, very funny. Go see it!\n",
      "Yes, it is a romantic comedy - but this has such intriguing social and cultural undercurrents that it tempts even the fairly observant watcher away from taking the \"Cinderella\" story at its glitzy face value. While the numerous characters had to have their backstories compressed to fit into just two hours, we are given enough great dialogue, effervescent or slightly evil portrayals, and sumptuous visual clues to make the friends and family members in Singapore come alive.\n",
      "All the aunties, cousins and ladies-in-waiting may be slightly overwhelming for people who haven't read the book, but anyone with wacky friends and pompous relatives should get it, even if they are not Asian. \n",
      "I liked film's especially clever use of graphics, as well as the smooth-to-rocking score, the lush and verdant locations, the perfect designer costuming, and pretty much everything else. One of my favorite lines was about having attended Cal State Fullerton; but you must to watch it for yourself. I honestly have not laughed out loud during a film this much in decades. Oh, and I rather liked Chris Pang, too. A totally hot actor, even though I'm old enough to be his mother.\n",
      "As soon as Crazy Rich Asians officially opens, I'm going to catch it again. The preview was not enough, and there were so many little moments that deserve a second or third look. Now we must all hope that a sequel with the same talented cast and Chu in charge is coming our way before too long. Thank you all, you fabulous Asian actors, crew, writers and backers who made this possible. And no, I'm not of even a little bit Asian ancestry.\n",
      "\n",
      "positive [1] probability [0.9411829]\n",
      "-------\n",
      "what a boring movie. This was a very boring film. I fell asleep in the cinema. This movie deserves no attention! I do not recommend this movie because it's a waste of time.\n",
      "positive [1] probability [0.7964469]\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "# Predict\n",
    "# Source: https://www.imdb.com/title/tt3104988/reviews\n",
    "\n",
    "test1 = \"\"\"I was so excited when I first learned that Kevin Kwan's \"Crazy Rich Asians\" was going to become a film! The book was way more appealing than I had first imagined it would be, and I'm happy to report that Jon Chu's screen version has surpassed my hopeful-but-wary expectations. Not to make it sound too simplistic, the movie was beautiful and very, very funny. Go see it!\n",
    "Yes, it is a romantic comedy - but this has such intriguing social and cultural undercurrents that it tempts even the fairly observant watcher away from taking the \"Cinderella\" story at its glitzy face value. While the numerous characters had to have their backstories compressed to fit into just two hours, we are given enough great dialogue, effervescent or slightly evil portrayals, and sumptuous visual clues to make the friends and family members in Singapore come alive.\n",
    "All the aunties, cousins and ladies-in-waiting may be slightly overwhelming for people who haven't read the book, but anyone with wacky friends and pompous relatives should get it, even if they are not Asian. \n",
    "I liked film's especially clever use of graphics, as well as the smooth-to-rocking score, the lush and verdant locations, the perfect designer costuming, and pretty much everything else. One of my favorite lines was about having attended Cal State Fullerton; but you must to watch it for yourself. I honestly have not laughed out loud during a film this much in decades. Oh, and I rather liked Chris Pang, too. A totally hot actor, even though I'm old enough to be his mother.\n",
    "As soon as Crazy Rich Asians officially opens, I'm going to catch it again. The preview was not enough, and there were so many little moments that deserve a second or third look. Now we must all hope that a sequel with the same talented cast and Chu in charge is coming our way before too long. Thank you all, you fabulous Asian actors, crew, writers and backers who made this possible. And no, I'm not of even a little bit Asian ancestry.\n",
    "\"\"\"\n",
    "test2 = \"\"\"what a boring movie. This was a very boring film. I fell asleep in the cinema. This movie deserves no attention! I do not recommend this movie because it's a waste of time.\"\"\"\n",
    "\n",
    "def clean_and_get_sequence(text):\n",
    "    # https://keras.io/preprocessing/text/#text_to_word_sequence\n",
    "    from keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "    test_sequence = text_to_word_sequence(text, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\n   ',\n",
    "                                          lower=True, split=' ')\n",
    "    # drop words not in vocab\n",
    "    test_sequence_cleaned = [s for s in test_sequence if s in text_to_index]\n",
    "\n",
    "    # map to indices\n",
    "    test_sequence_index = [text_to_index[s] for s in test_sequence_cleaned]\n",
    "    #print('as index\\n', sequence_index)\n",
    "\n",
    "    # filter out top 3000\n",
    "    test_sequence_index_3000 = [i for i in test_sequence_index if i <= 3000]\n",
    "    #print('as index (top 3000 only)\\n', test_sequence_index_3000)\n",
    "\n",
    "    # look at review\n",
    "    test_review = ' '.join([index_to_text[i] for i in test_sequence_index_3000])\n",
    "    #print('as words (top 3000 only)\\n', test_review)\n",
    "    \n",
    "    return test_sequence_index_3000\n",
    "\n",
    "test1_index = clean_and_get_sequence(test1)\n",
    "test2_index = clean_and_get_sequence(test2)\n",
    "\n",
    "test_reviews = [test1_index, test2_index]\n",
    "\n",
    "#print('Pad sequences (samples x time)')\n",
    "test_reviews = sequence.pad_sequences(test_reviews, maxlen=window_size)\n",
    "#print('test_reviews shape:', test_reviews.shape)\n",
    "\n",
    "tests = [test1, test2]\n",
    "pred_prob = model.predict(test_reviews)\n",
    "pred_label = model.predict_classes(test_reviews)\n",
    "\n",
    "for text, label, probability in zip(tests, pred_label, pred_prob):\n",
    "    print(text)\n",
    "    print('positive', label, 'probability', probability)\n",
    "    print('-------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_14 (Embedding)     (None, 238, 50)           150000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 236, 250)          37750     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_3 (Glob (None, 250)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 250)               62750     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 250,751\n",
      "Trainable params: 250,751\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv1D, GlobalMaxPooling1D, Activation\n",
    "\n",
    "# Try 1D Convolution as a comparison\n",
    "filters = 250 # rectangle depth\n",
    "kernel_size = 3\n",
    "\n",
    "# building CNN model\n",
    "model_cnn = Sequential()\n",
    "\n",
    "# featurizer\n",
    "# 3000 vocab, 50-dimension embedding vector, review length 238\n",
    "model_cnn.add(Embedding(vocab_size, embedding_size,\n",
    "              input_length=window_size))\n",
    "model_cnn.add(Conv1D(filters, kernel_size, activation='relu'))\n",
    "model_cnn.add(GlobalMaxPooling1D())\n",
    "\n",
    "# classifier\n",
    "model_cnn.add(Dense(filters, activation='relu'))\n",
    "model_cnn.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 134s 7ms/step - loss: 0.4076 - acc: 0.7989 - val_loss: 0.2965 - val_acc: 0.8726\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 135s 7ms/step - loss: 0.2317 - acc: 0.9077 - val_loss: 0.2972 - val_acc: 0.8748\n"
     ]
    }
   ],
   "source": [
    "model_cnn.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs/imdb_cnn%d' % time.time())\n",
    "earlystop = EarlyStopping(patience=1)\n",
    "\n",
    "history = model_cnn.fit(X_train, y_train, batch_size=32, epochs=10,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[tensorboard, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 111s 6ms/step - loss: 0.1220 - acc: 0.9570 - val_loss: 0.3163 - val_acc: 0.8760\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 117s 6ms/step - loss: 0.0484 - acc: 0.9874 - val_loss: 0.3799 - val_acc: 0.8774\n"
     ]
    }
   ],
   "source": [
    "model_cnn.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs/imdb_cnn%d' % time.time())\n",
    "earlystop = EarlyStopping(patience=1)\n",
    "\n",
    "history = model_cnn.fit(X_train, y_train, batch_size=64, epochs=10,\n",
    "                        validation_split=0.2,\n",
    "                        callbacks=[tensorboard, earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
