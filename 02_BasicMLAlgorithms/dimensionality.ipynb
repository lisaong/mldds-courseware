{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coping with Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- The Curse of Dimensionality\n",
    "- Principal Component Analysis\n",
    "- Singular Value Decomposition\n",
    "- Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where are we?\n",
    "\n",
    "![is there a 4th dimension?](assets/linear-regression/machine-learning-cheet-sheet.png)\n",
    "\n",
    "(image: [sas.com](https://www.sas.com/en_us/insights/analytics/machine-learning.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualizing data\n",
    "\n",
    "Humans can't visualize data in more than 3-D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    "- As number of dimensions increase, need exponentially more data to create a generalized model\n",
    "\n",
    "- $d$ dimensions, $v$ target values: $O(v^d)$ examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimension Reduction - Objective\n",
    "\n",
    "\"Project\" data from high dimensions to lower dimensions\n",
    "\n",
    "There will be data loss, but should be within acceptable limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Dimension Reduction - Algorithms\n",
    "\n",
    "- Principal Component Analysis (PCA)\n",
    "- Singular Value Decomposition (SVD)\n",
    "- Linear Discriminant Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "- 'Black box' matrix factorization / decomposition.\n",
    "- Finds the subspace (defined by principal component axes) that can capture the highest variance\n",
    "  - Capturing the most variance minimizes information loss\n",
    "  - PCA is designed to be reversible (but not 100%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pca example](assets/dim/fig_pca_illu3d.png)\n",
    "\n",
    "(image: http://phdthesis-bioinformatics-maxplanckinstitute-molecularplantphys.matthias-scholz.de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA - Concept\n",
    "\n",
    "To project $X$ from $p\\,x\\,n$ dimensions to $L\\,x\\,n$ dimensions:\n",
    "$$T_{L} = XW_{L}$$\n",
    "\n",
    "- $X$: original matrix, $p\\,x\\,n$\n",
    "- $W_{L}$: basis vector matrix $p\\,x\\,n$\n",
    "- $T_{L}$: resulting projection of X, $L\\,x\\,n$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA - Types\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "\n",
    "- Linear PCA\n",
    "- Kernel (non-linear) PCA\n",
    "  - linear, polynomial, radial basis function, sigmoid kernel functions\n",
    "- Sparce PCA\n",
    "  - removes noise in the components due to matrix factorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![pca](assets/dim/sphx_glr_plot_kernel_pca_001.png)\n",
    "\n",
    "(image: http://scikit-learn.org/stable/auto_examples/decomposition/plot_kernel_pca.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA - Steps\n",
    "\n",
    "1. Put with data into an $p\\,x\\,n$ matrix, where $p$ is number of features, $n$ is number of samples\n",
    "2. Zero-center by subtracting the mean\n",
    "3. Perform Singular vector (SVD) or Eigenvector Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA - Memory Constraints\n",
    "\n",
    "- All data must fit in memory\n",
    "- Larger datasets: use Incremental PCA: http://scikit-learn.org/stable/modules/decomposition.html#incremental-pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Singular Value Decomposition\n",
    "\n",
    "- An implementation of PCA\n",
    "- A type of matrix factorization that uses singular matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## SVD\n",
    "\n",
    "$$T = XW = U\\Sigma W^TW = U\\Sigma$$\n",
    "\n",
    "- $U$: left singular vectors of $X$, $n\\,x\\,n$\n",
    "- $\\Sigma$: singular matrix of $X$, $n\\,x\\,p$\n",
    "- $W$: right singular vectors of $X$, $p\\,x\\,p$\n",
    "\n",
    "Get $T_L = U_L \\Sigma_L$ by truncating the first $L$ columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Detailed explanation of PCA and SVD\n",
    "\n",
    "A Tutorial on Principal Component Analysis: https://arxiv.org/abs/1404.1100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "- Supervised\n",
    "  - Uses the class labels to compute class separation of data\n",
    "- Given $C$ classes, $p$ features, LDA can reduce features from $p$ into $C-1$ features.\n",
    "- Dual purpose:\n",
    "  - Dimensionality reduction\n",
    "  - Classifier\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![PCA vs. LDA](assets/dim/lda_1.png)\n",
    "\n",
    "(image: https://sebastianraschka.com/Articles/2014_python_lda.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workshop - Data Projections\n",
    "\n",
    "In this workshop, we will\n",
    "- Perform 2-dimensional and 3-dimensional PCA projections, using SVD solvers, on a multi-dimensional dataset\n",
    "- Perform Linear Discriminant Analysis to project that same dataset into n_classes - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Dataset - Motion Capture Hand Postures\n",
    "\n",
    "5 types of hand postures from 12 users were recorded using unlabeled markers on fingers of a glove in a motion capture environment. Due to resolution and occlusion, missing values are common.\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Motion+Capture+Hand+Postures\n",
    "\n",
    "1. Download the data set from the link above\n",
    "2. Extract into a folder and update `read_csv` to use your path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('D:/tmp/Postures/Postures.csv') # update this to your path\n",
    "df = df.apply(pd.to_numeric, errors='coerce') # convert non-numeric values to NaN\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# skip the first row, which is all zeros\n",
    "df = df.iloc[1:]\n",
    "\n",
    "# randomly sample 5000 points to plot\n",
    "df = df.sample(n=5000)\n",
    "print('df.shape:', df.shape)\n",
    "\n",
    "# split into X and y (ignore User column)\n",
    "X = df.loc[:, 'X0':]\n",
    "y = df.loc[:, 'Class']\n",
    "\n",
    "print('X.shape:', X.shape)\n",
    "print('y.shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Run PCA to project into 2 components\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X) # will fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Removing NaNs for PCA\n",
    "\n",
    "If you try to run PCA directly above, you should see this error:\n",
    "> Input contains NaN, infinity or a value too large for dtype('float64').\n",
    "\n",
    "In this exercise:\n",
    "1. Replace the NaN values using `sklearn.preprocessing.Imputer`: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.Imputer.html\n",
    "2. Re-run PCA\n",
    "3. Plot the PCA projection\n",
    "\n",
    "For step 1, try different imputation strategies ('mean', 'median', 'most_frequent') to see if they make a difference in the plot and `PCA.explained_variance_ratio_`\n",
    "\n",
    "Note: `sklearn.impute.SimpleImputer` will replace `sklearn.preprocessing.Imputer` in future versions of sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# impute missing values because PCA can't handle it\n",
    "# can't just dropna() because some classes will have no coverage\n",
    "#\n",
    "# http://scikit-learn.org/dev/auto_examples/plot_missing_values.html\n",
    "\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Re-run PCA after fixing the data\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "print('Before: X.shape', X.shape)\n",
    "print('After: X.shape', X_pca.shape)\n",
    "\n",
    "# Percentage of variance explained by the 2 components (higher is better)\n",
    "print('PCA: explained variance ratio (first two components): %s'\n",
    "      % str(pca.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Plot the PCA projection in 2D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# target names from the data webpage\n",
    "target_names = ['fist', 'stop', 'point 1 finger', 'point 2 fingers', 'grab']\n",
    "n_classes = len(target_names)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# https://matplotlib.org/examples/color/colormaps_reference.html\n",
    "colors = [plt.cm.viridis(each)\n",
    "          for each in np.linspace(0, 1, n_classes)]\n",
    "\n",
    "for color, i, target_name in zip(colors, y.unique(), target_names):\n",
    "    ax.scatter(X_pca[y == i, 0], X_pca[y == i, 1], color=color, alpha=.8,\n",
    "               lw=.5, s=10, label=target_name)\n",
    "ax.legend(loc='upper right', shadow=False, scatterpoints=1, fontsize='large')\n",
    "ax.set(title='2-D PCA of MoCap Hand Postures dataset')\n",
    "ax.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Walkthrough: PCA projection into 3D-space\n",
    "\n",
    "We can also project the Hand Postures data into 3 components and do a 3-D plot.\n",
    "\n",
    "Credits: http://scikit-learn.org/dev/auto_examples/decomposition/plot_pca_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X)\n",
    "\n",
    "print('Before: X.shape', X.shape)\n",
    "print('After: X.shape', X_pca_3d.shape)\n",
    "\n",
    "# Percentage of variance explained by the 3 components (higher is better)\n",
    "print('PCA: explained variance ratio (first three components): %s'\n",
    "      % str(pca_3d.explained_variance_ratio_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Plot the PCA projection in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# make the plot interactive, since we're in 3D!\n",
    "%matplotlib notebook\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw=dict(projection='3d'), figsize=(10, 10))\n",
    "\n",
    "colors = [plt.cm.viridis(each)\n",
    "          for each in np.linspace(0, 1, n_classes)]\n",
    "\n",
    "for color, i, target_name in zip(colors, y.unique(), target_names):\n",
    "    ax.scatter(X_pca_3d[y == i, 0], X_pca_3d[y == i, 1], X_pca_3d[y == i, 2],\n",
    "               s=10, color=color, label=target_name)\n",
    "\n",
    "ax.legend(loc='upper right', shadow=False, scatterpoints=1, fontsize='large')\n",
    "ax.set(title='3-D PCA of MoCap Hand Postures dataset')\n",
    "ax.axis('tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: Linear Discriminant Analysis\n",
    "\n",
    "We'll now try LDA. \n",
    "\n",
    "LDA can project the dataset from 35 features to 4 features (n_classes - 1)\n",
    "\n",
    "Tasks:\n",
    "1. Fit `sklearn.discriminant_analysis.LinearDiscriminantAnalysis` on X and y, with the default number of components\n",
    "2. Plot the first 3 features in 3-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Fit sklearn.discriminant_analysis.LinearDiscriminantAnalysis\n",
    "# Your code here\n",
    "\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Plot the first 3 components in 3-D\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Workshop - Impact of Dimensionality\n",
    "\n",
    "In this workshop, we will explore whether dimensionality reduction can improve a classification model.\n",
    "\n",
    "We will:\n",
    "- Train a Random Forest Clasifier using all 11 features\n",
    "- Train Random Forest Classifiers with the projections of features into these dimensions:\n",
    "  - 2-dimensions\n",
    "  - 5-dimensions\n",
    "- Compare the performances of all the above models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset - Wine Quality\n",
    "\n",
    "Two datasets are included, related to red and white vinho verde wine samples, from the north of Portugal. The goal is to model wine quality based on physicochemical tests (see [Cortez et al., 2009], [Web Link](http://www3.dsi.uminho.pt/pcortez/wine/)).\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Wine+Quality\n",
    "\n",
    "1. Download the two .csv files from the link above into a folder of your choice\n",
    "2. Update `read_csv` to use your folder path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# we'll use the smaller dataset\n",
    "df = pd.read_csv('D:/tmp/wine-quality/winequality-red.csv', delimiter=';') # update this to your path\n",
    "df = df.apply(pd.to_numeric, errors='coerce') # convert non-numeric values to NaN\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X = df.loc[:, 'fixed acidity':'alcohol']\n",
    "y = df.loc[:, 'quality']\n",
    "print('X.shape:', X.shape)\n",
    "print('y.shape:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Classification model with all 11 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def TrainRFClassifier(data, labels, test_data, test_labels):\n",
    "    \"\"\"Trains a Random Forest classifier, using GridSearchCV to find\n",
    "    and return the best model\n",
    "    Args:\n",
    "        data: the training data\n",
    "        labels: the labels        \n",
    "        test_data: the test data\n",
    "        test_labels: the test labels        \n",
    "    Returns:\n",
    "        The best estimator found by GridSearchCV\n",
    "    \"\"\"\n",
    "    gs_rfc = GridSearchCV(RandomForestClassifier(), {'max_depth': [2, 4, 6, 8],\n",
    "                                                     'n_estimators': [5, 10, 20, 30]})\n",
    "    gs_rfc.fit(data, labels)\n",
    "\n",
    "    # select the best estimator\n",
    "    print('Best score:', gs_rfc.best_score_)\n",
    "    print('Best parameters:', gs_rfc.best_params_)\n",
    "\n",
    "    # predict\n",
    "    pred_labels = gs_rfc.predict(test_data)\n",
    "\n",
    "    # evaluation metrics\n",
    "    print(classification_report(test_labels, pred_labels))\n",
    "    cm = confusion_matrix(test_labels, pred_labels)\n",
    "    print(cm)\n",
    "    \n",
    "    return gs_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "TrainRFClassifier(train_X, train_y, test_X, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Classification model with fewer dimensions\n",
    "\n",
    "The code below performs PCA projections before fitting the RandomForestClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dimensions = [2, 3, 5, 7]\n",
    "\n",
    "for n in dimensions:\n",
    "    # 1. Run PCA to project X into n-dimension\n",
    "    # 2. Train the classifier using the projected X training data\n",
    "    print('========= Projection into %d dimensions =========' % n)\n",
    "    pca = KernelPCA(n_components=n)\n",
    "\n",
    "    # As usual, transform both X datasets, but only fit on train_X\n",
    "    # to avoid contamination by test_X    \n",
    "    train_X_pca = pca.fit(train_X).transform(train_X)\n",
    "    test_X_pca = pca.transform(test_X)\n",
    "    print('PCA explained variance ratio: %s' % str(pca.explained_variance_ratio_))\n",
    "        \n",
    "    TrainRFClassifier(train_X_pca, train_y, test_X_pca, test_y)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "In this case, the projections didn't help. Here's a possible reason why:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "train_y.value_counts() # not very balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: K Nearest Neighbors with Dimensionality Reduction\n",
    "\n",
    "Since Random Forest has mediocre performance, let's try a different algorithm that may not be as sensitive to an unbalanced dataset.\n",
    "\n",
    "Your task is to repeat the above with the K-nearest Neighbors classifier. \n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "\n",
    "Follow the guides below to fill in the code sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "def TrainKNClassifier(data, labels, test_data, test_labels):\n",
    "    \"\"\"Trains a K-nearest neighbors classifier, using GridSearchCV to find\n",
    "    and return the best model\n",
    "    Args:\n",
    "        data: the training data\n",
    "        labels: the labels        \n",
    "        test_data: the test data\n",
    "        test_labels: the test labels        \n",
    "    Returns:\n",
    "        The best estimator found by GridSearchCV\n",
    "    \"\"\"\n",
    "    gs_knc = GridSearchCV(KNeighborsClassifier(), {'n_neighbors': [4, 8, 10, 12, 15],\n",
    "                                                   'weights': ['uniform', 'distance']})\n",
    "    gs_knc.fit(data, labels)\n",
    "\n",
    "    # select the best estimator\n",
    "    print('Best score:', gs_knc.best_score_)\n",
    "    print('Best parameters:', gs_knc.best_params_)\n",
    "\n",
    "    # predict\n",
    "    pred_labels = gs_knc.predict(test_data)\n",
    "\n",
    "    # evaluation metrics\n",
    "    print(classification_report(test_labels, pred_labels))\n",
    "    cm = confusion_matrix(test_labels, pred_labels)\n",
    "    print(cm)\n",
    "    \n",
    "    return gs_knc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Train a K-nearest neighbors classifier using the\n",
    "# helper function TrainKNClassifier() on the original training set\n",
    "# (before any dimensionality reduction)\n",
    "\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# For dimensions [2, 3, 5, 7]\n",
    "# 1. Run PCA to project X into n-dimension\n",
    "# 2. Train the classifier using the projected X training data\n",
    "\n",
    "dimensions = [2, 3, 5, 7]\n",
    "\n",
    "for n in dimensions:\n",
    "   # Your code here\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
