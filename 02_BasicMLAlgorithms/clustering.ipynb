{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "![so easy for us](https://uploads.toptal.io/blog/image/92521/toptal-blog-image-1463639098844-eb9ad14c7f665e556b2cb66a65b6c257.jpg)\n",
    "\n",
    "(image: [toptal](https://www.toptal.com/machine-learning/clustering-algorithms))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "![yay, one more quadrant covered](assets/linear-regression/machine-learning-cheet-sheet.png)\n",
    "\n",
    "(image: [sas.com](https://www.sas.com/en_us/insights/analytics/machine-learning.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Supervised / Unsupervised Learning\n",
    "\n",
    "Unsupervised learning:\n",
    "- Just dataset, no targets or labels\n",
    "- Algorithm needs to \"make sense\" of the data itself\n",
    "- E.g. clustering, dimension reduction\n",
    "\n",
    "Supervised learning:\n",
    "- Dataset with targets or labels\n",
    "- Algorithm learns by minimizing loss against the targets / labels\n",
    "- E.g. Classification, regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![supervised vs unsupervised](assets/clustering/supervised_unsupervised.png)\n",
    "\n",
    "(image: [Vibhor Agarwal](https://medium.com/@agarwalvibhor84/getting-started-with-machine-learning-using-sklearn-python-7d165618eddf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "Objective: given a dataset with just features, find groups (or clusters) of them\n",
    "\n",
    "Broad applications:\n",
    "- Market segmentation\n",
    "- User recommendations\n",
    "- Anormaly detection\n",
    "\n",
    "https://en.wikipedia.org/wiki/Cluster_analysis#Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithms\n",
    "\n",
    "- K-means / K-modes\n",
    "- Expectation Maximization / Gaussian Mixture\n",
    "- DBSCAN\n",
    "\n",
    "Comparison:\n",
    "http://scikit-learn.org/stable/modules/clustering.html#overview-of-clustering-methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means\n",
    "\n",
    "- Divides $N$ samples into $K$ clusters $C$\n",
    "  - Each cluster is centered on a mean $\\mu_j$, known as a \"centroid\"\n",
    "- Objective is to minimize \"inertia\" (= within-cluster sum-of-squares)\n",
    "\n",
    "$$\\sum_{i=0}^n \\underset{\\mu_j \\in C} {\\arg \\min}{(\\|x_i - \\mu_j\\|^2)}$$\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: K-means clustering\n",
    "\n",
    "### Dataset: COE Bidding Results\n",
    "\n",
    "https://data.gov.sg/dataset/coe-bidding-results\n",
    "\n",
    "1. Download the dataset on your machine\n",
    "2. Extract into a folder and note the path for use in `pd.read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('D:/tmp/coe-bidding-results/coe-results.csv', # fix to your path\n",
    "                 usecols=['vehicle_class', 'quota', 'premium'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Task\n",
    "\n",
    "1. Find clusters using the `quota`, and `premium` columns.\n",
    "\n",
    "2. Compare the clusters found with the actual `vehicle_class` label\n",
    "\n",
    "*Note: in unsupervised learning, we DON'T provide the label for training the algorithm*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Data Processing\n",
    "\n",
    "1. Take the last 300 samples for demo purposes\n",
    "2. Encode labels from ['Category A', ...] to [0, 1, 2, ...]\n",
    "3. Shuffle and split the data into train and test\n",
    "4. Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# For demo purposes, use the first 300 samples\n",
    "n_samples = 300\n",
    "df = df.iloc[:n_samples]\n",
    "\n",
    "df_data = df.loc[:, 'quota':'premium']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df_labels = le.fit_transform(df.loc[:, 'vehicle_class'])\n",
    "\n",
    "# Train, test split, and shuffle\n",
    "df_data_train, df_data_test, df_labels_train, df_labels_test = train_test_split(df_data, df_labels)\n",
    "\n",
    "# Scale the data to simplify processing\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_data_train)\n",
    "df_data_train_scaled = scaler.transform(df_data_train)\n",
    "df_data_test_scaled = scaler.transform(df_data_test)\n",
    "\n",
    "print('Labels\\n', df_labels_train[:5])\n",
    "print('Data\\n', df_data_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Helper functions\n",
    "\n",
    "Here's a helper function to visualize the k-means algorithm.\n",
    "\n",
    "It does this:\n",
    "1. For the min-max ranges of `quota` and `premium` (after scaling), run `Kmeans.predict` to get the label outputs [0, 1, 2, 3, 4] (for 5 categories)\n",
    "2. Plot a colour mesh, with colour assigned to each label\n",
    "3. Scatter plot the training data in black\n",
    "4. Scatter plot the centroids in red"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def plot_decision_boundaries(ax, title, kmeans_model, data):\n",
    "    \"\"\"Plots the decision boundaries for a fitted k-means model\n",
    "    Args:\n",
    "        ax: subplot axis\n",
    "        title: subplot title\n",
    "        kmeans_model: a fitted sklearn.cluster.KMeans model\n",
    "        data: 2-dimensional input data to cluster and plot\n",
    " \n",
    "    Based on: http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html\n",
    "    \"\"\"\n",
    "    # Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "    h = .02     # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "    # Obtain labels for each point in mesh using the trained model.\n",
    "    Z = kmeans_model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    ax.imshow(Z, interpolation='nearest',\n",
    "              extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "              cmap=plt.cm.Pastel2,\n",
    "              aspect='auto', origin='lower')\n",
    "\n",
    "    ax.plot(data[:, 0], data[:, 1], 'k.', markersize=4)\n",
    "\n",
    "    # Plot the centroids as a red X\n",
    "    centroids = kmeans.cluster_centers_\n",
    "\n",
    "    ax.scatter(centroids[:, 0], centroids[:, 1],\n",
    "               marker='x', s=169, linewidths=3,\n",
    "               color='red', zorder=10, label='centroids')\n",
    "    ax.set(title=title,\n",
    "           xlim=(x_min, x_max), ylim=(y_min, y_max),\n",
    "           xticks=(), yticks=())\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Train and plot progress\n",
    "\n",
    "Now we'll run k-means with different iterations.\n",
    "\n",
    "Observe how the centroids move as the algorithm makes progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Run k-means iteratively and plot to demonstrate the algorithm at work\n",
    "n_classes = len(df.loc[:, 'vehicle_class'].unique())\n",
    "n_iterations = [1, 5, 10, 20]\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(15, 15))\n",
    "\n",
    "for n, ax in zip(n_iterations, axes.flatten()):\n",
    "    kmeans = KMeans(init='random', n_clusters=n_classes, max_iter=n, n_init=1)\n",
    "    kmeans.fit(df_data_train_scaled)\n",
    "    plot_decision_boundaries(ax, 'K-means, %d iteration(s)' % n, kmeans, df_data_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the above, trying different options for k-means:\n",
    "1. Try a different initialization method: `k-means++`\n",
    "2. Try averaging over different centroid seeds by setting `n_init` to a number greater than 1\n",
    "3. Try a different algorithm: `full`, `elkan`\n",
    "\n",
    "You can run `KMeans?` to see its documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Clustering Algorithms\n",
    "\n",
    "When you have the ground truth labels:\n",
    "- Homogeniety: each cluster contains only members of 1 class\n",
    "- Completeness: all members of 1 class are assigned to the same cluster\n",
    "- V-measure: harmonic mean of homogeniety and completeness\n",
    "$$v\\_measure = \\frac{2 * homogeniety * completeness}{homogeniety+completeness}$$\n",
    "- Adjusted Rand index: similarity measure, ignoring ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Clustering Algorithms\n",
    "\n",
    "When you don't have the ground truth labels:\n",
    "- Silhouette Coefficient\n",
    "\n",
    "$$S = \\frac{b-a}{max(a, b)}$$\n",
    "\n",
    "Where:\n",
    "- $a$: Mean distance between 1 sample and others in same class\n",
    "- $b$: Mean distance between 1 sample and others in nearest clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluating Clustering Algorithms\n",
    "\n",
    "Advantages and disadvantages of each metric:\n",
    "http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Evaluating K-means on the COE dataset\n",
    "\n",
    "Since we have our labels, we can evaluate the performance of the clustering algorithm using all of the above.\n",
    "\n",
    "Higher numbers are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "# Let's run to the maximum iterations\n",
    "kmeans = KMeans(init='random', n_clusters=n_classes, max_iter=100, n_init=1)\n",
    "kmeans.fit(df_data_train_scaled)\n",
    "\n",
    "# Run prediction on the test data\n",
    "test_labels_predict = kmeans.predict(df_data_test_scaled)\n",
    "test_labels_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Homogeneity: %0.3f\"\n",
    "      % metrics.homogeneity_score(df_labels_test, test_labels_predict))\n",
    "print(\"Completeness: %0.3f\"\n",
    "      % metrics.completeness_score(df_labels_test, test_labels_predict))\n",
    "print(\"V-measure: %0.3f\"\n",
    "      % metrics.v_measure_score(df_labels_test, test_labels_predict))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(df_labels_test, test_labels_predict))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(df_data_test_scaled, test_labels_predict,\n",
    "                                 sample_size=n_samples,\n",
    "                                 metric='euclidean'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise\n",
    "Compare the Evaluation Metrics for the different k-means settings you tried in the earlier exercise. \n",
    "\n",
    "- A different initialization method: k-means++\n",
    "- Averaging over different centroid seeds by setting n_init to a number greater than 1\n",
    "- A different algorithm: full, elkan\n",
    "\n",
    "Use max_iter = 100 or a similar value.\n",
    "\n",
    "Which combination performs the best on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Gaussian Mixture Models\n",
    "\n",
    "Gaussian mixture models use the normal distribution\n",
    "- Means are centered around clusters\n",
    "- Assigns probabilities that a sample belongs to a cluster\n",
    "\n",
    "http://scikit-learn.org/stable/modules/mixture.html#gaussian-mixture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Covariance Matrix Types\n",
    "\n",
    "The covariance matrix type controls how the gaussian models are \"mixed\" together.\n",
    "\n",
    "- Spherical: all components have spherical covariance matrices\n",
    "- Diagonal: each component has its own axis-aligned covariance matrix\n",
    "- Tied: all components have the same covariance matrix\n",
    "- Full: all components can have different covariance matrix\n",
    "\n",
    "Visualization:\n",
    "http://www.stats.ox.ac.uk/~sejdinov/teaching/dmml/Mixtures.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Gaussian Mixture Model on the COE dataset\n",
    "\n",
    "In this workshop, we will apply GMMs to cluster the COE dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Credits: https://github.com/scikit-learn/scikit-learn/blob/master/examples/mixture/plot_gmm_pdf.py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "def plot_gmm(ax, title, gmm, data):\n",
    "    \"\"\"Plots a Gaussian Mixture Model as a contour plot\n",
    "    Args:\n",
    "        ax: subplot axis\n",
    "        title: plot title\n",
    "        gmm: fitted Gaussian Mixture Model\n",
    "        data: data samples\n",
    "    \"\"\"\n",
    "    h = 0.1\n",
    "    x_min, x_max = data[:, 0].min() - 1, data[:, 0].max() + 1\n",
    "    y_min, y_max = data[:, 1].min() - 1, data[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "\n",
    "    # negative log-likelihood scores for each point in the mesh\n",
    "    # using the trained model\n",
    "    Z = -gmm.score_samples(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    cax = ax.contour(xx, yy, Z, norm=LogNorm(vmin=1.0, vmax=1000.0), \n",
    "                     levels=np.logspace(0, 3, 10))\n",
    "    plt.colorbar(cax, shrink=0.8, extend='both', ax=ax)\n",
    "    ax.scatter(data[:, 0], data[:, 1], color='black', marker='x')\n",
    "    ax.set(title=title,\n",
    "       xlim=(x_min, x_max), ylim=(y_min, y_max),\n",
    "       xticks=(), yticks=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = len(df.loc[:, 'vehicle_class'].unique())\n",
    "\n",
    "covariance_types = ['spherical', 'diag', 'tied', 'full']\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20),\n",
    "                        sharex=True, sharey=True)\n",
    "\n",
    "for cov_type, ax in zip(covariance_types, axes.flatten()):\n",
    "    gmm = GaussianMixture(n_components=n_classes, covariance_type=cov_type)\n",
    "    gmm.fit(df_data_train_scaled)\n",
    "    plot_gmm(ax, 'Negative log-likelihood predicted by a GMM (covariance type = %s)' % cov_type,\n",
    "             gmm, df_data_train_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise: Evaluation metrics and Spherical plots for GMM\n",
    "\n",
    "A contour plot is useful for visualizing the shape of a Gaussian Mixture Model, because it shows the probability distributions for each class.\n",
    "\n",
    "For clustering purposes, if we don't care so much about the probabilities, we can do spherical plots instead.\n",
    "\n",
    "1. Complete the `get_metrics` helper function to get the evaluation metrics for each GMM covariance type.\n",
    "2. Which model will you choose, based on the evaluation metrics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def get_gmm_cov_matrix(gmm, label):\n",
    "    \"\"\"Returns the covariance matrix\n",
    "    Args:\n",
    "        gmm: the Gaussian Mixture Model\n",
    "        label: the label index\n",
    "    Returns:\n",
    "        the covariance matrix\n",
    "\n",
    "    Credits: http://www.stats.ox.ac.uk/~sejdinov/teaching/dmml/Mixtures.html\n",
    "    \"\"\"\n",
    "    if gmm.covariance_type == 'full':\n",
    "        # one per label (of any type)\n",
    "        cov = gmm.covariances_[label]\n",
    "    elif gmm.covariance_type == 'tied':\n",
    "        # all the same\n",
    "        cov = gmm.covariances_\n",
    "    elif gmm.covariance_type == 'diag':\n",
    "        # diagonal matrix per label\n",
    "        cov = np.diag(gmm.covariances_[label])\n",
    "    elif gmm.covariance_type == 'spherical':\n",
    "        # all spherical\n",
    "        cov = np.eye(gmm.means_.shape[1]) * gmm.covariances_[label]\n",
    "    return cov\n",
    "    \n",
    "def plot_cov_ellipse(cov, pos, color='b', ax=None, **kwargs):\n",
    "    \"\"\"Plots the covariance ellipse\n",
    "    Args:\n",
    "        cov: the covariance to plot\n",
    "        pos: the position\n",
    "        color: the ellipse color\n",
    "        ax: the subplot axis\n",
    "    Returns:\n",
    "        the ellipse\n",
    "\n",
    "    Credits: http://www.stats.ox.ac.uk/~sejdinov/teaching/dmml/Mixtures.html\n",
    "    \"\"\"\n",
    "\n",
    "    def eigsorted(cov):\n",
    "        vals, vecs = np.linalg.eigh(cov)\n",
    "        order = vals.argsort()[::-1]\n",
    "        return vals[order], vecs[:,order]\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    vals, vecs = eigsorted(cov)\n",
    "    theta = np.degrees(np.arctan2(*vecs[:,0][::-1]))\n",
    "    nstd = 2 # ???\n",
    "\n",
    "    # Width and height are \"full\" widths, not radius\n",
    "    width, height = 2 * nstd * np.sqrt(vals)\n",
    "    ellip = Ellipse(xy=pos, width=width, height=height, angle=theta, color=color, alpha=0.2,**kwargs)\n",
    "    ax.add_artist(ellip)\n",
    "    return ellip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def get_metrics(model, test_data, test_labels):\n",
    "    \"\"\"Returns the cluster evaluation metrics\n",
    "    Args:\n",
    "        model - the model\n",
    "        test_data - the test dataset\n",
    "        test_labels - the test labels\n",
    "    Returns:\n",
    "        a tuple of (homogeniety_score, completeness_score\n",
    "        v_measure_score, adjusted_rand_score, silhouette_score)    \n",
    "    \"\"\"\n",
    "    # Your code here\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(20, 20),\n",
    "                        sharex=True, sharey=True)\n",
    "\n",
    "colors = [plt.cm.Spectral(each)\n",
    "          for each in np.linspace(0, 1, n_classes)]\n",
    "\n",
    "covariance_types = ['spherical', 'diag', 'tied', 'full']\n",
    "\n",
    "for cov_type, ax in zip(covariance_types, axes.flatten()):\n",
    "    gmm = GaussianMixture(n_components=n_classes, covariance_type=cov_type)\n",
    "    gmm.fit(df_data_train_scaled)\n",
    "    ax.scatter(df_data_train_scaled[:, 0], df_data_train_scaled[:, 1],\n",
    "               color='black', marker='x')\n",
    "\n",
    "    homogeneity, completeness, vmeasure, adjusted_rand, silhouette = \\\n",
    "        get_metrics(gmm, df_data_test_scaled, df_labels_test)    \n",
    "\n",
    "    for label, color in enumerate(colors):\n",
    "        cov = get_gmm_cov_matrix(gmm, label)                \n",
    "        position = gmm.means_[label]\n",
    "        plot_cov_ellipse(cov, position, color=color, ax=ax)\n",
    "    \n",
    "    ax.set(title='GMM with \\'%s\\' covariance\\n'\n",
    "           'homogeneity %.2f, completeness %.2f, v-measure %.2f\\n'\n",
    "           'adjusted rand score %.2f, silhouette coefficient %.2f'\n",
    "           % (cov_type, homogeneity, completeness, vmeasure, adjusted_rand,\n",
    "              silhouette),\n",
    "       xticks=(), yticks=())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DBSCAN\n",
    "\n",
    "The final clustering algorithm we'll look at is DBSCAN.\n",
    "\n",
    "The key feature of DBSCAN is that it can find clusters of any shape (not just spherical or convex).\n",
    "\n",
    "Instead of telling it how many clusters to find, DBSCAN can estimate the clusters from the data.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/clustering.html#dbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Walkthrough - DBSCAN on the COE dataset\n",
    "\n",
    "Credits: http://scikit-learn.org/stable/auto_examples/cluster/plot_dbscan.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Data preparation\n",
    "    \n",
    "DBSCAN does not have the concept of 'predict'. \n",
    "\n",
    "We'll use the \"full\" dataset (first 300 values), without splitting train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_all = StandardScaler()\n",
    "df_data_scaled = scaler_all.fit_transform(df_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Fit the model to get:\n",
    "- The core samples that DBSCAN considers as part of a cluster\n",
    "- The cluster labels that DBSCAN found\n",
    "\n",
    "The `eps` and `min_samples` settings are tunable to the dataset, depending on how many clusters you would like DBSCAN to find.\n",
    "\n",
    "More details: `DBSCAN?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "db = DBSCAN(eps=0.3, min_samples=10) # these can be tuned\n",
    "                                     # based on how many clusters\n",
    "                                     # you want DBSCAN to find\n",
    "db.fit(df_data_scaled)\n",
    "\n",
    "core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n",
    "core_samples_mask[db.core_sample_indices_] = True\n",
    "db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Number of clusters in labels, ignoring noise if present.\n",
    "labels_predict = db.labels_\n",
    "n_clusters = len(set(labels_predict)) - (1 if -1 in labels_predict else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Compute the Evaluation Metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "labels_true = df_labels\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels_true, labels_predict))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels_true, labels_predict))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels_true, labels_predict))\n",
    "print(\"Adjusted Rand Index: %0.3f\"\n",
    "      % metrics.adjusted_rand_score(labels_true, labels_predict))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(df_data_scaled, labels_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Plot the clusters found by DBSCAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_dbscan(ax, title, X, labels):\n",
    "\n",
    "    # Black removed and is used for noise instead.\n",
    "    unique_labels = set(labels)\n",
    "    colors = [plt.cm.Spectral(each)\n",
    "              for each in np.linspace(0, 1, len(unique_labels))]\n",
    "    for k, col in zip(unique_labels, colors):\n",
    "        if k == -1:\n",
    "            # Black used for noise.\n",
    "            col = [0, 0, 0, 1]\n",
    "\n",
    "        class_member_mask = (labels == k)\n",
    "\n",
    "        xy = X[class_member_mask & core_samples_mask]\n",
    "        ax.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=14)\n",
    "\n",
    "        xy = X[class_member_mask & ~core_samples_mask]\n",
    "        ax.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n",
    "                 markeredgecolor='k', markersize=6)\n",
    "    ax.set(title=title, xticks=(), yticks=())\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "plot_dbscan(ax, 'COE Quotas vs Premiums, Estimated number of clusters: %d' % n_clusters,\n",
    "            df_data_scaled, labels_predict)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
