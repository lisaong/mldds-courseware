{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering Workshop: Online Retail Dataset\n",
    "\n",
    "Dataset:\n",
    "https://archive.ics.uci.edu/ml/datasets/online+retail\n",
    "\n",
    "Objective:\n",
    "Explore the dataset by finding clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Information:\n",
    "\n",
    "This is a transnational data set which contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail.The company mainly sells unique all-occasion gifts. Many customers of the company are wholesalers.\n",
    "\n",
    "### Attribute Information:\n",
    "- InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation. \n",
    "- StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product. \n",
    "- Description: Product (item) name. Nominal. \n",
    "- Quantity: The quantities of each product (item) per transaction. Numeric.\t\n",
    "- InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated. \n",
    "- UnitPrice: Unit price. Numeric, Product price per unit in sterling. \n",
    "- CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer. \n",
    "- Country: Country name. Nominal, the name of the country where each customer resides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# You can use read_excel, but it requires additional dependencies, and\n",
    "# isn't as easy to use as pd.read_csv\n",
    "\n",
    "df = pd.read_csv('d:/tmp/online-retail/Online Retail.csv', parse_dates=True, encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See what types we need to convert\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation (Estimated time: 60 minutes)\n",
    "\n",
    "\n",
    "### Which of these should we convert to numbers?\n",
    "```\n",
    "InvoiceNo              object -> Label Encode\n",
    "StockCode              object -> Label Encode\n",
    "Description            object -> Tfidf\n",
    "Quantity                int64\n",
    "InvoiceDate            object -> pd.to_datetime\n",
    "UnitPrice             float64\n",
    "CustomerID            float64\n",
    "Country                object -> Label Encode\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Label Encode string columns (except Description)\n",
    "\n",
    "Q: is it better to keep separate encoders or to use separate columns?\n",
    "\n",
    "A: separate encoders are cheaper for large datasets because you would only need to store the label mapping in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Convert InvoiceDate column from string to datetime\n",
    "\n",
    "Try something like:\n",
    "```\n",
    "pd.to_datetime(..., format='%d/%m/%Y %H:%M')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Convert Description to Tf-Idf features\n",
    "\n",
    "Description is fairly simple text, so we can try scikit learn's tokenizer. \n",
    "\n",
    "No need to use spacy to tokenize.\n",
    "\n",
    "```\n",
    "vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',\n",
    "                             max_df=3, min_df=1)\n",
    "```\n",
    "\n",
    "- max_df allows us to skip words that are too frequent\n",
    "- min_df allows us to skip words are are too unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine all converted columns into our dataframe\n",
    "\n",
    "Something like this:\n",
    "\n",
    "```\n",
    "# Recall that TFIDF has each term as a feature\n",
    "\n",
    "df_tfidf = pd.DataFrame(list(description_tfidf.toarray()),  \n",
    "                        columns=vectorizer.get_feature_names(),\n",
    "                        index=df.index)\n",
    "                        \n",
    "# Combine into 1 dataframe\n",
    "\n",
    "df_combined = pd.concat([df, df_tfidf], axis=1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster! (Estimated time: 90 minutes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pick our numeric columns\n",
    "\n",
    "```\n",
    "columns = ['InvoiceNo', 'StockCode', 'Quantity', 'UnitPrice', 'CustomerID', 'Country']\n",
    "\n",
    "columns = columns + vectorizer.get_feature_names()\n",
    "```\n",
    "\n",
    "... Then apply .loc to select them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Pick a subset of datapoints to try clustering\n",
    "\n",
    "(let's say 300 datapoints)\n",
    "\n",
    "(You can always add more datapoints after you have the initial clustering model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Plot the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply PCA to convert to 2-dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Apply KMeans clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Elbow diagram to pick the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Re-plot the PCA plot with cluster centroids for the best k\n",
    "\n",
    "1. Pick the best k\n",
    "\n",
    "2. Do something like this to PCA transform the cluster centers:\n",
    "```\n",
    "centroids_2d = pca.transform(kmeans.cluster_centers_)\n",
    "```\n",
    "\n",
    "3. Re-plot the PCA scatter plot with the cluster centers overlaid.\n",
    "\n",
    "\n",
    "4. You can also colour the scatter plots using the cluster ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Cluster Metrics\n",
    "\n",
    "Since we don't have the labels, we have to use silhouette_score\n",
    "\n",
    "```\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# S=(b-a)/max(a, b)\n",
    "# a: average distance between each sample and samples from the same cluster\n",
    "# b: average distance between each sample and nearest cluster samples\n",
    "\n",
    "print(silhouette_score(X, clusters, sample_size=300, random_state=42))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring data with clusters\n",
    "\n",
    "Now that we have the clusters, we can use pandas to divide the dataset into the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = clusters\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cluster==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cluster==2].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cluster==3].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cluster==1].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cluster==2].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.cluster==3].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
