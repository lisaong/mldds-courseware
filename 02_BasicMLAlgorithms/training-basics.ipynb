{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Basics Workshop\n",
    "![training](assets/training-basics/puppy-training.jpg)\n",
    "\n",
    "(image: amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "General concepts in training models\n",
    "- Loss functions\n",
    "- Gradient descent\n",
    "- Overfitting, underfitting\n",
    "- Regularization\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Objective: a model that trains fast and performs well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not an exhaustive list. We'll encounter more as we go over the different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss Functions\n",
    "\n",
    "What they are: a metric of how far away the predictions are from the truth\n",
    "\n",
    "For example:\n",
    "\n",
    "![MSE](http://scikit-learn.org/stable/_images/math/44f36557fef9b30b077b21550490a1b9a0ade154.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a.k.a.:\n",
    "- Objective function\n",
    "- Cost function\n",
    "- Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definitions\n",
    "\n",
    "$$x^* = \\arg \\min L(x)$$\n",
    "\n",
    "where $x^*$ = value that minimizes the loss function $L(x)$\n",
    "\n",
    "The process of finding $x^*$ is called \"Optimization\". It usually involves running some type of Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss Function Examples\n",
    "\n",
    "Scikit-learn:\n",
    "- [Mean squared error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error): `sklearn.metrics.mean_squared_error(y_true, y_pred)`\n",
    "- [Log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss): `sklearn.metrics.log_loss(y_true, y_pred)`\n",
    "- [Zero one loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss)\n",
    "`sklearn.metrics.zero_one_loss(y_true, y_pred)`\n",
    "- etc\n",
    "\n",
    "Keras:\n",
    "- https://keras.io/losses/\n",
    "- `keras.losses.mean_squared_error(y_true, y_pred)`\n",
    "- `keras.losses.binary_crossentropy(y_true, y_pred)`\n",
    "- etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Log Loss\n",
    "# Equation from: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "p = np.linspace(0.01, 0.99, 50) # avoid 0 and 1 because of div by zero\n",
    "y_1 = np.ones(p.shape)\n",
    "y_0 = np.zeros(p.shape)\n",
    "\n",
    "# sklearn.metrics.log_loss returns a number, because\n",
    "# it's a metric across test samples.\n",
    "# So we implement our log_loss equation here: \n",
    "def log_loss(y, p):\n",
    "    return -(y * np.log(p) + (1 - y) * np.log(1 - p))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(p, log_loss(y_1, p), label='log loss for y=1')\n",
    "ax.plot(p, log_loss(y_0, p), label='log loss y=0')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradient Descent\n",
    "\n",
    "What it is: technique for minimizing loss function for a given model\n",
    "\n",
    "Objective: find $w^*$ such that $$w^* = \\underset{w}\\arg \\min{L\\big(y_{true}, y_{pred}\\big)}$$\n",
    "\n",
    "$$w^* = \\underset{w}\\arg \\min{L\\big(y_{true}, f(x, w)\\big)}$$\n",
    "\n",
    "\n",
    "where\n",
    "- $L(...)$ is the loss function\n",
    "- $w$ are the weights\n",
    "- $f(x, w)$ is the model that computes $y_{pred}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent algorithm\n",
    "\n",
    "1. Initialize $w$ to some value (e.g. random)\n",
    "2. Compute gradient of $L\\big(y_{true}, f(x, w)\\big)$\n",
    "3. Update $w$ by a \"tiny factor\" in the negative of the gradient\n",
    "4. Repeat 2-3 until we reach the \"stopping criteria\" (more on this later)\n",
    "\n",
    "The \"tiny factor\" is known as the \"learning rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Learning Rate and Convergence\n",
    "\n",
    "![learning rate](assets/training-basics/learningrates.jpg)\n",
    "\n",
    "(image: https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_optimization.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Workshop: Gradient descent. Animated\n",
    "\n",
    "![wake up](assets/training-basics/descend.jpg)\n",
    "\n",
    "(training incantation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Environment Setup\n",
    "\n",
    "Create a new environment called `mldds02`. You may also reuse `mldds01`, but it's good to keep separate environments for different experiments.\n",
    "\n",
    "```\n",
    "conda create -n mldds02 python=3\n",
    "conda activate mldds02\n",
    "\n",
    "(mldds02) conda install jupyter numpy pandas matplotlib scikit-learn\n",
    "(mldds02) conda install -c conda-forge ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Credits: https://jed-ai.github.io/py1_gd_animation/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Example gradient descent implementation\"\"\"\n",
    "\n",
    "def func_y(x):\n",
    "    \"\"\"A demonstrative loss function that happens to be convex (has global a minimum)\n",
    "    Args:\n",
    "        x - the input (can be the weights of a machine learning algorithm)\n",
    "    Returns:\n",
    "        The loss value\n",
    "    \"\"\"\n",
    "    return x**2 - 4*x + 2\n",
    "\n",
    "def gradient_func_y(x):\n",
    "    \"\"\"The gradient of func_y\n",
    "    Args:\n",
    "        x - the input\n",
    "    Returns:\n",
    "        The gradient value\n",
    "    \"\"\"\n",
    "    return 2*x - 4 # d(x^2 - 4x + 2)/dx = 2x - 4\n",
    "\n",
    "def gradient_descent(previous_x, learning_rate, epochs):\n",
    "    \"\"\"An implementation of gradient descent\n",
    "    Args:\n",
    "        previous_x - the previous input value\n",
    "        learning_rate - how much to change x per iteration\n",
    "        epochs - number of steps to run gradient descent\n",
    "    Returns:\n",
    "        A tuple: array of x values, array of loss values\n",
    "    \"\"\"\n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(func_y(previous_x))\n",
    "    \n",
    "    # loop to update x and y\n",
    "    for i in range(epochs):\n",
    "        # x = lr * gradient(func(prev_x))\n",
    "        update = learning_rate *gradient_func_y(previous_x)\n",
    "        x = previous_x - update\n",
    "        print('step', i, 'previous x', previous_x,\n",
    "              'update:', -update, 'new x:', x)\n",
    "        x_gd.append(x)\n",
    "        y_gd.append(func_y(x))\n",
    "        \n",
    "        # update previous_x\n",
    "        previous_x = x\n",
    "    \n",
    "    return x_gd, y_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "With gradient descent implemented, we'll will now run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x0 = 0.7\n",
    "learning_rate = 0.15\n",
    "epochs = 10\n",
    "\n",
    "x = np.arange(-1, 5, 0.01)\n",
    "y = func_y(x)\n",
    "x_gd, y_gd = gradient_descent(x0, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Plot the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim([min(x), max(x)])\n",
    "ax.set_ylim([min(y)-1, max(y)+1])\n",
    "ax.plot(x, y, lw = 0.9, color = 'k')\n",
    "\n",
    "line, = ax.plot([], [], 'r', label = 'Gradient descent', lw = 1.5)\n",
    "point, = ax.plot([], [], 'bo', animated=True)\n",
    "value_display = ax.text(0.02, 0.02, '', transform=ax.transAxes)\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initializes the animation\"\"\"\n",
    "    line.set_data([], [])\n",
    "    point.set_data([], [])\n",
    "    value_display.set_text('')\n",
    "\n",
    "    return line, point, value_display\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"Animates the plot at step i\n",
    "    Args:\n",
    "        i: the step to animate\n",
    "        return: a tuple of line, point, and value_display\n",
    "    \"\"\"\n",
    "    # Animate line\n",
    "    line.set_data(x_gd[:i], y_gd[:i])\n",
    "    \n",
    "    # Animate points\n",
    "    point.set_data(x_gd[i], y_gd[i])\n",
    "\n",
    "    # Animate value display\n",
    "    value_display.set_text('Min = ' + str(y_gd[i]))\n",
    "\n",
    "    return line, point, value_display\n",
    "\n",
    "# call the animator\n",
    "rc('animation', html='html5')\n",
    "anim = FuncAnimation(fig, animate, init_func=init,\n",
    "                     frames=len(x_gd), interval=360,\n",
    "                     repeat_delay=60, blit=True)\n",
    "\n",
    "# display the video\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. Try initializing x0 to something > 2, what do you observe?\n",
    "2. Try increasing the learning_rate to something large like 10. Does the gradient still converge?\n",
    "3. Replace func_y and gradient_func_y above with a cubic function. What do you observe?\n",
    "  ```\n",
    "  y = x^3 - 5x^2 + x + 1\n",
    "  gradient(y) = 3x^2 - 10x + 1\n",
    "  ```\n",
    "4. Replace func_y and gradient_func_y with `cos(x)` and its derivative `-sin(x)`. What do you observe?  What needs to reach convergence?\n",
    "  ```\n",
    "  y = np.tan(x)\n",
    "  gradient(y) = -np.sin(x)\n",
    "  ```\n",
    "  \n",
    "Derivative formulas: https://www.derivative-calculator.net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent Variants\n",
    "\n",
    "1. Stochastic Gradient Descent (SGD)\n",
    "2. Minibatch SGD\n",
    "3. Minibatch SGD with Momentum\n",
    "4. Adaptive Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "\"Regular\" Gradient Descent is expensive because it processes all samples at once\n",
    "- Imagine you have millions of training samples\n",
    "\n",
    "Stochastic Gradient Descent speeds this up by:\n",
    "- Running gradient descent, one randomly selected training sample at a time\n",
    "- Stochastic: random noise, because samples can vary a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notation\n",
    "\n",
    "$\\leftarrow$ = replace value\n",
    "Some texts use this symbol $:=$\n",
    "\n",
    "Examples\n",
    "- $\\theta \\leftarrow \\theta - \\epsilon g$\n",
    "- $\\theta := \\theta - \\epsilon g$\n",
    "\n",
    "Means\n",
    "1. Compute $\\theta' = \\theta - \\epsilon g$\n",
    "2. Update $\\theta = \\theta'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![sgd](assets/training-basics/sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Symbols:\n",
    "- The $\\eta_t$ denotes the learning rate\n",
    "- Note: $\\Theta$ is denotes the weights matrix\n",
    "\n",
    "(image: Neural Networks in Natural Language Processing, Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minibatch Stochastic Gradient Descent\n",
    "\n",
    "Instead of 1 random sample at a time:\n",
    "- Sample a \"minibatch\" of m training samples\n",
    "- Run gradient descent on that minibatch\n",
    "- \"Smooths\" out the randomness by operating on a minibatch.\n",
    "- The minibatch size can be tuned (\"hyperparameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![minibatch sgd](assets/training-basics/minibatch-sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Symbols:\n",
    "- $\\epsilon_k$ denotes the learning rate\n",
    "- $\\theta$ denotes the weights matrix\n",
    "- $\\nabla_{\\theta}$ means gradient w.r.t. $\\theta$\n",
    "\n",
    "(image: Deep Learning, Goodfellow, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minibatch SGD, with momentum\n",
    "\n",
    "Speeds up minibatch SGD by:\n",
    "- Applying an exponentially decaying moving average of the previous gradients ($v$)\n",
    "  - if gradients point the same way, will reach minimum faster\n",
    "- Minibatch SGD: $\\theta \\leftarrow \\theta - \\epsilon g$\n",
    "- Minibatch SGD + momentum: $\\theta \\leftarrow \\theta - \\epsilon g + \\alpha v$, $v \\leftarrow v - \\epsilon g$\n",
    "\n",
    "Variant: Nesterov's momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![minibatch sgd with momentum](assets/training-basics/minibatch-sgd-momentum.png)\n",
    "\n",
    "(image: Deep Learning, Goodfellow, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adaptive Learning Rate Strategies\n",
    "\n",
    "- Learning rate will control the amount of gradient update\n",
    "  - Large learning rate: risk overshoot and not converge\n",
    "  - Small learning rate: too slow\n",
    "  - Ideal: start large(r), then reduce as we get closer to minima\n",
    "- Strategies\n",
    "  - Constant learning rate\n",
    "  - Time-based or step-based decay\n",
    "  - AdaGrad\n",
    "  - RMSProp\n",
    "  - Adam\n",
    "- What works best depends on your domain (true for **any** optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Comparison](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png)\n",
    "\n",
    "(image: [machine learning mastery](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Another comparison](http://scikit-learn.org/stable/_images/sphx_glr_plot_mlp_training_curves_001.png)\n",
    "\n",
    "(image: [scikit-learn](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Speed of Convergence\n",
    "\n",
    "![optimizers](assets/training-basics/OtherOptimizers.gif)\n",
    "\n",
    "(image: https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_optimization.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Underfiting, Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goals of training\n",
    "1. Minimize training error\n",
    "2. Minimize gap between test error and training error. (the \"generalization gap\")\n",
    "\n",
    "(Reference: Deep Learning - Goodfellow, Bengio, Courville, MIT press, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalization\n",
    "\n",
    "How well will the model handle data not found during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting, Overfitting\n",
    "\n",
    "1. A model is set to \"underfit\" if the training error is too large\n",
    "2. A model is set to \"overfit\" if the gap between test and training error is too large\n",
    "\n",
    "A good-fit model: neither underfit nor overfit (Goldilocks and the Three Bears)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Underfitting / High Bias\n",
    "\n",
    "A model is set to \"underfit\" if the training error is too large\n",
    "  - Means model does not work\n",
    "  - It learnt nothing\n",
    "  - Mode has \"high bias\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting / High Variance\n",
    "\n",
    "A model is set to \"overfit\" if the gap between test and training error is too large\n",
    "  - Means model does not generalize well\n",
    "  - It only learnt the training set (or learnt too much training set noise)\n",
    "  - Model has \"high variance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![comics](https://imgs.xkcd.com/comics/linear_regression.png)\n",
    "\n",
    "(image: xkcd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixing Underfitting\n",
    "\n",
    "Symptom: poor model performance even on training data\n",
    "\n",
    "Potential cures:\n",
    "- More data\n",
    "- More features and/or different features\n",
    "- Different algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Fixing Overfitting\n",
    "\n",
    "Symptom: model has good performance in training data, but poor generalization on test data\n",
    "\n",
    "Potential cures:\n",
    "- Early stoppping\n",
    "- Fewer features\n",
    "- Regularization\n",
    "- Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: SGD, Overfitting, Underfitting\n",
    "\n",
    "In this workshop, we will try to fit a more complex linear regression model.\n",
    "\n",
    "We'll explore the following concepts:\n",
    "- Loss functions\n",
    "- Stochastic Gradient Descent\n",
    "- Underfitting and Overfitting\n",
    "\n",
    "Inspiration:\n",
    "https://sdsawtelle.github.io/blog/output/week6-andrew-ng-machine-learning-with-python.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Prediction Tasks\n",
    "\n",
    "We'll try to fit a curve to predict, for recent Singapore University Graduates:\n",
    "```\n",
    "y: Gross Monthly Median Salary (S$)\n",
    "x: Overall Employment Rate (%)\n",
    "```\n",
    "\n",
    "To illustrate underfitting/overfitting, we'll explore:\n",
    "- A first-order linear model: $y = w^Tx$\n",
    "- A linear model with polynomial features\n",
    "$$y = w^TX$$\n",
    "where $X = [1, x, x^2, x^3, ...]$\n",
    "\n",
    "We'll also explore different loss functions and plotting of loss curves during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dataset(s)\n",
    "\n",
    "1. Download this dataset: https://data.gov.sg/dataset/graduate-employment-survey-ntu-nus-sit-smu-sutd\n",
    "2. Unzip it and note the path for use in `pandas.read_csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 1. Data Transformation and Cleaning\n",
    "\n",
    "The first step is to inspect the data to see what transformation/cleaning is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# ISO-8859-1 encoding is needed\n",
    "#   UnicodeDecodeError: 'utf-8' codec can't decode byte 0xe4 in position 20: invalid continuation byte\n",
    "\n",
    "df = pd.read_csv('D:/tmp/graduate-employment-survey-ntu-nus-sit-smu-sutd/graduate-employment-survey-ntu-nus-sit-smu-sutd.csv',\n",
    "                 encoding='ISO-8859-1',\n",
    "                 usecols=['university', 'employment_rate_overall', 'gross_monthly_median'])\n",
    "\n",
    "# filter by NUS to keep things simple\n",
    "df = df.loc[df.university == 'National University of Singapore']\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The columns to inspect are:\n",
    "- employment_rate_overall\n",
    "- gross_monthly_median\n",
    "\n",
    "We are looking for:\n",
    "- invalid values\n",
    "- data types to transform to numeric\n",
    "- feature ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sample size is fairly small, we can inspect using .unique()\n",
    "print(df.describe())\n",
    "print('')\n",
    "print(df.employment_rate_overall.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "`employment_rate_overall` column\n",
    "- invalid values: yes, there is an 'na' value above\n",
    "- data type is object (these are string values)\n",
    "  - need to transform to numbers\n",
    "  - How: use [pandas.to_numeric](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html) to convert to numbers\n",
    "- range: 0-100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "If the dataset is large, it is hard to find the 'na' values.\n",
    "\n",
    "A quick test is to try converting the column first, to see if errors appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.to_numeric(df.employment_rate_overall) # see if we get parse errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We will be using the `errors='coerce'` option in [pandas.to_numeric](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_numeric.html) when we do the actual conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_numeric(df.employment_rate_overall, errors='coerce') # 'na' becomes NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The `gross_monthly_median` column is handled the same way.\n",
    "\n",
    "Let's now perform the data transformation and cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# transform string to numbers, forcing invalid values to float NaN\n",
    "data = {\n",
    "    'gross_monthly_median': pd.to_numeric(df.gross_monthly_median, errors='coerce'),\n",
    "    'employment_rate_overall': pd.to_numeric(df.employment_rate_overall, errors='coerce')\n",
    "}\n",
    "\n",
    "# drop NaNs\n",
    "df_dataset = pd.DataFrame(data).dropna()\n",
    "\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Visualize Dataset\n",
    "Let's plot the dataset to visualize our data.\n",
    "\n",
    "We'll use a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "ax.scatter(df_dataset.employment_rate_overall, df_dataset.gross_monthly_median)\n",
    "\n",
    "ax.set(title='Gross Monthly Median Income and Employment Rate for Recent NUS Graduates',\n",
    "       xlabel='Employment Rate (%)',\n",
    "       ylabel='Gross Monthly Median Income (S$)')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Inspecting the plot above, we can roughly make out a curve through it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2. Feature selection\n",
    "    \n",
    "#### Inputs\n",
    "\n",
    "|Feature|Description|Column name|Transformation before model input?|\n",
    "|--|--|--|\n",
    "|$x_1$|Overall Employment Rate (%)|employment_rate_overall|pd.to_numeric|\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "|Output|Description|Truth column|Transformation from model output?|\n",
    "|--|--|--|\n",
    "|$\\hat{y}$|Predicted Gross Monthly Medium Income S\\$|$y$ = gross_monthly_median|None, output will stay numeric|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Prepare the numpy arrays for use with sklearn\n",
    "X = df_dataset.loc[:, 'employment_rate_overall']\n",
    "print('X.shape:', X.shape)\n",
    "\n",
    "y = df_dataset.loc[:, 'gross_monthly_median']\n",
    "print('y.shape:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### 3. Model creation and training, and validation\n",
    "\n",
    "In this section, we will\n",
    "1. Shuffle and then split dataset into train and test\n",
    "2. Perform feature scaling\n",
    "3. Train and validate our models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Dataset shuffling and train-test-split\n",
    "\n",
    "In a previous workshop, we did the randomization and train-test split manually. \n",
    "\n",
    "Turns out, [sklearn.model_selection.train_test_split](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) can do this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train-test split, witholding 15% for test data\n",
    "# shuffle=True is the default\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "X_train.head() # view the shuffled dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Feature scaling\n",
    "\n",
    "Since we are using Stochastic Gradient Descent, we are recommended to scale the features between [0, 1] or [-1, 1].\n",
    "- See [Tips on Practical Use for SGDRegressor](http://scikit-learn.org/stable/modules/sgd.html#tips-on-practical-use).\n",
    "\n",
    "- Scaling subtracts the mean and divides by the standard deviation\n",
    "- We will scale both the input and output values, using [sklearn.preprocessing.StandardScaler](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n",
    "- In both cases (input and output), the mean and standard deviation will be based on the training set only. This way we don't look at the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We train the scaler based on the training set, and use\n",
    "# it for both the training and test sets\n",
    "#\n",
    "# Reason: the test set shouldn't pollute the dataset\n",
    "x_scaler = StandardScaler()\n",
    "x_scaler.fit(X_train.values.reshape(-1, 1)) # must be 2D array\n",
    "X_train_scaled = x_scaler.transform(X_train.values.reshape(-1, 1))\n",
    "X_test_scaled = x_scaler.transform(X_test.values.reshape(-1, 1))\n",
    "\n",
    "y_scaler = StandardScaler()\n",
    "y_scaler.fit(y_train.values.reshape(-1, 1))\n",
    "y_train_scaled = y_scaler.transform(y_train.values.reshape(-1, 1))[:, 0]\n",
    "y_test_scaled = y_scaler.transform(y_test.values.reshape(-1, 1))[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train.head() # before scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_train_scaled[:5] # after scaling (showing first 5 values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_train.head() # before scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "y_train_scaled[:5] # after scaling (showing first 5 values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Underfit Model: First Order Linear Regressor\n",
    "\n",
    "For the first order linear regressor, we'll use `sklearn.linear_model.SGDRegressor` with the dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "\n",
    "# SGDRegressor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "linear_model = SGDRegressor(verbose=1,\n",
    "                            random_state=np.random.RandomState(123),\n",
    "                            loss='squared_loss', # mean squared loss\n",
    "                            penalty='none', # no regularization\n",
    "                            max_iter=100,\n",
    "                            tol=1e-3, # stopping condition\n",
    "                            eta0=0.01, # learning rate\n",
    "                            learning_rate='constant') # learning rate schedule\n",
    "%time linear_model.fit(X_train_scaled, y_train_scaled)\n",
    "\n",
    "print('Coefficient', linear_model.coef_)\n",
    "print('Intercept', linear_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Model Validation\n",
    "\n",
    "Let's validate our model's performance by computing metrics and plotting the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "pred_scaled = linear_model.predict(X_test_scaled)\n",
    "print('Truth:', y_test_scaled)\n",
    "print('Predictions:', pred_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "print('MSE:', mean_squared_error(y_test_scaled, pred_scaled))\n",
    "print('R2:', r2_score(y_test_scaled, pred_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "#### Model Visualization\n",
    "\n",
    "We'll visualize the entire dataset (train + test), by plotting the data as a sactter plot, and the linear function as a curve or line.\n",
    "\n",
    "The model needs to be called with pre- and post-processing stages:\n",
    "1. Scale the input\n",
    "2. Call `predict`\n",
    "3. Unscale the output\n",
    "\n",
    "In both steps, we'll use the already-fitted scalers from previously. This ensures that the pre- and post-processing matches exactly with how the model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# First, call the model in 3 stages\n",
    "# 1. Pre-process the input\n",
    "X_scaled = x_scaler.transform(X.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Predict\n",
    "y_pred_scaled = linear_model.predict(X_scaled)\n",
    "\n",
    "# 3. Post-process the output\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "# Finally, plot dataset and predictions\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.scatter(X.values, y, label='truth')\n",
    "ax.plot(X.values, y_pred, label='predictions', color='orange')\n",
    "\n",
    "ax.set(title='Gross Monthly Median Income and Employment Rate for Recent NUS Grads: First-Order Model',\n",
    "       xlabel='Employment Rate (%)',\n",
    "       ylabel='Gross Monthly Median Income (S$)')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Polynomial Regression\n",
    "\n",
    "A linear model is obviously underfit, because it tries to draw a straight line. Let's try a curve instead (polynomial model).\n",
    "\n",
    "To generate polynomial features from x (Employment Rate), we use [sklearn.preprocessing.PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(12)\n",
    "poly.fit(X_train.values.reshape(-1, 1))\n",
    "\n",
    "X_train_poly = poly.transform(X_train.values.reshape(-1, 1))\n",
    "X_test_poly = poly.transform(X_test.values.reshape(-1, 1))\n",
    "\n",
    "# Scale the features back to SGD happy ranges ([-1, 1])\n",
    "x_scaler_poly = StandardScaler()\n",
    "x_scaler_poly.fit(X_train_poly)\n",
    "X_train_poly_scaled = x_scaler_poly.transform(X_train_poly)\n",
    "X_test_poly_scaled = x_scaler_poly.transform(X_test_poly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "poly_model = SGDRegressor(verbose=1,\n",
    "                          random_state=np.random.RandomState(123),\n",
    "                          loss='squared_loss', # mean squared loss\n",
    "                          penalty='none', # no regularization\n",
    "                          max_iter=100,\n",
    "                          tol=1e-4, # stopping condition\n",
    "                          eta0=0.01, # learning rate\n",
    "                          learning_rate='constant') # learning rate schedule\n",
    "%time poly_model.fit(X_train_poly_scaled, y_train_scaled)\n",
    "\n",
    "print('Coefficients', poly_model.coef_)\n",
    "print('Intercept', poly_model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise: Model Validation and Visualization\n",
    "\n",
    "Follow the pattern above to validate and visualize the new model.\n",
    "\n",
    "This means two tasks:\n",
    "1. Computing metrics\n",
    "2. Plotting the curve (hint: use a scatter plot for both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot curve\n",
    "\n",
    "# First, call the model in 3 stages\n",
    "# 1. Pre-process the input\n",
    "# Hint: you need to create the polynomial, then scale\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 2. Predict\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3. Post-process the output by un-scaling y\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Finally, plot dataset and predictions\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Learning Curves for Underfit/Overfit Detection\n",
    "\n",
    "Even with the above polynomial curve, it's hard to tell if we are truly overfitting.\n",
    "\n",
    "The standard practice to determine model fit is to plot learning curves of the training scores and test scores during training iterations.\n",
    "\n",
    "Let's compare the learning curves for both the linear and polynomial case.\n",
    "\n",
    "Reference: http://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# learning_curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(axis, title, tr_sizes, tr_scores, val_scores):\n",
    "    \"\"\"Plots the learning curve for a training session\n",
    "    Arg:\n",
    "        axis: axis to plot\n",
    "        title: plot title\n",
    "        tr_sizes: sizes of the training set\n",
    "        tr_scores: training scores\n",
    "        val_scores: validation scores\n",
    "    \"\"\"\n",
    "    tr_scores_mean = np.mean(tr_scores, axis=1)\n",
    "    tr_scores_std = np.std(tr_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    axis.fill_between(tr_sizes, tr_scores_mean - tr_scores_std,\n",
    "                    tr_scores_mean + tr_scores_std, alpha=0.1,\n",
    "                    color=\"r\")\n",
    "    axis.fill_between(tr_sizes, val_scores_mean - val_scores_std,\n",
    "                    val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    axis.plot(tr_sizes, tr_scores_mean, 'o-', color=\"r\",\n",
    "            label=\"Training score\")\n",
    "    axis.plot(tr_sizes, val_scores_mean, 'o-', color=\"g\",\n",
    "            label=\"Cross-validation score\")\n",
    "    axis.set(title=title,\n",
    "           xlabel='Training examples',\n",
    "           ylabel='R2 Scores')\n",
    "    axis.grid()\n",
    "    axis.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the learning curve for linear_model\n",
    "# By default this uses 3-fold Cross Validation (more on that later)\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    linear_model, X_train_scaled, y_train_scaled,\n",
    "    random_state=np.random.RandomState(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the learning curve, along with the stats\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "plot_learning_curve(ax, '1st Order Model',\n",
    "                    train_sizes, train_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: Plot Learning Curve\n",
    "\n",
    "Follow the example above and:\n",
    "1. Generate the learning curve for the polynomial model (`poly_model`)\n",
    "2. Plot the learning curve and the statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Generate the learning curve for poly_model\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Plot the learning curve, along with the stats\n",
    "#\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Observations\n",
    "\n",
    "The learning curves show:\n",
    "- Both models are underfit: the training scores are very low. \n",
    "  - Why? The data is quite dispersed, and linear regression is probably not a good way to model this \n",
    "- The polynomial model generalizes slightly better\n",
    "  - It has a narrower gap between training scores and validation scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Robust Linear Regression against Outliers\n",
    "\n",
    "The most obvious problem with the data is the presence of lots of outliers.\n",
    "\n",
    "There are some models that are a bit more robust against outliers. One of them is the RANdom SAmple Consensus model (RANSAC), which can be trained using a baseline estimator to split the data into two sets: inliers and outliers.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/linear_model.html#ransac-random-sample-consensus\n",
    "\n",
    "Let's give it a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RANSACRegressor\n",
    "\n",
    "ransac = RANSACRegressor(poly_model,\n",
    "                         random_state=np.random.RandomState(123))\n",
    "\n",
    "%time ransac.fit(X_train_poly_scaled, y_train_scaled)\n",
    "\n",
    "pred_scaled = ransac.predict(X_test_poly_scaled)\n",
    "print('Truth:', y_test_scaled)\n",
    "print('Predictions:', pred_scaled)\n",
    "\n",
    "# Compute metrics\n",
    "print('MSE:', mean_squared_error(y_test_scaled, pred_scaled))\n",
    "print('R2:', r2_score(y_test_scaled, pred_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "inlier_mask = ransac.inlier_mask_\n",
    "outlier_mask = np.logical_not(inlier_mask)\n",
    "\n",
    "X_poly = poly.transform(X.values.reshape(-1, 1))\n",
    "X_poly_scaled = x_scaler_poly.transform(X_poly)\n",
    "y_pred_scaled = ransac.predict(X_poly_scaled)\n",
    "y_pred = y_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "ax.scatter(X_train[inlier_mask], y_train[inlier_mask],\n",
    "           color='cornflowerblue', marker='o', label='inliers')\n",
    "ax.scatter(X_train[outlier_mask], y_train[outlier_mask],\n",
    "           color='red', marker='x', label='outliers')\n",
    "ax.scatter(X.values, y_pred,\n",
    "           color='orange', label='predictions')\n",
    "\n",
    "ax.set(title='Gross Monthly Median Income and Employment Rate: RANSAC with 12th Degree Baseline',\n",
    "       xlabel='Employment Rate (%)',\n",
    "       ylabel='Gross Monthly Median Income (S$)')\n",
    "\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    ransac, X_train_poly_scaled, y_train_scaled,\n",
    "    train_sizes=[0.25, 0.5, 0.75, 1],\n",
    "    random_state=np.random.RandomState(123))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "plot_learning_curve(ax, 'RANSAC Model',\n",
    "                    train_sizes, train_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Cross Validation\n",
    "\n",
    "Cross validation was used when plotting the learning curves. Here's what K-fold Cross Validation does:\n",
    "1. Partition training samples into K equal-sized groups\n",
    "2. Pick a group for each training iteration:\n",
    " - train with the remaining K-1 groups\n",
    " - validate with this group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![cv](https://upload.wikimedia.org/wikipedia/commons/1/1c/K-fold_cross_validation_EN.jpg)\n",
    "(image: wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Benefits:\n",
    "- Better measure of generalization than sticking to the same training set\n",
    "- Helps smaller datasets train better by varying training set\n",
    "\n",
    "More explanation: http://scikit-learn.org/stable/modules/cross_validation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "Regularization is applied when there is overfitting. \n",
    "\n",
    "- It's a heuristic.\n",
    "- Adds a large constant value $\\lambda$ * some aggregate of weights\n",
    "- Penalizes the weights, so that they won't fit as well to training data as models get more complicated\n",
    "\n",
    "L1 regularization:\n",
    "\n",
    "![l1](assets/training-basics/least_squares_l1.png)\n",
    "\n",
    "L2 regularization:\n",
    "\n",
    "![l2](assets/training-basics/least_squares_l2.png)\n",
    "\n",
    "(images: chioka.in)\n",
    "\n",
    "To illustrate this, let's try adding regularization to the Polynomial Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
    "\n",
    "print('Regularization parameter: 100')\n",
    "\n",
    "poly_model = SGDRegressor(penalty='l1',\n",
    "                          alpha=100,\n",
    "                          tol=1e-4,\n",
    "                          max_iter=1000,\n",
    "                          random_state=np.random.RandomState(123))\n",
    "\n",
    "poly_model.fit(X_train_poly_scaled, y_train_scaled)\n",
    "\n",
    "print('Coefficients', poly_model.coef_)\n",
    "print('Intercept', poly_model.intercept_)\n",
    "\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    poly_model, X_train_poly_scaled, y_train_scaled,\n",
    "    random_state=np.random.RandomState(123))\n",
    "\n",
    "plot_learning_curve(ax[0], 'Regularization Type: L1 (alpha = 100)',\n",
    "                    train_sizes, train_scores, validation_scores)\n",
    "\n",
    "print('Regularization parameter: 0.05')\n",
    "\n",
    "poly_model = SGDRegressor(penalty='l1',\n",
    "                          alpha=0.05,\n",
    "                          tol=1e-4,\n",
    "                          max_iter=1000,\n",
    "                          random_state=np.random.RandomState(123))\n",
    "\n",
    "poly_model.fit(X_train_poly_scaled, y_train_scaled)\n",
    "\n",
    "print('Coefficients', poly_model.coef_)\n",
    "print('Intercept', poly_model.intercept_)\n",
    "\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    poly_model, X_train_poly_scaled, y_train_scaled,\n",
    "    random_state=np.random.RandomState(123))\n",
    "\n",
    "plot_learning_curve(ax[1], 'Regularization Type: L1 (alpha = 0.05)',\n",
    "                    train_sizes, train_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Reading List\n",
    "\n",
    "|Material|Read it for|URL|\n",
    "|--|--|--|\n",
    "|Optimization - Artificial Intelligence|Overview of Gradient Descent|https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/model_optimization.html|\n",
    "|Chapter 5, Pages 107-119|Capacity, Overfitting and Underfitting|http://www.deeplearningbook.org/contents/ml.html|\n",
    "|Machine Learning Explained: Regularization|Graphical comparison of different types of Regularization|http://enhancedatascience.com/2017/07/04/machine-learning-explained-regularization/|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
