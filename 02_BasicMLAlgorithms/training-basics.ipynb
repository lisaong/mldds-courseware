{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![training](assets/training-basics/puppy-training.jpg)\n",
    "\n",
    "(image: amazon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Training basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Concepts in training models\n",
    "- Loss functions\n",
    "- Gradient descent\n",
    "- Overfitting, underfitting, bias, variance\n",
    "- Regularization\n",
    "- Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Objective: a model that trains fast and performs well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Not an exhaustive list. We'll encounter more as we go over the different algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. Loss Functions\n",
    "\n",
    "What they are: a metric of how far away the predictions are from the truth\n",
    "\n",
    "For example:\n",
    "\n",
    "![MSE](http://scikit-learn.org/stable/_images/math/44f36557fef9b30b077b21550490a1b9a0ade154.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "a.k.a.:\n",
    "- Objective function\n",
    "- Cost function\n",
    "- Error function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Definitions\n",
    "\n",
    "$$x^* = \\arg \\min L(x)$$\n",
    "\n",
    "where $x^*$ = value that minimizes the loss function $L(x)$\n",
    "\n",
    "The process of finding $x^*$ is called \"Optimization\". It usually involves running some type of Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Loss Function Examples\n",
    "\n",
    "Scikit-learn:\n",
    "- [Mean squared error](http://scikit-learn.org/stable/modules/model_evaluation.html#mean-squared-error): `sklearn.metrics.mean_squared_error(y_true, y_pred)`\n",
    "- [Log loss](http://scikit-learn.org/stable/modules/model_evaluation.html#log-loss): `sklearn.metrics.log_loss(y_true, y_pred)`\n",
    "- [Zero one loss](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.zero_one_loss.html#sklearn.metrics.zero_one_loss)\n",
    "`sklearn.metrics.zero_one_loss(y_true, y_pred)`\n",
    "- etc\n",
    "\n",
    "Keras:\n",
    "- https://keras.io/losses/\n",
    "- `keras.losses.mean_squared_error(y_true, y_pred)`\n",
    "- `keras.losses.binary_crossentropy(y_true, y_pred)`\n",
    "- etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2. Gradient Descent\n",
    "\n",
    "What it is: technique for minimizing loss function for a given model\n",
    "\n",
    "Objective: find $w^*$ such that $$w^* = \\underset{w}\\arg \\min{L\\big(y_{true}, y_{pred}\\big)}$$\n",
    "\n",
    "$$w^* = \\underset{w}\\arg \\min{L\\big(y_{true}, f(x, w)\\big)}$$\n",
    "\n",
    "\n",
    "where\n",
    "- $L(...)$ is the loss function\n",
    "- $w$ are the weights\n",
    "- $f(x, w)$ is the model that computes $y_{pred}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient descent algorithm\n",
    "\n",
    "1. Initialize $w$ to some value (e.g. random)\n",
    "2. Compute gradient of $L\\big(y_{true}, f(x, w)\\big)$\n",
    "3. Update $w$ by a \"tiny factor\" in the negative of the gradient\n",
    "4. Repeat 2-3 until we reach the \"stopping criteria\" (more on this later)\n",
    "\n",
    "The \"tiny factor\" is known as the \"learning rate\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Workshop: Gradient descent. Animated\n",
    "\n",
    "![wake up](assets/training-basics/descend.jpg)\n",
    "\n",
    "(training incantation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Environment Setup\n",
    "\n",
    "Create a new environment called `mldds02`. You may also reuse `mldds01`, but it's good to keep separate environments for different experiments.\n",
    "\n",
    "```\n",
    "conda create -n mldds02 python=3\n",
    "conda activate mldds02\n",
    "\n",
    "(mldds02) conda install jupyter numpy pandas matplotlib scikit-learn\n",
    "(mldds02) conda install -c conda-forge ffmpeg\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Credits: https://jed-ai.github.io/py1_gd_animation/\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\"\"\"Example gradient descent implementation\"\"\"\n",
    "\n",
    "def func_y(x):\n",
    "    \"\"\"A demonstrative loss function that happens to be convex (has global a minimum)\n",
    "    Args:\n",
    "        x - the input (can be the weights of a machine learning algorithm)\n",
    "    Returns:\n",
    "        The loss value\n",
    "    \"\"\"\n",
    "    return x**2 - 4*x + 2\n",
    "\n",
    "def gradient_func_y(x):\n",
    "    \"\"\"The gradient of func_y\n",
    "    Args:\n",
    "        x - the input\n",
    "    Returns:\n",
    "        The gradient value\n",
    "    \"\"\"\n",
    "    return 2*x - 4 # d(x^2 - 4x + 2)/dx = 2x - 4\n",
    "\n",
    "def gradient_descent(previous_x, learning_rate, epochs):\n",
    "    \"\"\"An implementation of gradient descent\n",
    "    Args:\n",
    "        previous_x - the previous input value\n",
    "        learning_rate - how much to change x per iteration\n",
    "        epochs - number of steps to run gradient descent\n",
    "    Returns:\n",
    "        A tuple: array of x values, array of loss values\n",
    "    \"\"\"\n",
    "    x_gd = []\n",
    "    y_gd = []\n",
    "    x_gd.append(previous_x)\n",
    "    y_gd.append(func_y(previous_x))\n",
    "    \n",
    "    # loop to update x and y\n",
    "    for i in range(epochs):\n",
    "        # x = lr * gradient(func(prev_x))\n",
    "        update = learning_rate *gradient_func_y(previous_x)\n",
    "        x = previous_x - update\n",
    "        print('step', i, 'previous x', previous_x,\n",
    "              'update:', -update, 'new x:', x)\n",
    "        x_gd.append(x)\n",
    "        y_gd.append(func_y(x))\n",
    "        \n",
    "        # update previous_x\n",
    "        previous_x = x\n",
    "    \n",
    "    return x_gd, y_gd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "With gradient descent implemented, we'll will now run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "x0 = 0.7\n",
    "learning_rate = 0.15\n",
    "epochs = 10\n",
    "\n",
    "x = np.arange(-1, 5, 0.01)\n",
    "y = func_y(x)\n",
    "x_gd, y_gd = gradient_descent(x0, learning_rate, epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Plot the animation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.set_xlim([min(x), max(x)])\n",
    "ax.set_ylim([min(y)-1, max(y)+1])\n",
    "ax.plot(x, y, lw = 0.9, color = 'k')\n",
    "\n",
    "line, = ax.plot([], [], 'r', label = 'Gradient descent', lw = 1.5)\n",
    "point, = ax.plot([], [], 'bo', animated=True)\n",
    "value_display = ax.text(0.02, 0.02, '', transform=ax.transAxes)\n",
    "\n",
    "def init():\n",
    "    \"\"\"Initializes the animation\"\"\"\n",
    "    line.set_data([], [])\n",
    "    point.set_data([], [])\n",
    "    value_display.set_text('')\n",
    "\n",
    "    return line, point, value_display\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"Animates the plot at step i\n",
    "    Args:\n",
    "        i: the step to animate\n",
    "        return: a tuple of line, point, and value_display\n",
    "    \"\"\"\n",
    "    # Animate line\n",
    "    line.set_data(x_gd[:i], y_gd[:i])\n",
    "    \n",
    "    # Animate points\n",
    "    point.set_data(x_gd[i], y_gd[i])\n",
    "\n",
    "    # Animate value display\n",
    "    value_display.set_text('Min = ' + str(y_gd[i]))\n",
    "\n",
    "    return line, point, value_display\n",
    "\n",
    "# call the animator\n",
    "rc('animation', html='html5')\n",
    "anim = FuncAnimation(fig, animate, init_func=init,\n",
    "                     frames=len(x_gd), interval=360,\n",
    "                     repeat_delay=60, blit=True)\n",
    "\n",
    "# display the video\n",
    "HTML(anim.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercises\n",
    "\n",
    "1. Try initializing x0 to something > 2, what do you observe?\n",
    "2. Try increasing the learning_rate to something large like 10. Does the gradient still converge?\n",
    "3. Replace func_y and gradient_func_y above with a cubic function. What do you observe?\n",
    "  ```\n",
    "  y = x^3 - 5x^2 + x + 1\n",
    "  gradient(y) = 3x^2 - 10x + 1\n",
    "  ```\n",
    "4. Replace func_y and gradient_func_y with `cos(x)` and its derivative `-sin(x)`. What do you observe?  What needs to reach convergence?\n",
    "  ```\n",
    "  y = np.tan(x)\n",
    "  gradient(y) = -np.sin(x)\n",
    "  ```\n",
    "  \n",
    "Derivative formulas: https://www.derivative-calculator.net/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Descent Variants\n",
    "\n",
    "1. Stochastic Gradient Descent (SGD)\n",
    "2. Minibatch SGD\n",
    "3. Minibatch SGD with Momentum\n",
    "4. Adaptive Learning Rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "\"Regular\" Gradient Descent is expensive because it processes all samples at once\n",
    "- Imagine you have millions of training samples\n",
    "\n",
    "Stochastic Gradient Descent speeds this up by:\n",
    "- Running gradient descent, one randomly selected training sample at a time\n",
    "- Stochastic: random noise, because samples can vary a lot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notation\n",
    "\n",
    "$\\leftarrow$ = replace value\n",
    "Some texts use this symbol $:=$\n",
    "\n",
    "Examples\n",
    "- $\\theta \\leftarrow \\theta - \\epsilon g$\n",
    "- $\\theta := \\theta - \\epsilon g$\n",
    "\n",
    "Means\n",
    "1. Compute $\\theta' = \\theta - \\epsilon g$\n",
    "2. Update $\\theta = \\theta'$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![sgd](assets/training-basics/sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Symbols:\n",
    "- The $\\eta_t$ denotes the learning rate\n",
    "- Note: $\\Theta$ is denotes the weights matrix\n",
    "\n",
    "(image: Neural Networks in Natural Language Processing, Goldberg, 2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minibatch Stochastic Gradient Descent\n",
    "\n",
    "Instead of 1 random sample at a time:\n",
    "- Sample a \"minibatch\" of m training samples\n",
    "- Run gradient descent on that minibatch\n",
    "- \"Smooths\" out the randomness by operating on a minibatch.\n",
    "- The minibatch size can be tuned (\"hyperparameter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![minibatch sgd](assets/training-basics/minibatch-sgd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Symbols:\n",
    "- $\\epsilon_k$ denotes the learning rate\n",
    "- $\\theta$ denotes the weights matrix\n",
    "- $\\nabla_{\\theta}$ means gradient w.r.t. $\\theta$\n",
    "\n",
    "(image: Deep Learning, Goodfellow, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Minibatch SGD, with momentum\n",
    "\n",
    "Speeds up minibatch SGD by:\n",
    "- Applying an exponentially decaying moving average of the previous gradients ($v$)\n",
    "  - if gradients point the same way, will reach minimum faster\n",
    "- Minibatch SGD: $\\theta \\leftarrow \\theta - \\epsilon g$\n",
    "- Minibatch SGD + momentum: $\\theta \\leftarrow \\theta - \\epsilon g + \\alpha v$, $v \\leftarrow v - \\epsilon g$\n",
    "\n",
    "Variant: Nesterov's momentum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![minibatch sgd with momentum](assets/training-basics/minibatch-sgd-momentum.png)\n",
    "\n",
    "(image: Deep Learning, Goodfellow, 2016)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Adaptive Learning Rate Strategies\n",
    "\n",
    "- Learning rate will control the amount of gradient update\n",
    "  - Large learning rate: risk overshoot and not converge\n",
    "  - Small learning rate: too slow\n",
    "  - Ideal: start large(r), then reduce as we get closer to minima\n",
    "- Strategies\n",
    "  - Constant learning rate\n",
    "  - Time-based or step-based decay\n",
    "  - AdaGrad\n",
    "  - RMSProp\n",
    "  - Adam\n",
    "- What works best depends on your domain (true for **any** optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Comparison](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png)\n",
    "\n",
    "(image: [machine learning mastery](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Another comparison](http://scikit-learn.org/stable/_images/sphx_glr_plot_mlp_training_curves_001.png)\n",
    "\n",
    "(image: [scikit-learn](http://scikit-learn.org/stable/auto_examples/neural_networks/plot_mlp_training_curves.html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Overfitting and Underfitting\n",
    "\n",
    "- Overfitting: High variance\n",
    "- Underfitting: High bias\n",
    "\n",
    "![creativity](http://blog.algotrading101.com/wp-content/uploads/2016/01/overfitting-comics.jpg)\n",
    "\n",
    "(image: algotrading101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![comics](https://imgs.xkcd.com/comics/linear_regression.png)\n",
    "\n",
    "(image: xkcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reading List\n",
    "\n",
    "|Material|Read it for|URL|\n",
    "|--|--|--|\n",
    "|Chapter 8, Pages 290-296|Stochastic Gradient Descent Theory|http://www.deeplearningbook.org/contents/optimization.html|\n",
    "|Ordinary Least Squares Linear Regression|Programming Example|http://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares|\n",
    "\n",
    "Note: in the reading list, SGD refers to \"Minibatch SGD\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
