{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "![galaxies](assets/classification/hubble_tuning_fork.jpg)\n",
    "\n",
    "(image: [NASA](https://imagine.gsfc.nasa.gov/educators/programs/cosmictimes/educators/guide/1929/nebulae.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Classification: binary, multi-class\n",
    "- Logistic Regression\n",
    "- Na√Øve Bayes Classification\n",
    "- K-nearest Neighbours\n",
    "- Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "![one of many cheatsheets but I keep using it](assets/linear-regression/machine-learning-cheet-sheet.png)\n",
    "\n",
    "(image: [sas.com](https://www.sas.com/en_us/insights/analytics/machine-learning.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification\n",
    "\n",
    "Given a sample with $n$ independent features\n",
    "\n",
    "$X^i = [x^i_1, x^i_2, ..., x^i_n]$\n",
    "\n",
    "Predict the probability $P(y)$ that this sample belongs to a class $y$\n",
    "\n",
    "i.e. we \"classify\" the sample as belonging to $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Algorithms for Classification\n",
    "\n",
    "- Logistic Regression*\n",
    "- Naive Bayes*\n",
    "- K-nearest Neighbours*\n",
    "- Support Vector Machines*\n",
    "- Decision Trees and Forests\n",
    "- Neural Networks\n",
    "- etc\n",
    "\n",
    "[* Covered today]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Linear Regression + either Activation or Softmax\n",
    "\n",
    "Activation: Binary Classification\n",
    "\n",
    "Softmax: Multi-class Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Binary\n",
    "\n",
    "![your mood today?](assets/classification/logistic-regression.png)\n",
    "\n",
    "(image: dataaspirant.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Logistic Sigmoid\n",
    "\n",
    "Converts the output to a value between 0 and 1\n",
    "- 1 can mean True, Happy, ...\n",
    "- 0 can mean False, Sad, ...\n",
    "\n",
    "$$\\sigma(x) = \\frac{1}{1+exp(-x)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Credits: https://ilparle.com/2017/04/21/plot-a-simple-sigmoid-function/\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(-8, 8, 0.1)\n",
    "sigmoid = 1 / (1 + np.exp(-x))\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, sigmoid)\n",
    "ax.set(xlabel = 'x', ylabel = 'sigmoid')\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Multi-Class\n",
    "\n",
    "![mnist logistic regression](assets/classification/mnist-logistic-regression.png)\n",
    "\n",
    "(image: [CNTK](https://cntk.ai/pythondocs/CNTK_103B_MNIST_LogisticRegression.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Softmax\n",
    "\n",
    "Softmax:\n",
    "- Converts multiple outputs to a percentage distribution between 0 and 1\n",
    "- Percentage distribution: numbers all add up to 1 (100%)\n",
    "- Outputs: 0.7 happy, 0.2 depressed, 0.1 unknown\n",
    "\n",
    "Example: [1, 2, 3, 4, 1, 2, 3]\n",
    "\n",
    "Result: [0.024, 0.064, 0.175, 0.475, 0.024, 0.064, 0.175]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Credits: https://en.wikipedia.org/wiki/Softmax_function\n",
    "import numpy as np\n",
    "\n",
    "z = [1.0, 2.0, 3.0, 4.0, 1.0, 2.0, 3.0]\n",
    "softmax = lambda x : np.exp(x)/np.sum(np.exp(x))\n",
    "softmax(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![binary v. multiclass](assets/classification/binary_v_multiclass.png)\n",
    "\n",
    "(image: [CNTK](https://cntk.ai/pythondocs/CNTK_103B_MNIST_LogisticRegression.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Iris flower dataset\n",
    "# https://archive.ics.uci.edu/ml/datasets/iris\n",
    "\n",
    "import sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=99)\n",
    "\n",
    "print('First 5 training data:', X_train[:5])\n",
    "print('First 5 training labels:', y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression().fit(X_train, y_train)\n",
    "y_pred_lr = lr.predict(X_test)\n",
    "\n",
    "print('Number of mislabeled points out of test set of %d points:' % (X_test.shape[0]))\n",
    "print('Logistic Regression: %d, Score: %f' % ((y_test != y_pred_lr).sum(), lr.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Naive Bayes Classification\n",
    "\n",
    "![naive bayes](assets/classification/naive-bayes.png)\n",
    "\n",
    "(image: [shatterline.com](http://shatterline.com/blog/2013/09/12/not-so-naive-classification-with-the-naive-bayes-classifier/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bayes Theorem\n",
    "\n",
    "- Inputs: independent features\n",
    "- Outputs: class probabilities\n",
    "  - Bayes Theorem computes the conditional probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries\n",
    "\n",
    "http://scikit-learn.org/stable/modules/naive_bayes.html#naive-bayes\n",
    "\n",
    "- Gaussian\n",
    "- Multinomial\n",
    "- Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes\n",
    "\n",
    "# Now that we added noise to our data, we need to scale the features to between [0, 1]\n",
    "# This is because Naive Bayes cannot handle negative features (throws an error)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "min_max_scaler = MinMaxScaler()\n",
    "min_max_scaler.fit(X_train)\n",
    "X_train_minmax = min_max_scaler.transform(X_train)\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "\n",
    "nb_titles = ['Gaussian Naive Bayes', 'Multinomial NB', 'Bernouilli NB']\n",
    "nb_models = (naive_bayes.GaussianNB(),\n",
    "             naive_bayes.MultinomialNB(),\n",
    "             naive_bayes.BernoulliNB())\n",
    "nb_models = (model.fit(X_train_minmax, y_train) for model in nb_models)\n",
    "\n",
    "print('Number of mislabeled points out of test set of %d points:' % (X_test.shape[0]))\n",
    "\n",
    "for model, title in zip(nb_models, nb_titles):\n",
    "    y_pred = model.predict(X_test_minmax)\n",
    "    wrong = (y_test != y_pred).sum()\n",
    "    score = model.score(X_test_minmax, y_test)\n",
    "    print('%s: %d (score: %.2f)' %(title, wrong, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "![svm](assets/classification/svm.png)\n",
    "\n",
    "(image: [Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines\n",
    "\n",
    "- Inputs: features (not necessarily independent)\n",
    "  - Features should be scaled\n",
    "- Output: classes, separated by \"hyperplane\"\n",
    "\n",
    "- SVM uses \"kernel functions\" to compute the similarity between input samples\n",
    "- Find hyperplane with the maximum margin of separation\n",
    "  - Why? Better generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "- Support Vector Classifier\n",
    "- Different kernel functions to choose from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Scikit-learn has a nifty example that shows how the different kernel functions look like.\n",
    "\n",
    "To illustrate them, we'll use their code example to train SVM models with only 2 features.\n",
    "- Why 2 features? Because it's easier to plot in 2-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/svm/plot_iris.html#sphx-glr-auto-examples-svm-plot-iris-py\n",
    "from sklearn import svm\n",
    "\n",
    "def make_meshgrid(x, y, h=.02):\n",
    "    \"\"\"Create a mesh of points to plot in\n",
    "\n",
    "    Args:\n",
    "        x: data to base x-axis meshgrid on\n",
    "        y: data to base y-axis meshgrid on\n",
    "        h: stepsize for meshgrid, optional\n",
    "\n",
    "    Returns:\n",
    "        xx, yy : ndarray\n",
    "    \"\"\"\n",
    "    x_min, x_max = x.min() - 1, x.max() + 1\n",
    "    y_min, y_max = y.min() - 1, y.max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    return xx, yy\n",
    "\n",
    "def plot_contours(ax, clf, xx, yy, **params):\n",
    "    \"\"\"Plot the decision boundaries for a classifier.\n",
    "\n",
    "    Args:\n",
    "        ax: matplotlib axes object\n",
    "        clf: a classifier\n",
    "        xx: meshgrid ndarray\n",
    "        yy: meshgrid ndarray\n",
    "        params: dictionary of params to pass to contourf, optional\n",
    "    \"\"\"\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    out = ax.contourf(xx, yy, Z, **params)\n",
    "    return out\n",
    "\n",
    "fig, sub = plt.subplots(nrows=2, ncols=2, figsize=(15, 10))\n",
    "plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "\n",
    "# Take the first two features. We could avoid this by using a two-dim dataset\n",
    "X_train_svc = X_train[:, :2]\n",
    "\n",
    "# LinearSVC uses liblinear, SVC uses libsvm\n",
    "# Both are different implementations of SVM\n",
    "svm_titles = ['LinearSVC (liblinear)',\n",
    "              'SVC (linear kernel)',\n",
    "              'SVC (RBF kernel)',\n",
    "              'SVC (3-degree polynomial kernel)']\n",
    "\n",
    "# we create an instance of SVM and fit our data. We do not scale our\n",
    "# data since we want to plot the support vectors\n",
    "C = 1.0  # SVM regularization parameter\n",
    "svm_2D_models = (svm.SVC(kernel='linear', C=C),\n",
    "          svm.LinearSVC(C=C),\n",
    "          svm.SVC(kernel='rbf', gamma=0.7, C=C),\n",
    "          svm.SVC(kernel='poly', degree=3, C=C))\n",
    "svm_2D_models = (clf.fit(X_train_svc, y_train) for clf in svm_2D_models)\n",
    "\n",
    "X0, X1 = X_train_svc[:, 0], X_train_svc[:, 1]\n",
    "xx, yy = make_meshgrid(X0, X1)\n",
    "\n",
    "for clf, title, ax in zip(svm_2D_models, svm_titles, sub.flatten()):\n",
    "    plot_contours(ax, clf, xx, yy,\n",
    "                  cmap=plt.cm.coolwarm, alpha=0.8)\n",
    "    ax.scatter(X0, X1, c=y_train, cmap=plt.cm.coolwarm, s=20, edgecolors='k')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xlabel('Sepal length')\n",
    "    ax.set_ylabel('Sepal width')\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(title)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's compare the performance of SVM with the other Classification models (Logistic Regression, Naive Bayes)\n",
    "\n",
    "To do that, we retrain the SVM models with the full features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise - Train and score SVM using different kernels\n",
    "\n",
    "Train SVM models for the 4 kernel functions.\n",
    "\n",
    "For each model:\n",
    "- Scale X_train and X_test using sklearn.preprocessing.StandardScaler.\n",
    "  - X_train and X_test are multi-dimensional numpy arrays, so you can pass them directly into the scaler without reshaping.\n",
    "- Print the number of mislabeled points\n",
    "- Print the score\n",
    "\n",
    "Use all the features instead of just the first two.\n",
    "\n",
    "Which model performs the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-nearest Neighbors\n",
    "\n",
    "K-nearest neighbors is a multi-purpose algorithm that can be used for multi-class classification.\n",
    "\n",
    "- Find K closest neighbors to that sample\n",
    "- Classify by majority vote of the classes of that sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![knn](assets/classification/knn.png)\n",
    "\n",
    "(image: [dataaspirant.com](http://dataaspirant.com/2016/12/30/k-nearest-neighbor-implementation-scikit-learn/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Libraries\n",
    "\n",
    "sklearn.neighbors.KNeighborsClassifier\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import neighbors\n",
    "\n",
    "# K (how many neighbors to consider for the vote)\n",
    "n_neighbors = [3, 5, 15]\n",
    "\n",
    "# types of weights\n",
    "weights = ['uniform', 'distance']\n",
    "\n",
    "kn_models = []\n",
    "\n",
    "for k in n_neighbors:\n",
    "    for weight in weights:\n",
    "        model = neighbors.KNeighborsClassifier(k, weights=weight)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        wrong = (y_test != y_pred).sum()\n",
    "        score = model.score(X_test, y_test)\n",
    "        print('k = %d, weights = %s: %d (score: %.2f)' %(k, weight, wrong, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/neighbors/plot_classification.html\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "X_train_plot = X_train[:, :2]\n",
    "h = .02  # step size in the mesh\n",
    "n_neighbors = 15\n",
    "\n",
    "# Create color maps\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])\n",
    "\n",
    "for weights in ['uniform', 'distance']:\n",
    "    clf = neighbors.KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights)\n",
    "    clf.fit(X_train_plot, y_train)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X_train_plot[:, 0].min() - 1, X_plot[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_plot[:, 1].min() - 1, X_plot[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X_train_plot[:, 0], X_train_plot[:, 1], c=y_train, cmap=cmap_bold,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"3-Class classification (k = %i, weights = '%s')\"\n",
    "              % (n_neighbors, weights))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A Quick Comparison\n",
    "\n",
    "|Criteria|Logistic Regression|Naive Bayes|SVM|K-nearest neighbors|\n",
    "|--|--|--|--|--|\n",
    "|Interpretability|Simple|Very simple (Conditional Probabilities)|Hard to understand parameters|Simple, but need to pick K|\n",
    "|Ease of training|Fast to train|Fast to train|Computationally and memory intensive for high dimensional data|Can be expensive for high dimensional data|\n",
    "|Requires independent features|Yes, but may still work|Yes, assumes independence|No (don't care)|No (don't care)|\n",
    "|Feature value ranges|No requirements, scale if vary too widely|No negative features|Must scale to [-1, 1]|No requirements|\n",
    "|Output usefulness|Returns probabilities and categories|Returns probabilities and categories|Returns categories only|Returns categories only|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Evaluation Metrics for Classification Problems\n",
    "\n",
    "- Confusion matrix\n",
    "- Accuracy\n",
    "- Precision, Recall, F1 score, ...\n",
    "- Area Under Curve\n",
    "\n",
    "Baselines: Random guess, Majority class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "|Truth/Prediction|Predicted Happy|Predicted Sad|\n",
    "|--|--|--|\n",
    "|Actually Happy|**True positive count**|*False negative count*|\n",
    "|Actually Sad|*False positive count*|**True negative count**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example\n",
    "\n",
    "|Truth/Prediction|Predicted Happy|Predicted Sad|\n",
    "|--|--|--|\n",
    "|Actually Happy|**4**|*3*|\n",
    "|Actually Sad|*2*|**1**|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Accuracy\n",
    "\n",
    "$$accuracy(y\\_true, y\\_pred) = \\frac{true\\_positives + true\\_negatives}{total}$$\n",
    "\n",
    "What's the accuracy in our example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Precision\n",
    "\n",
    "$$precision(y\\_true, y\\_pred) = \\frac{true\\_positives}{true\\_positives + false\\_positives}$$ \n",
    "\n",
    "What's the precision in our example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recall (True Positive Rate / Sensitivity)\n",
    "\n",
    "$$recall(y\\_true, y\\_pred) = \\frac{true\\_positives}{true\\_positives + false\\_negatives}$$ \n",
    "\n",
    "What's the recall in our example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Specificity (True Negative Rate)\n",
    "\n",
    "$$recall(y\\_true, y\\_pred) = \\frac{true\\_negatives}{true\\_negatives + false\\_positives}$$\n",
    "\n",
    "What's the specificity in our example?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### F1 score / F measure\n",
    "\n",
    "$$F(y\\_true, y\\_pred) = 2 * \\frac{precision * recall}{precision + recall}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What do they mean?\n",
    "\n",
    "<img src='assets/classification/precisionrecall.svg.png' width=60%/>\n",
    "\n",
    "(image: [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Libraries\n",
    "\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics\n",
    "- classification_report\n",
    "- confusion_matrix\n",
    "- accuracy_score\n",
    "- f1_score\n",
    "- ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "print('Logistic Regression:')\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Colormaps:\n",
    "# https://matplotlib.org/gallery/color/colormap_reference.html\n",
    "plt.matshow(cm, cmap='Blues')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "iris.target_names # the integer values [0, 1, 2] map to these labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise - Evaluation Metrics for Naive Bayes and SVM Classifiers\n",
    "\n",
    "For the Naive Bayes and SVM models we've seen so far:\n",
    "1. Get the classification metrics.\n",
    "2. Plot the confusion matrix\n",
    "3. How would you interpret the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Get the classification metrics\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices for Naive Bayes\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the confusion matrices for SVM\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Area under ROC curve\n",
    "\n",
    "ROC curve: Receiver Operating Characteristic\n",
    "\n",
    "A plot of Recall (True Positive Rate) vs. Specificity (True Negative Rate)\n",
    "\n",
    "Larger area under ROC curve: better performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Area under ROC curve\n",
    "\n",
    "![roc](assets/classification/sphx_glr_plot_roc_crossval_001.png)\n",
    "\n",
    "(image: [scikit-learn](http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Libraries\n",
    "\n",
    "sklearn.metrics.roc_curve: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html\n",
    "\n",
    "sklearn.metrics.auc: http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### ROC curves and Multi-class Classification\n",
    "\n",
    "- ROC is typically for binary classification\n",
    "- To plot ROC for multi-class:\n",
    "  - Either draw 1 curve per class\n",
    "  - Or compute average for all the classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "\n",
    "# binarize the y labels\n",
    "y = label_binarize(iris.target, classes=[0, 1, 2])\n",
    "\n",
    "n_classes = y.shape[1]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Add noisy features to make the problem harder\n",
    "random_state = np.random.RandomState(0)\n",
    "n_samples, n_features = X.shape\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "\n",
    "# Shuffle and split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5,\n",
    "                                                    random_state=42)\n",
    "\n",
    "classifier = OneVsRestClassifier(LogisticRegression())\n",
    "\n",
    "y_score = classifier.fit(X_train, y_train).decision_function(X_test)\n",
    "y_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and AROC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the ROC curves\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "\n",
    "for i in range(n_classes):\n",
    "    ax.plot(fpr[i], tpr[i], label='ROC curve of class %d (area = %0.2f)' % (i, roc_auc[i]))\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
