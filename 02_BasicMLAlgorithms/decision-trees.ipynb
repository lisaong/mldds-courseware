{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees, Forests, and Ensembles\n",
    "\n",
    "![japanese maple](assets/trees/japanese-maple-2947680_640.jpg)\n",
    "\n",
    "(image: [pixabay](https://pixabay.com/en/japanese-maple-foliage-green-leaves-2947680/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Topics\n",
    "\n",
    "- Decision Trees\n",
    "- Ensemble Forests\n",
    "- Classification\n",
    "- Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Where are we?\n",
    "\n",
    "(lost in the forest?)\n",
    "\n",
    "![spot the trees](assets/linear-regression/machine-learning-cheet-sheet.png)\n",
    "\n",
    "(image: [sas.com](https://www.sas.com/en_us/insights/analytics/machine-learning.html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "- Classification: predicts a class\n",
    "- Regression: predicts a number\n",
    "\n",
    "http://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Advantanges:\n",
    "- Easy to visualize and understand\n",
    "- $O(log(N))$ prediction cost\n",
    "- Flexible for simple tasks: binary / multi-class classification, regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees\n",
    "\n",
    "Disadvantages:\n",
    "- Overfitting\n",
    "- Unbalanced dataset can cause biased trees\n",
    "- Instability: small changes in data can result in completely different tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "![ensemble](assets/trees/teamwork-2499638_960_720.jpg)\n",
    "\n",
    "(image: [pixabay](https://pixabay.com/en/teamwork-team-gear-board-chalk-2499638/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Ensemble Methods\n",
    "\"Teaming up weaker models to create a stronger model\"\n",
    "\n",
    "- Random Forest\n",
    "- AdaBoost\n",
    "- Gradient Boosted Trees\n",
    "- etc\n",
    "\n",
    "http://scikit-learn.org/stable/modules/ensemble.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Random Forests\n",
    "\n",
    "- Fits a few decision trees on subsets of the dataset, then averages the results\n",
    "  - Improves accuracy and reduces overfitting\n",
    "- Classifier: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- Regressor: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "- http://scikit-learn.org/stable/modules/ensemble.html#adaboost\n",
    "- Fit a sequence of small decision trees on repeatedly modified versions of data\n",
    "  - apply weights on the training samples\n",
    "    - incorrectly predicted training sample: increase its weight\n",
    "    - correctly predicted sample: decrease its weight\n",
    "- Combine predictions using sum or majority vote\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### AdaBoost\n",
    "\n",
    "- Classifier: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html\n",
    "- Regressor: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosted Trees\n",
    "\n",
    "- Optimized using Gradient Descent\n",
    "- Better generalization\n",
    "- Classifier: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html\n",
    "- Regressor: http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "![really?](assets/trees/blog_Gradient-Boosting-Image.png)\n",
    "\n",
    "(image: http://blog.kaggle.com/2017/01/23/a-kaggle-master-explains-gradient-boosting/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting: Concept\n",
    "\n",
    "1. Train first tree, measure training error (\"residuals\")\n",
    "$$F_1(x) = y$$\n",
    "2. Train second tree on the residuals, using dataset $\\big(x, y-F_1(x)\\big)$\n",
    "$$h_1(x) = y - F_1(x)$$\n",
    "3. Combine to get a better model\n",
    "$$F_2(x) = F_1(x) + h_1(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting: Concept\n",
    "\n",
    "General formulation:\n",
    "$$F_{m+1}(x) = F_m(x) + h_m(x) = y$$\n",
    "\n",
    "Where $m$ is tuned by cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting: Gradient Descent\n",
    "\n",
    "- Objective: minimize cost function: $L\\big(y, F(x)\\big)$\n",
    "- Apply Tree Boosting, but compute residuals from the **gradients of the cost function**\n",
    "- $n$ training examples $(x_1, ... x_n)$\n",
    "- Residual for the $i^{th}$ example, $m^{th}$ tree:\n",
    "\n",
    "$$r_{im} = -\\biggl[\\frac{\\partial{L\\big(y_i, F_{m-1}(x_i)\\big)}}{\\partial{F_{m-1}(x_i)}}\\biggr]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Gradient Boosting: Algorithm\n",
    "\n",
    "1. Compute residual using $F_{m-1}(x_i)$ and $y_i$\n",
    "2. Train decision tree $h_m(x)$, using dataset ${(x_i, r_{im})}$\n",
    "3. Compute update multiplier:\n",
    "$$\\gamma_m = \\underset{\\gamma}{\\arg \\min} \\sum^n_{i=1} L\\big(y, F_{m-1} + \\gamma h_{m}(x)\\big)$$\n",
    "4. Get next model using multiplier:\n",
    "$$F_m(x) = F_{m-1}(x) + \\gamma_mh_m(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### XGBoost\n",
    "\n",
    "eXtreme Gradient Boosting\n",
    "- A parallel, distributed gradient boosting library\n",
    "- https://github.com/dmlc/xgboost\n",
    "- https://xgboost.readthedocs.io/en/latest/model.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Evaluation Metrics\n",
    "\n",
    "- General metrics for Classification and Regression\n",
    "- Decision Tree-specific metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Decision Tree-specific Metrics\n",
    "- Gini: gini impurity, which measures the quality of a split\n",
    "  - greater than 0: split needed\n",
    "  - 0: all cases fall in 1 category\n",
    "- Information gain / entropy\n",
    "  - pick the split with the highest information gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Workshop: Classification with Decision Trees\n",
    "\n",
    "Credits: http://scikit-learn.org/stable/modules/tree.html#classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "We'll be using Graphviz to visualize the decision tree after training it.\n",
    "\n",
    "Add this module to your `mldds02` conda environment:\n",
    "\n",
    "```\n",
    "conda install python-graphviz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "Train and compare decision tree-based classifiers to predict the `sector` from research and development expenditure type, cost type, and expenditure amount."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "#### Research and Development Expenditure by Type of Cost\n",
    "\n",
    "https://data.gov.sg/dataset/research-and-development-expenditure-by-type-of-cost\n",
    "\n",
    "1. Download dataset from the above URL\n",
    "2. Extract the folder and note the path for use in `read_csv` below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "1. Encode string labels to numbers\n",
    "2. Ensure dataset is balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('D:/tmp/research-and-development-expenditure-by-type-of-cost/research-and-development-expenditure-by-type-of-cost.csv',\n",
    "\n",
    "                 usecols=['sector', 'type_of_expenditure', 'type_of_cost', 'rnd_expenditure'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Label Encoding for Classification\n",
    "\n",
    "Training data needs to be numeric.\n",
    "\n",
    "To convert string labels to numbers, we will do something called \"label encoding\".\n",
    "\n",
    "There are multiple ways to do this: http://pbpython.com/categorical-encoding.html\n",
    "- Dummy columns\n",
    "- Integer labels\n",
    "\n",
    "For this dataset, we'll try assigning integer labels to each unique string value in the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# encode sector from strings to integer labels\n",
    "df['sector_c'] = le.fit_transform(df['sector'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Exercise: Label Encoding\n",
    "\n",
    "Encode the `type_of_expenditure` and `type_of_cost` columns in a similar way as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Ensuring Balanced Dataset for Decision Tree Algorithms\n",
    "\n",
    "For decision trees, it is important to balance the dataset to reduce class bias.\n",
    "\n",
    "If a dataset contains too many samples of one sector (e.g. 'Private Sector', the tree will learn to pick that sector more frequently, which means it's no better than random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Detect if a dataset is unbalanced\n",
    "df['sector_c'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "We got lucky here with the dataset, as there are equal numbers of value_counts for each sector.\n",
    "\n",
    "Suppose we need to balance the dataset, we can use this technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# simulate an unbalanced dataset by replicating columns for sector_c=0\n",
    "\n",
    "df.loc[df.sector_c == 0]\n",
    "\n",
    "# make a copy so as not to affect our original df\n",
    "unbalanced_df = pd.concat([unbalanced_df, df.loc[df.sector_c == 0]], ignore_index=True)\n",
    "\n",
    "# show the unbalanced dataset, sector_c = 0 will have double the number of entries\n",
    "unbalanced_df['sector_c'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_groups = unbalanced_df.groupby('sector_c')\n",
    "\n",
    "# use pandas.DataFrame.sample to create a DataFrame\n",
    "# where all sector groups are re-sampled to the smallest sized group\n",
    "balanced_df = sector_groups.apply(lambda x: x.sample(sector_groups.size().min())).\\\n",
    "    reset_index(drop=True)\n",
    "\n",
    "# show the balanced_df, all sectors are balanced\n",
    "balanced_df['sector_c'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Select Features\n",
    "\n",
    "We'll now break our DataFrame into data and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = df[['rnd_expenditure', 'type_of_expenditure_c', 'type_of_cost_c']]\n",
    "target = df['sector_c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "target.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Train the Decision Tree Classifier\n",
    "\n",
    "1. Shuffle and split the data set into train and test\n",
    "2. Train a `DecisionTreeClassifier`\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dtc.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Probabilities are expressed as a\n",
    "# fraction of samples of the same class in a leaf\n",
    "dtc.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Evaluate Metrics: Classification Accuracy\n",
    "\n",
    "Since this is a classification task, the metrics we used for Logistic Regression also apply here, such as\n",
    "- Precision, recall\n",
    "- Confusion matrix\n",
    "- Accuracy\n",
    "\n",
    "References:\n",
    "- http://scikit-learn.org/stable/modules/classes.html#classification-metrics\n",
    "- http://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "pred_y = dtc.predict(test_X)\n",
    "\n",
    "print(classification_report(test_y, pred_y))\n",
    "\n",
    "cm = confusion_matrix(test_y, pred_y)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.matshow(cm, cmap='Blues')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Visualizing the Decision Tree\n",
    "\n",
    "We'll visualize the learned decision tree using graphviz, to see what type of rules it has learnt from the training data.\n",
    "\n",
    "https://graphviz.readthedocs.io/en/stable/api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "def visualize_tree(fitted_tree, feature_names, target_names, filename):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        fitted_tree: the fitted decision tree. If using ensemble methods\n",
    "            pick the first estimator using model.estimators[0]\n",
    "        feature_names: array containing the feature names\n",
    "        target_names: array containing the target labels\n",
    "        filename: the filename to save the .dot file\n",
    "    \"\"\"\n",
    "    export_graphviz(fitted_tree, out_file=filename,\n",
    "                    feature_names=feature_names,\n",
    "                    class_names=target_names,\n",
    "                    filled=True, rounded=True)\n",
    "\n",
    "    source = graphviz.Source.from_file(filename)\n",
    "    source.render(view=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "feature_names=['rnd_expenditure', 'type_of_expenditure', 'type_of_cost']\n",
    "target_names=df['sector'].unique()\n",
    "filename = 'govt_sector_by_expenditure_tree.dot'\n",
    "\n",
    "visualize_tree(dtc, feature_names, target_names, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise - Decision Tree Classification using Entropy\n",
    "\n",
    "Repeat the steps above to:\n",
    "1. Train a decision tree using the 'entropy' criteria using the training set\n",
    "2. Evaluate the classification metrics\n",
    "3. Visualize the decision tree\n",
    "\n",
    "Which criteria performs better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Visualizing Decision Tree Surfaces\n",
    "\n",
    "Here's a neat trick to try in lieu of what we saw with clustering.\n",
    "\n",
    "Credits: http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from: http://scikit-learn.org/stable/auto_examples/tree/plot_iris.html\n",
    "import numpy as np\n",
    "\n",
    "def plot_decision_surface(classifier, n_classes, X, y, title):\n",
    "    \"\"\"Plots a decision surface using pair-wise combination\n",
    "    of features\n",
    "    Args:\n",
    "        classifier - the decision tree classifier\n",
    "        n_classes - the number of classes\n",
    "        X - the data\n",
    "        y - the labels\n",
    "        title - the plot title\n",
    "    \"\"\"\n",
    "    plot_colors = 'ryb'\n",
    "    plot_step = 0.02\n",
    "\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for pairidx, pair in enumerate([[0, 1], [0, 2], [1, 2]]):\n",
    "        # We only take the two corresponding features\n",
    "        x = X.values[:, pair]\n",
    "\n",
    "        clf = classifier.fit(x, y)\n",
    "\n",
    "        # Plot the decision boundary\n",
    "        plt.subplot(2, 3, pairidx + 1)\n",
    "\n",
    "        x_min, x_max = x[:, 0].min() - 1, x[:, 0].max() + 1\n",
    "        y_min, y_max = x[:, 1].min() - 1, x[:, 1].max() + 1\n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                             np.arange(y_min, y_max, plot_step))\n",
    "        plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
    "\n",
    "        Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
    "\n",
    "        plt.xlabel(feature_names[pair[0]])\n",
    "        plt.ylabel(feature_names[pair[1]])\n",
    "\n",
    "        # Plot the training points\n",
    "        for i, color in zip(range(n_classes), plot_colors):\n",
    "            idx = np.where(y == i)\n",
    "            plt.scatter(x[idx, 0], x[idx, 1], c=color, label=target_names[i],\n",
    "                        cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
    "    plt.axis(\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(target.unique())\n",
    "plot_decision_surface(DecisionTreeClassifier(), n_classes, train_X, train_y,\n",
    "                      \"Decision surface of a Decision Tree classifier using paired features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# To get the category label mapping\n",
    "df[['type_of_expenditure', 'type_of_expenditure_c']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[['type_of_cost', 'type_of_cost_c']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df[['sector', 'sector_c']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Random Forest Classifier with GridSearchCV\n",
    "\n",
    "Now that we have our baseline tree, the next step is to try an ensemble method, such as Random Forest.\n",
    "\n",
    "Let's also maximize our chances of getting the best model by doing Grid Search cross-validation.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Use GridSearchCV to select the optimum hyperparameters\n",
    "gs_rfc = GridSearchCV(RandomForestClassifier(), {'max_depth': [2, 4, 6, 8],\n",
    "                                                 'n_estimators': [5, 10, 20, 30]},\n",
    "                      verbose=1)\n",
    "\n",
    "gs_rfc.fit(train_X, train_y)\n",
    "\n",
    "# select the best estimator\n",
    "print('Best score:', gs_rfc.best_score_)\n",
    "print('Best parameters:', gs_rfc.best_params_)\n",
    "\n",
    "# predict\n",
    "pred_y = gs_rfc.predict(test_X)\n",
    "\n",
    "# evaluation metrics\n",
    "print(classification_report(test_y, pred_y))\n",
    "cm = confusion_matrix(test_y, pred_y)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.matshow(cm, cmap='Blues')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rfc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_rfc.best_estimator_.estimators_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the first tree in the forest\n",
    "rfc = gs_rfc.best_estimator_\n",
    "\n",
    "visualize_tree(rfc.estimators_[0], feature_names, target_names,\n",
    "               'govt_sector_by_expenditure_rf_first.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Visualize the last tree in the forest\n",
    "visualize_tree(rfc.estimators_[-1], feature_names, target_names,\n",
    "               'govt_sector_by_expenditure_rf_last.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the decision surface for pair-wise features, using\n",
    "# the best estimator found by GridSearchCV\n",
    "best_n_estimators = n_estimators=gs_rfc.best_params_['n_estimators']\n",
    "best_max_depth = n_estimators=gs_rfc.best_params_['max_depth']\n",
    "\n",
    "plot_decision_surface(RandomForestClassifier(n_estimators=best_n_estimators,\n",
    "                                             max_depth=best_max_depth),\n",
    "                      n_classes, train_X, train_y,\n",
    "                      \"Decision surface of a Random Forest classifier using paired features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## XGBoost Classifier with GridSearchCV\n",
    " \n",
    "Finally, let's try XGBoost on our dataset, to see how well it does.\n",
    "\n",
    "### Setup\n",
    "\n",
    "XGBoost is a separate library from sklearn (https://xgboost.readthedocs.io/en/latest/build.html)\n",
    "\n",
    "```\n",
    "conda install -c anaconda py-xgboost\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### XGBoost and Scikit-learn\n",
    "\n",
    "XGBoost has its own API, but includes an sklearn wrapper for convenience.\n",
    "\n",
    "https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# Use GridSearchCV to select the optimum hyperparameters\n",
    "gs_xgb = GridSearchCV(xgb.XGBClassifier(), {'max_depth': [2, 4, 6, 8],\n",
    "                                            'n_estimators': [5, 10, 20, 30]},\n",
    "                      verbose=1)\n",
    "\n",
    "gs_xgb.fit(train_X, train_y)\n",
    "\n",
    "# select the best estimator\n",
    "print('Best score:', gs_xgb.best_score_)\n",
    "print('Best parameters:', gs_xgb.best_params_)\n",
    "\n",
    "# predict\n",
    "pred_y = gs_xgb.predict(test_X)\n",
    "\n",
    "# evaluation metrics\n",
    "print(classification_report(test_y, pred_y))\n",
    "cm = confusion_matrix(test_y, pred_y)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plt.matshow(cm, cmap='Blues')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Workshop: Regression with Decision Trees\n",
    "\n",
    "In this workshop, we will apply decision tree-related algorithms to a multi-variate linear regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Goal\n",
    "\n",
    "Train a regression model to predict the `Lifetime Post Total Reach` of a Facebook post to seven input features (category, page total likes, type, month, hour, weekday, paid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Dataset\n",
    "\n",
    "#### Facebook metrics Data Set\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/Facebook+metrics\n",
    "\n",
    "The data is related to posts' published during the year of 2014 on the Facebook's page of a renowned cosmetics brand.\n",
    "\n",
    "1. Download dataset from the above URL\n",
    "2. Extract the folder and note the path for use in read_csv below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv('D:/tmp/Facebook_metrics/dataset_Facebook.csv',\n",
    "                 delimiter=';',\n",
    "                 usecols=['Page total likes', 'Type', 'Category',\n",
    "                         'Post Month', 'Post Weekday', 'Post Hour', 'Paid',\n",
    "                         'Lifetime Post Total Reach'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Prepare Dataset\n",
    "\n",
    "### Convert string columns\n",
    "\n",
    "There are two options for the `Type` column.\n",
    "  1. Convert to dummy columns.\n",
    "  2. Encode the labels to numbers.\n",
    "\n",
    "Either option is fine because the number of labels is small (4). Since we've demonstrated label encoding, we'll try the dummy column approach.\n",
    "\n",
    "### Balance dataset\n",
    "\n",
    "This is a regression task, so we don't need to balance the `Lifetime Post Total Reach` output, because it is a continuous variable.\n",
    "- Instead, we'll check that the discrete columns (`Type`, `Category`, `Paid`) values are balanced.\n",
    "- It's less critical, but a good idea to also check the `Post Month`, `Post Weekday` and `Post Hour` columns, just in case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Convert string columns\n",
    "\n",
    "Convert `Type` to dummy columns and append to DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df_type_dummies = pd.get_dummies(df['Type'])\n",
    "df_type_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Append to existing dataset\n",
    "df = pd.concat([df, df_type_dummies], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Balance dataset\n",
    "\n",
    "Check that these discrete columns are balanced\n",
    "- `Type`\n",
    "- `Category`\n",
    "- `Paid`\n",
    "\n",
    "Optionally, check that these discrete columns are balanced\n",
    "- `Post Month`\n",
    "- `Post Weekday`\n",
    "- `Post Hour`\n",
    "\n",
    "Balanced means that the counts for each discrete values are not overly biased to one or two values. As a rule of thumb, overly means 10x or larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Detect if the `Type` column is balanced\n",
    "df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Instead of dropping all the columns, let's reduce just the `Photo` rows by a random sample of 90 entries\n",
    "num_photo_rows = 90\n",
    "photos_df = df.loc[df.Type == 'Photo']\n",
    "\n",
    "indices_to_keep = photos_df.sample(num_photo_rows).index # random sample the indices\n",
    "indices_to_drop = list(set(photos_df.index) - set(indices_to_keep))\n",
    "\n",
    "balanced_df = df.drop(df.index[indices_to_drop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "balanced_df['Type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "balanced_df['Category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "balanced_df['Paid'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Inspect the other discrete columns as an optional exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Exercise: Select Features\n",
    "\n",
    "We will now select the features X and y from `balanced_df`.\n",
    "\n",
    "X should be these columns:\n",
    "- `Page total likes`\n",
    "- `Category`\n",
    "- `Post Month`\n",
    "- `Post Weekday`\n",
    "- `Post Hour`\n",
    "- `Paid`\n",
    "- `Link`\n",
    "- `Photo`\n",
    "- `Status`\n",
    "- `Video`\n",
    "\n",
    "y should be:\n",
    "- `Lifetime Post Total Reach`\n",
    "\n",
    "Hint: you can do something like:\n",
    "```\n",
    "X = balanced_df[['col 1', 'col 2',  ...]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns to create the dataframe X\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Select the columns to create the dataframe y\n",
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Train the Decision Tree Regressor\n",
    "\n",
    "1. Shuffle and split the data set into train and test\n",
    "2. Train a `DecisionTreeRegressor`\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(train_X, train_y) # Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "### Troubleshooting Data Errors\n",
    "\n",
    "```\n",
    "Input contains NaN, infinity or a value too large for dtype('float32').\n",
    "```\n",
    "\n",
    "Let's print the data types for X (the input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for c in balanced_df.columns:\n",
    "    print(c, balanced_df[c].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Let's convert the input to the supported data type (`float32`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "balanced_df['Paid'] = balanced_df['Paid'].astype('float32')\n",
    "\n",
    "# Verify the dtypes are now fixed\n",
    "for c in X.columns:\n",
    "    print(c, balanced_df[c].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Retrain the decision tree.\n",
    "\n",
    "Note that we didn't have to update train_X or test_X, because these are *views* on balanced_df, not copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(train_X, train_y) # ok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Evaluate Metrics: Regression\n",
    "\n",
    "We'll now plot the learning curve for regression to see how well the decision tree performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "def plot_learning_curve(axis, title, tr_sizes, tr_scores, val_scores):\n",
    "    \"\"\"Plots the learning curve for a training session\n",
    "    Arg:\n",
    "        axis: axis to plot\n",
    "        title: plot title\n",
    "        tr_sizes: sizes of the training set\n",
    "        tr_scores: training scores\n",
    "        val_scores: validation scores\n",
    "    \"\"\"\n",
    "    tr_scores_mean = np.mean(tr_scores, axis=1)\n",
    "    tr_scores_std = np.std(tr_scores, axis=1)\n",
    "    val_scores_mean = np.mean(val_scores, axis=1)\n",
    "    val_scores_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    axis.fill_between(tr_sizes, tr_scores_mean - tr_scores_std,\n",
    "                    tr_scores_mean + tr_scores_std, alpha=0.1,\n",
    "                    color=\"r\")\n",
    "    axis.fill_between(tr_sizes, val_scores_mean - val_scores_std,\n",
    "                    val_scores_mean + val_scores_std, alpha=0.1, color=\"g\")\n",
    "    axis.plot(tr_sizes, tr_scores_mean, 'o-', color=\"r\",\n",
    "            label=\"Training score\")\n",
    "    axis.plot(tr_sizes, val_scores_mean, 'o-', color=\"g\",\n",
    "            label=\"Cross-validation score\")\n",
    "    axis.set(title=title,\n",
    "           xlabel='Training examples',\n",
    "           ylabel='R2 Scores')\n",
    "    axis.grid()\n",
    "    axis.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the learning curve for decision tree classifier, using 3-fold Cross Validation\n",
    "random_state = np.random.RandomState(123)\n",
    "train_sizes, train_scores, validation_scores = learning_curve(\n",
    "    DecisionTreeRegressor(random_state = random_state), train_X, train_y,\n",
    "    random_state=random_state)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "plot_learning_curve(ax, 'Learning Curve: Decision Tree Regressor',\n",
    "                    train_sizes, train_scores, validation_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The Training R2 Score is constant. Can you think of a reason why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Visualizing the Decision Tree\n",
    "\n",
    "Finally, we'll visualize the decision tree.\n",
    "\n",
    "This tree will look very complicated, because:\n",
    "1. There are many features\n",
    "2. We didn't place any limits on max_depth (which we should to make the tree more robust and less likely to overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "feature_names=X.columns\n",
    "target_names=y.columns\n",
    "filename = 'facebook_likes_regressor.dot'\n",
    "\n",
    "visualize_tree(dtr, feature_names, target_names, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Optional Exercises\n",
    "\n",
    "This is a loooong workshop, but if you desire to learn more about Decision Trees, try the following:\n",
    "\n",
    "1. XGBoost regression\n",
    " - Replace `DecisionTreeRegressor()` with `xgb.Regressor()` with optional `GridSearchCV()`\n",
    " - https://github.com/dmlc/xgboost/blob/master/demo/guide-python/sklearn_examples.py\n",
    " \n",
    "2. Random Forest Regressor\n",
    " - Replace `DecisionTreeRegressor()` with `RandomForestRegressor()` with optional `GridSearchCV()`\n",
    "\n",
    "3. Try other Decision Tree algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "livereveal": {
   "autolaunch": false,
   "overlay": "<div class='logo'><img src='assets/Stackup_Logo_Small.png' width='90%'/></div>"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
